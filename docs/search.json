[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "Hi!\nMy name is Carlo and I teach political science, including data analysis, at the University of Stavanger in Norway.\nI often find that students have quite smart ideas that they want to test in a course paper or thesis project and which could be tested with a relatively simple quantitative analysis – but they don’t know how to actually go about this (and might also be intimidated by quantitative methods/coding).\nThe aim of this blog is to show these students how they can test big ideas – e.g., about the role of class, social norms, political institutions, or gender – with relatively simple methods that they should be familiar with from their introductory statistics course and with the free and open-source R statistical programming language.\nThe posts are very much focussed on explaining the general logic of an analysis and assume that readers are at least a bit familiar with the basic of statistics and data analysis methods and how to work with R. Each post also gives pointers to more advanced resources (textbooks, journal articles) on quantitative methods."
  },
  {
    "objectID": "posts/globalization/index.html",
    "href": "posts/globalization/index.html",
    "title": "How vulnerable are workers to globalization (and what effects does this have)?",
    "section": "",
    "text": "Free trade and globalization are back on the political agenda. Starting with the Brexit campaign in 2015 and the UK’s subsequent actual exit from the European Union and now, in 2025, with Donald Trump taking the US into a trade war with essentially the rest of the world, there is a new discussion about the benefits but also drawbacks of free trade (see also Colantone and Stanig 2018b, 2018a, 2019; Agnolin, Colantone, and Stanig 2025).\nAnd, indeed, even though free trade is generally beneficial for economic growth and development, there are usually some workers who are negatively affected by increased competition from abroad (Autor, Dorn, and Hanson 2013; Heckscher 1919; Ohlin 1933). These negative economic effects can then also have further political and social effects, for example in the form of increased demands for social protection (Walter 2010), increased political polarization and conflict (Autor et al. 2020), or rising rates of child poverty, out-of-wedlock births, and premature deaths among men (Autor, Dorn, and Hanson 2019).\nTo be able to study these (and potentially other) effects, researchers use measurements of how exposed and vulnerable workers are to the negative effects of trade and international competition. One way to measure this is with indicators of ‘offshoreability’ — how easy or difficult is it to move the work done by people in different occupations to other countries? Different such indicators have been developed, but one that is relatively widely used are the indicators of offshoreability by Blinder & Krueger (2013). They use a survey-based approach, where they ask workers about their perceptions of how easy it would be to move their jobs abroad and confirm this with a second survey of experts.\nThe rest of this post will show you how you can access measures of worker’s vulnerability to trade and globalization, how you import them into R, and how you merge them with data from round 7 (2014) of the European Social Survey so that you can analyze how vulnerability to globalization affects people’s attitudes and behavior."
  },
  {
    "objectID": "posts/globalization/index.html#the-losers-of-globalization-and-trade",
    "href": "posts/globalization/index.html#the-losers-of-globalization-and-trade",
    "title": "How vulnerable are workers to globalization (and what effects does this have)?",
    "section": "",
    "text": "Free trade and globalization are back on the political agenda. Starting with the Brexit campaign in 2015 and the UK’s subsequent actual exit from the European Union and now, in 2025, with Donald Trump taking the US into a trade war with essentially the rest of the world, there is a new discussion about the benefits but also drawbacks of free trade (see also Colantone and Stanig 2018b, 2018a, 2019; Agnolin, Colantone, and Stanig 2025).\nAnd, indeed, even though free trade is generally beneficial for economic growth and development, there are usually some workers who are negatively affected by increased competition from abroad (Autor, Dorn, and Hanson 2013; Heckscher 1919; Ohlin 1933). These negative economic effects can then also have further political and social effects, for example in the form of increased demands for social protection (Walter 2010), increased political polarization and conflict (Autor et al. 2020), or rising rates of child poverty, out-of-wedlock births, and premature deaths among men (Autor, Dorn, and Hanson 2019).\nTo be able to study these (and potentially other) effects, researchers use measurements of how exposed and vulnerable workers are to the negative effects of trade and international competition. One way to measure this is with indicators of ‘offshoreability’ — how easy or difficult is it to move the work done by people in different occupations to other countries? Different such indicators have been developed, but one that is relatively widely used are the indicators of offshoreability by Blinder & Krueger (2013). They use a survey-based approach, where they ask workers about their perceptions of how easy it would be to move their jobs abroad and confirm this with a second survey of experts.\nThe rest of this post will show you how you can access measures of worker’s vulnerability to trade and globalization, how you import them into R, and how you merge them with data from round 7 (2014) of the European Social Survey so that you can analyze how vulnerability to globalization affects people’s attitudes and behavior."
  },
  {
    "objectID": "posts/globalization/index.html#accessing-the-data",
    "href": "posts/globalization/index.html#accessing-the-data",
    "title": "How vulnerable are workers to globalization (and what effects does this have)?",
    "section": "Accessing the data",
    "text": "Accessing the data\nThe Blinder/Krueger measurements of vulnerability to trade (i.e., “offshoreability”) are publicly available as part of the replication data package of a second (important) research article, which compares the effects of globalization and technological change on workers (Goos, Manning, and Salomons 2014). Goos et al. have archived the research data they use in the ICPSR Data Archive: https://doi.org/10.3886/E112846V1. There, under Data, is the task.dta dataset, which contains the offshoreability scores and, as a bonus, also additional scores on how exposed workers are to losing their jobs to robots and other automated technology (from Autor, Levy, and Murnane 2003; used by e.g., Thewissen and Rueda 2019). You can download the dataset for free, after a quick login."
  },
  {
    "objectID": "posts/globalization/index.html#importing-and-merging-the-data-with-ess-data",
    "href": "posts/globalization/index.html#importing-and-merging-the-data-with-ess-data",
    "title": "How vulnerable are workers to globalization (and what effects does this have)?",
    "section": "Importing and merging the data with ESS data",
    "text": "Importing and merging the data with ESS data\n\nPreparation\nMerging the data with ESS survey data is not as difficult as it might seem. The first step is to make sure that you also have access to ESS data (which can be downloaded for free from europeansocialsurvey.org), and that both datasets are stored in your current working directly (“folder”) so that R can directly find and import them. As mentioned above, the example here uses data from the 7th ESS round, so if you do not already have the data on your computer, quickly download them (in Stata/.dta format).\nIn R, we first load the tidyverse because it contains functions that we need to be able to work with and visualize the data. I am also setting the ggplot theme to classic:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntheme_set(theme_classic())\n\n\n\nData import\nThen we first import the ESS dataset with haven:\n\ness7 &lt;- haven::read_dta(\"ess7.dta\")\n\nThe ESS is quite big, so to make things easier we select only a few variables that we want to work with:\n\ness7 %&gt;% \n  select(idno,cntry,essround,isco08,gndr,agea,trstplt) -&gt; ess7\n\n\nidno and essround are “administrative” variables that are always good to keep\ncntry designates which country a given respondent comes from, and this one should always be kept\nisco08 is the respondents’ occupation as measured by the 2008 version of the International Standard Classification of Occupations (ISCO; see also the other post on measuring class). This variable is important because we will use it to link the ESS data with the vulnerability indicator data\nagea, gndr, and trstplt measure the respondents’ age, gender, and their trust in politicans (on a 0-10 scale)\n\nNext, we import the vulnerability indicator scores:\n\nscores &lt;- haven::read_dta(\"task.dta\")\n\n\n\nA quick exploration\nIt is time to take a quick look at the data to get a sense of what we are working with:\n\nhead(scores)\n\n# A tibble: 6 × 7\n  occupation           RTI_alm_isco_77     BK OFF1_ffl OFF2_ffl OFF3_ffl OFF_gms\n  &lt;dbl+lbl&gt;                      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 12 [Corporate manag…          -0.747 -0.320   -1.43     0.798   -1.66   -0.593\n2 13 [Managers of sma…          -1.52  -0.634   -0.937    0.265   -1.19   -0.593\n3 21 [Physical, mathe…          -0.822  1.05     0.390    0.798   -1.48   -0.375\n4 22 [Life science an…          -1.00  -0.758   -1.92     0.544   -1.70   -0.639\n5 24 [Other professio…          -0.732  0.212   -0.892    1.56    -1.18   -0.513\n6 31 [Physical, mathe…          -0.397 -0.123    0.116   -0.393   -0.433  -0.272\n\n\nYou might notice that every row in the dataset is an occupation: Row #1 is “Corporate managers”, row #2 is “Managers of small enterprises”, and so on. The first column (or variable) indicates the title of the occupation and a 2-digit code based on the 1988 version of the International Standard Classifiation of Occupations (ISCO) classification, where “Corporate managers” have the code 12.1 The other variables are different indicators of vulnerability:\n\nRTI_alm_isco_77 is an indicator of vulnerability to automation (“routine task intensity”) that was developed by Autor, Levy, and Murnane (2003);\nBK is the indicator of offshoreability by Blinder & Krueger (2013);\nOFF1_ffl to OFF3_ffl are alternative indicators of offshoreability by Firpo et al. (see their paper for details: Firpo, Fortin, and Lemieux 2011);\nOFF_gms is the BK indicator in a normalized (“rescaled”) version;\n\nLet’s take a closer look at the BK indicator:\n\nsummary(scores$BK)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.9985 -0.7585 -0.3203  0.0000  0.3996  2.3458 \n\n\nThe indicator ranges from (almost) -1 to 2.35 and is centered on 0. Maybe more interesting is to see which occupations are particularly exposed to globalization. To see that, we reorder the data from the highest to lowest BK score and let R display the five highest-scoring occupations:\n\nscores %&gt;% \n  arrange(-BK) %&gt;% \n  head()\n\n# A tibble: 6 × 7\n  occupation            RTI_alm_isco_77    BK OFF1_ffl OFF2_ffl OFF3_ffl OFF_gms\n  &lt;dbl+lbl&gt;                       &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 82 [Machine operator…           0.493 2.35     1.66    -1.02   0.993     3.18 \n2 73 [Precision, handi…           1.59  1.66     0.585   -0.564  0.00830  -0.616\n3 81 [Stationary plant…           0.323 1.59     0.990   -1.41   0.306     1.63 \n4 74 [Other craft and …           1.24  1.15     1.49    -0.453  1.07     -0.272\n5 21 [Physical, mathem…          -0.822 1.05     0.390    0.798 -1.48     -0.375\n6 41 [Office clerks]              2.24  0.400    0.301    1.26   0.913     1.21 \n\n\nEvidently, typical manual or industrial occupations are the most vulnerable to globalization, but so are physicists and mathematicians!\nJust to complete the picture, we also look at the least exposed occupations:\n\nscores %&gt;% \n  arrange(BK) %&gt;% \n  head()\n\n# A tibble: 6 × 7\n  occupation           RTI_alm_isco_77     BK OFF1_ffl OFF2_ffl OFF3_ffl OFF_gms\n  &lt;dbl+lbl&gt;                      &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 83 [Drivers and mob…         -1.50   -0.999   0.399   -1.37     0.703   -0.628\n2 51 [Personal and pr…         -0.598  -0.937  -0.910    0.312    0.240   -0.639\n3 71 [Extraction and …         -0.185  -0.934   0.603   -1.23     0.0133  -0.593\n4 52 [Models, salespe…          0.0534 -0.893   0.0565   0.913    1.34    -0.639\n5 91 [Sales and servi…          0.0274 -0.808   0.729    0.0522   1.52    -0.375\n6 22 [Life science an…         -1.00   -0.758  -1.92     0.544   -1.70    -0.639\n\n\nHere, it is again some manual workers (e.g., drivers and builders) that top the list (or, rather, are at the bottom). However, salespersons and medical professionals, who obviously cannot do their jobs from abroad, also have low exposure to globalization.\n\n\nAdjusting the ISCO scores\nAs mentioned, the vulnerability score data are based on the ISCO-88 occupational classification, which is an older version of the ISCO-08 classifcation that is used to measure the occupation of ESS respondents. In addition, you may have noticed that the ISCO-scores in the ESS data have four numbers while those in the vulnerability data have two. The latter is because the vulnerability data use a simplified or less fine-grained version of the ISCO classification.\nThis means that to be able to merge the two datasets, we need to make sure that both use the same ISCO-classification so that we can match the different occupations, and that they are measured at the same level of detail.\nFortunately, there are methods to convert the scores back and forth between the different versions of the ISCO classification, and one of these is the occupar package (similar to the DIGCLASS package that is used in the the other post on measuring class). In case you do not already have it installed, you can do so by running the following in your Console:\n\nremotes::install_github(\"DiogoFerrari/occupar\")\n\nTechnically, we could either adjust the ISCO-08 scores in the ESS to match the ISCO-88 scores used in the other dataset, or the other way around, and the occupar package has functions to do both. The only important thing is that both datasets contain the same version of the ISCO classication.\nWe will use the isco08to88() function which, as the name suggests, converts ISCO-08 scores in the ESS to ISCO-88 scores, so that we can then match the ESS data and the vulnerability indicators along these scores and merge the two datasets. This function needs the numerical ISCO scores (not the titles of the occupations but the associated numbers).\nThis means that we need to extract the numerical scores from the ISCO-08 variable in the ESS. The following code does this and stores the converted scores into a new variable:\n\ness7 %&gt;% \n  mutate(isco08_scores = as.numeric(isco08)) -&gt; ess7\n\nWe can take a quick look at the data to see if the conversion worked:\n\nhead(ess7)\n\n# A tibble: 6 × 8\n   idno cntry essround isco08                gndr    agea  trstplt isco08_scores\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt;             &lt;dbl+l&gt; &lt;dbl&gt; &lt;dbl+l&gt;         &lt;dbl&gt;\n1     1 AT           7  7126 [Plumbers and … 1 [Mal… 51    3 [3]            7126\n2     2 AT           7  8312 [Railway brake… 1 [Mal… 67    3 [3]            8312\n3     3 AT           7 NA(a) [Not applicabl… 2 [Fem… 89    3 [3]              NA\n4     4 AT           7  7223 [Metal working… 1 [Mal… 32    0 [No …          7223\n5     5 AT           7  9321 [Hand packers]  2 [Fem… 56    0 [No …          9321\n6     6 AT           7  9321 [Hand packers]  2 [Fem… 67    0 [No …          9321\n\n\nIndeed, the function worked as intended. For example, it correctly extracted the ISCO-08 score 7126 for the “plumber” in row #1 and saved it as 7126 in the new variable.\nThis means we can now convert the new scores to the ISCO-88 classification:\n\ness7 %&gt;% \n  mutate(isco88 = occupar::isco08to88(isco08_scores)) -&gt; ess7\n\nIf we take another quick look at the dataset, you can see the new ISCO-88 scores in the last column; you may also notice that the scores are slightly different (for example, 9321 in the ISCO-08 classification is 9322 in the old one):\n\nhead(ess7)\n\n# A tibble: 6 × 9\n   idno cntry essround isco08         gndr    agea  trstplt isco08_scores isco88\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt;      &lt;dbl+l&gt; &lt;dbl&gt; &lt;dbl+l&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n1     1 AT           7  7126 [Plumbe… 1 [Mal… 51    3 [3]            7126   7136\n2     2 AT           7  8312 [Railwa… 1 [Mal… 67    3 [3]            8312   8312\n3     3 AT           7 NA(a) [Not ap… 2 [Fem… 89    3 [3]              NA     NA\n4     4 AT           7  7223 [Metal … 1 [Mal… 32    0 [No …          7223   7223\n5     5 AT           7  9321 [Hand p… 2 [Fem… 56    0 [No …          9321   9322\n6     6 AT           7  9321 [Hand p… 2 [Fem… 67    0 [No …          9321   9322\n\n\nThe almost last step in the data preparation is to “simplify” the new ISCO-88 scores in the ESS data to the same level of detail that the vulnerability data have. This is relatively easy, we simply “extract” the first two numbers of the four-number ISCO-scores.2 The following code does that:\n\ness7 %&gt;% \n  mutate(isco88_2d = as.numeric(substr(as.character(isco88), 1,2))) -&gt; ess7\n\nIn human language, this code first converts the isco88 variable to text (as.character()), then uses substr() to extract the first two items (“from 1 to 2”), and then converts the result back to numbers with as.numeric().\nIf we take a look at the final result (with variables re-arranged so that we can see them next to each other), we can confirm that all worked as it should:\n\ness7 %&gt;% \n  relocate(idno,cntry,essround,isco08,isco88,isco88_2d) %&gt;% \n  head()\n\n# A tibble: 6 × 10\n   idno cntry essround isco08             isco88 isco88_2d gndr    agea  trstplt\n  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt;           &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl+l&gt; &lt;dbl&gt; &lt;dbl+l&gt;\n1     1 AT           7  7126 [Plumbers a…   7136        71 1 [Mal… 51    3 [3]  \n2     2 AT           7  8312 [Railway br…   8312        83 1 [Mal… 67    3 [3]  \n3     3 AT           7 NA(a) [Not applic…     NA        NA 2 [Fem… 89    3 [3]  \n4     4 AT           7  7223 [Metal work…   7223        72 1 [Mal… 32    0 [No …\n5     5 AT           7  9321 [Hand packe…   9322        93 2 [Fem… 56    0 [No …\n6     6 AT           7  9321 [Hand packe…   9322        93 2 [Fem… 67    0 [No …\n# ℹ 1 more variable: isco08_scores &lt;dbl&gt;\n\n\nFinally, we quickly convert the ISCO-88 scores in the vulnerability dataset to a pure numerical score. We also give it the same name as the corresponding variable in the ESS dataset to make merging the two datasets more straightforward:\n\nscores %&gt;% \n  mutate(isco88_2d = as.numeric(occupation)) -&gt; scores\n\n\n\nMerging the datasets\nMerging two datasets can seem a bit daunting to beginners, but it is actually quite easy. The dplyr package has functions for so-called “mutating joins” (see https://dplyr.tidyverse.org/reference/mutate-joins.html), which in essence simply merge two datasets along one (or potentially more) “identifier” variable. As long as one has one (or more) variables that can directly match observations between datasets – which in our case is the two-number ISCO-88 score – one can use these functions to merge two datasets.\nUsually, the correct function to use to merge datasets is left_join(). In our case, we want to join the ESS dataset with the vulnerability scores dataset along the two-digit ISCO-88 scores, so we specify this in our code:\n\ness7 %&gt;% \n  left_join(scores, by = \"isco88_2d\") -&gt; ess7\n\nAnd that is it.\nIf we take a look at the relevant variables in the final dataset, we can see the vulnerability scores matched to the ESS data:\n\ness7 %&gt;% \n  select(idno,isco08,isco88_2d,BK,RTI_alm_isco_77) %&gt;% \n  head()\n\n# A tibble: 6 × 5\n   idno isco08                                  isco88_2d     BK RTI_alm_isco_77\n  &lt;dbl&gt; &lt;dbl+lbl&gt;                                   &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;\n1     1  7126 [Plumbers and pipe fitters]              71 -0.934          -0.185\n2     2  8312 [Railway brake, signal and switc…        83 -0.999          -1.50 \n3     3 NA(a) [Not applicable]                         NA NA              NA    \n4     4  7223 [Metal working machine tool sett…        72 -0.451           0.457\n5     5  9321 [Hand packers]                           93 -0.658           0.449\n6     6  9321 [Hand packers]                           93 -0.658           0.449\n\n\nYou see that respondent with ID number 1 (the plumber) has a BK offshorability score of -9.34 and also an automation-vulnerability (RTI) score of -0.185.\nAs a last step, we convert the final dataset to the traditional R format so that it is easier to work with later:\n\ness7 &lt;- labelled::unlabelled(ess7)"
  },
  {
    "objectID": "posts/globalization/index.html#vulnerability-to-globalization-gender-age-and-political-trust",
    "href": "posts/globalization/index.html#vulnerability-to-globalization-gender-age-and-political-trust",
    "title": "How vulnerable are workers to globalization (and what effects does this have)?",
    "section": "Vulnerability to globalization, gender, age, and political trust",
    "text": "Vulnerability to globalization, gender, age, and political trust\nWith the merged data in hand, we can now do lots of different analyses. For example, we could use the vulnerabiility scores in a regression analysis to see if they are related to some other variable. Just to illustrate this, we will do a few quick visual analyses of the relationships between the offshoreability scores and gender, age, and political trust.\nFor example, let’s see if men or women are on average more exposed to globalization:\n\ness7 %&gt;% \n  group_by(gndr) %&gt;% \n  summarise(avg_BK = mean(BK, na.rm = T)) %&gt;% \n  drop_na() %&gt;%\n  ggplot(aes(x = gndr, y = avg_BK)) +\n    geom_col()\n\n\n\n\n\n\n\n\nThere is only a tiny difference: Women are very slightly less exposed to globalization than men.\nNext, we can see if there is a relation to age. To do that, we can calculate the average age in a given occupation and relate that average age to the vulnerability score in a scatterplot. Note that we use geom_text() to add the occupation titles to the graph:\n\ness7 %&gt;% \n  group_by(occupation) %&gt;% \n  summarise(avg_age = mean(agea, na.rm = T),\n            avg_BK = mean(BK, na.rm = T)) %&gt;% \n  drop_na() %&gt;% \n  ggplot(aes(x = avg_age, y = avg_BK)) +\n    geom_point() +\n    geom_text(aes(label = occupation), vjust = 1, \n              color = \"grey\",\n              size = 3) +\n    geom_smooth(method = \"lm\", color = \"grey\", \n                linetype = \"dashed\", se = F) +\n    labs(x = \"Average age in occupational group\",\n         y = \"Average offshoreability\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere does seem to be a positive relationship: Occupations where workers are older are also more exposed to globalization.\nFinally, let’s see if exposure to globalization has political effects by testing if there is a relationship to trust in politicians. We can do this in more fancy way with a hexbin plot (and relatively large bin sizes):\n\ness7 %&gt;% \n  drop_na(trstplt,BK) %&gt;%\n  mutate(trstplt_num = as.numeric(trstplt)-1) %&gt;%\n  ggplot(aes(y = trstplt_num, x = BK)) +\n    geom_hex(binwidth = c(.75,1))\n\n\n\n\n\n\n\n\nAt least based on this (admittedly) very quick and superficial look at the data, there does not seem to be a clear relationship."
  },
  {
    "objectID": "posts/globalization/index.html#next-steps",
    "href": "posts/globalization/index.html#next-steps",
    "title": "How vulnerable are workers to globalization (and what effects does this have)?",
    "section": "Next steps",
    "text": "Next steps\nEven if there is not link to political trust, maybe you can think of other variables that are measured in the ESS that globalization vulnerability could have an effect on? And maybe you can even think of macro-level variables that could make these effects stronger or weaker (and then use the other post to compare the effects of globalization vulnerability between two or more selected countries that differ in relevant ways on these variables)?\nFinally, you can also merge the vulnerability scores to other survey datasets such as the ISSP or the Eurobarometer so long as the survey dataset contains ISCO occupation scores."
  },
  {
    "objectID": "posts/globalization/index.html#footnotes",
    "href": "posts/globalization/index.html#footnotes",
    "title": "How vulnerable are workers to globalization (and what effects does this have)?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://ilostat.ilo.org/methods/concepts-and-definitions/classification-occupation/ for details on the ISCO classification.↩︎\nThis works because the numbers reflect the hierarchical structure of the ISCO classification. If we cut off the last two numbers, we get to a higher level of aggregation. See https://ilostat.ilo.org/methods/concepts-and-definitions/classification-occupation/#elementor-toc__heading-anchor-2 for details.↩︎"
  },
  {
    "objectID": "posts/measuringclass/index.html",
    "href": "posts/measuringclass/index.html",
    "title": "Measuring class with survey data",
    "section": "",
    "text": "Class is a key concept in the social and political sciences, and it explains many important phenomena, from party preferences and voting over social attitudes to health outcomes (e.g., Elo 2009; Gingrich 2017; Schwander and Häusermann 2013; Häusermann et al. 2022; Evans 2000). Therefore, it is important for every empirical social and political researcher to know how to measure people’s positions in class structures.\nSociologists have spent a lot of time on developing class schemes that make the abstract concept of “class” empirically measurable. The probably most famous class scheme is the Erikson-Goldthorpe-Portocarero (EGP) class scheme that was developed in the 1970s (Erikson, Goldthorpe, and Portocarero 1979), but there are also more recent schemes that take into account the fact that, as a result of technological change, increased educational attainment, and other factors, societies and labor markets in the 21st century look quite different than they did in the 1970s or 1980s. Daniel Oesch’s (2006) scheme is an important modern class scheme.\nThe basis for class schemes is generally occupation – what job does someone have? For example, someone who is a medical doctor would typically be seen as a “higher-skilled professional”, whereas a welder would usually be classified as a “skilled manual worker”. People’s occupations are usually measured with occupational classification schemes, the most widely used is the International Labour Organization’s (ILO) International Standard Classification of Occupations (ISCO) scheme.1 This scheme comes in different versions reflecting the years they were adopted: ISCO-68, ISCO-88, and ISCO-08.\nThere are of course some people who’s occupation is being self-employed – they run their own businesses, which can be a small one-person business (e.g., a shop) but it can also be a medium-sized company with 500 employees. Obviously, this has effects on their class membership: A small shop owner would often be considered to be a member of the “petite bourgeoisie”, while someone who owns a larger company might be considered a “capital owner”."
  },
  {
    "objectID": "posts/measuringclass/index.html#class-still-matters",
    "href": "posts/measuringclass/index.html#class-still-matters",
    "title": "Measuring class with survey data",
    "section": "",
    "text": "Class is a key concept in the social and political sciences, and it explains many important phenomena, from party preferences and voting over social attitudes to health outcomes (e.g., Elo 2009; Gingrich 2017; Schwander and Häusermann 2013; Häusermann et al. 2022; Evans 2000). Therefore, it is important for every empirical social and political researcher to know how to measure people’s positions in class structures.\nSociologists have spent a lot of time on developing class schemes that make the abstract concept of “class” empirically measurable. The probably most famous class scheme is the Erikson-Goldthorpe-Portocarero (EGP) class scheme that was developed in the 1970s (Erikson, Goldthorpe, and Portocarero 1979), but there are also more recent schemes that take into account the fact that, as a result of technological change, increased educational attainment, and other factors, societies and labor markets in the 21st century look quite different than they did in the 1970s or 1980s. Daniel Oesch’s (2006) scheme is an important modern class scheme.\nThe basis for class schemes is generally occupation – what job does someone have? For example, someone who is a medical doctor would typically be seen as a “higher-skilled professional”, whereas a welder would usually be classified as a “skilled manual worker”. People’s occupations are usually measured with occupational classification schemes, the most widely used is the International Labour Organization’s (ILO) International Standard Classification of Occupations (ISCO) scheme.1 This scheme comes in different versions reflecting the years they were adopted: ISCO-68, ISCO-88, and ISCO-08.\nThere are of course some people who’s occupation is being self-employed – they run their own businesses, which can be a small one-person business (e.g., a shop) but it can also be a medium-sized company with 500 employees. Obviously, this has effects on their class membership: A small shop owner would often be considered to be a member of the “petite bourgeoisie”, while someone who owns a larger company might be considered a “capital owner”."
  },
  {
    "objectID": "posts/measuringclass/index.html#what-information-do-you-need-and-where-do-you-get-it",
    "href": "posts/measuringclass/index.html#what-information-do-you-need-and-where-do-you-get-it",
    "title": "Measuring class with survey data",
    "section": "What information do you need, and where do you get it?",
    "text": "What information do you need, and where do you get it?\nClass is an individual-level variable: A person can be a member of the working class, but a country cannot. This means that we use individual-level data – survey data – to measure class. Such survey data need to contain three pieces of information (variables) that reflect people’s class membership:\n\nTheir occupation. This needs to be measured at the highest level of detail, meaning with the four-digit ISCO-88 or ISCO-08 scheme.\nWhether or not they are self-employed.\nIf they are self-employed, how many employees they have.\n\nMany survey datasets contain this information in some form, but it is usually easiest to use either data from large and well-known comparative social survey projects like the International Social Survey Project (ISSP) or the European Social Survey (ESS).2 Both are free to use (but you do need to register as a user). Many national survey projects also contain that information, but occupation is often coded based on the ISCO scheme but based on national occupational classification schemes (e.g., ANZSCO for Australia and New Zealand or SOC for the United States). These can be translated to the ISCO scheme with specific conversion tables, but this often takes quite a bit of time and effort.\nTechnically speaking, applying a class scheme to survey data is quite a bit of work because you need to go over a long list of occupations – the four-digit ISCO08 scheme contains 473 different occupations – and decide which class they belong to. Following this, you have to write code to group all the different observations in your dataset into their classes. Obviously, this would take a lot of time.\nFortunately, people have written packages for R that make this a quick and (normally) easy thing to do. Two relevant packages are the DIGCLASS package, which was developed by researchers at the EU, and the occupar package.3\nThe rest of this tutorial shows how you can measure people’s class with data from the ISSP and using the DIGCLASS package for R. Most of this also applies if you work with data from the ESS, but some data import and cleaning steps might be different. Below is an example of how your dataset needs to look like that you can use to guide your data cleaning and preparation when you work with the ESS."
  },
  {
    "objectID": "posts/measuringclass/index.html#installing-the-digclass-package",
    "href": "posts/measuringclass/index.html#installing-the-digclass-package",
    "title": "Measuring class with survey data",
    "section": "Installing the DIGCLASS package",
    "text": "Installing the DIGCLASS package\nThe DIGCLASS package is not on CRAN (the official R “app store”), but you can install it with the remotes-package (which you need to have installed first, of course):\n\n# install.packages(\"remotes\")\nremotes::install_git(\"https://code.europa.eu/digclass/digclass.git\")\n\nNext, we load the package with library(), in addition to the tidyverse package:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DIGCLASS)\ntheme_set(theme_classic())"
  },
  {
    "objectID": "posts/measuringclass/index.html#getting-issp-data",
    "href": "posts/measuringclass/index.html#getting-issp-data",
    "title": "Measuring class with survey data",
    "section": "Getting ISSP data",
    "text": "Getting ISSP data\nIn this tutorial, we will work with data from the 2016 Role of Government round of the ISSP (v. 2.0.0; 19.09.2018), which you can download from the GESIS data repository: https://www.gesis.org/en/issp/data-and-documentation/role-of-government/2016#c127852. As mentioned earlier, you need to register as a user, but this is free – and also gives you access to many other survey datasets like the Eurobarometer or the European Values Study.\nMake sure that you download the data in SPSS (.sav) format and that you store them in the folder that you are working in (ideally your RStudio Project folder)."
  },
  {
    "objectID": "posts/measuringclass/index.html#importing-the-dataset",
    "href": "posts/measuringclass/index.html#importing-the-dataset",
    "title": "Measuring class with survey data",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nTo import the dataset, you can use the read_sav() function from the haven package. Important: Simply import the dataset for now, do not yet convert it with labelled::unlabelled()! I have stored the dataset as issp16.sav, so I need to specify this in my code – you obviously need to use the name that you gave your dataset file:\n\nissp &lt;- haven::read_sav(\"issp16.sav\")\n\nAs other large survey datasets, the ISSP dataset is very large and contains almost 400 variables:\n\ndim(issp)\n\n[1] 48720   395\n\n\nTo make things easier for now, we trim the data to the variables we actually need plus one variable (v19) that we can later use as a dependent variable in an example analysis:\n\nissp %&gt;% \n  select(studyno,country, # good to keep these in \n         ISCO08, # ISCO-08 occupational codes\n         EMPREL, # Employment relationship, to identify self-employed\n         NEMPLOY, # number of employees if self-employed\n         v19 # a variable measuring respondents' views on whether the government should spend more on the unemployed\n         ) -&gt; issp"
  },
  {
    "objectID": "posts/measuringclass/index.html#data-preparation",
    "href": "posts/measuringclass/index.html#data-preparation",
    "title": "Measuring class with survey data",
    "section": "Data preparation",
    "text": "Data preparation\nThe DIGCLASS package expects the data it works with to be in a specific format. If you for example call up the help file for the DIGCLASS::isco08_to_oesch() function with ?DIGCLASS::isco08_to_oesch and scroll down a bit, you see that the function needs three main inputs:\n\nx, which is the four-digit ISCO-08 scores. They need to be stored as text (character)\nself_employed, which needs to be a “numeric vector indicating whether each individual is self-employed (1) or an employee (0).”\nn_employees, which needs to be a “numeric vector indicating the number of employees under each respondent.”\n\nThis means we need to have three variables that correspond exactly to this: ISCO-08 scores as text, a 0/1 dummy indicating whether someone is self-employed, and a variable containing the number of employees for those who are self-employed.\n\nPreparing the ISCO-08 scores\nLet’s start the data preparation with ISCO08, and let’s first take a closer look at how it is stored now:\n\nclass(issp$ISCO08)\n\n[1] \"haven_labelled\" \"vctrs_vctr\"     \"double\"        \n\n\nFrom the result of class(), we see that the variable is stored in a labelled-type format – which is because the dataset was imported with haven – and this is also the case for all the other variables (see the Environment).\nTo see a bit more clearly how the ISCO08 variable looks like, let’s look at the first few observations:\n\nissp %&gt;% \n  select(ISCO08) %&gt;% \n  slice_head(n = 10) # to get first ten observations\n\n# A tibble: 10 × 1\n   ISCO08                                                               \n   &lt;dbl+lbl&gt;                                                            \n 1 2611 [Lawyers]                                                       \n 2 2512 [Software developers]                                           \n 3 1212 [Human resource managers]                                       \n 4 1439 [Services managers not elsewhere classified]                    \n 5 4419 [Clerical support workers not elsewhere classified]             \n 6 1345 [Education managers]                                            \n 7 3230 [Traditional and complementary medicine associate professionals]\n 8 2654 [Film, stage and related directors and producers]               \n 9 2611 [Lawyers]                                                       \n10 5131 [Waiters]                                                       \n\n\nYou see that the first observation is a lawyer, which has the ISCO-08 code 2611, the next is a software developer (ISCO-08 code 2512), and so on.\nNow comes an important step: We need to convert the ISCO08 variable to a character-type variable – for some reason, the DIGCLASS package expects that the ISCO codes are stored as text (e.g., “2611”, “2512”), and that is what we need to deliver for the package to work.\nTo do that, we simply use as.character():\n\nissp %&gt;% \n  mutate(isco_nums_as_text = as.character(ISCO08)) -&gt; issp\n\nThe new variable should now be a character-type variable:\n\nclass(issp$isco_nums_as_text)\n\n[1] \"character\"\n\n\nThis means that the ISCO-08 scores are taken care off and we can move on to the next piece of information that we need: a 0/1 variable that tells us if people are self-employed.\n\n\n\nSelf-employment\nInformation about how people earn their living in general is contained in the EMPREL variable. To see how this looks like, we can quickly tabulate the individual categories:\n\ntable(issp$EMPREL)\n\n\n    1     2     3     4 \n33504  4169  1797  1185 \n\n\nUnfortunately, we only get numbers. This is because the dataset is still stored in the labelled format, and we can quickly fix this by using unlabelled():\n\nissp &lt;- labelled::unlabelled(issp)\n\nNow the tabulation should work as intended:\n\ntable(issp$EMPREL)\n\n\nNAP (Code 3 in WORK; NZ: Code 2-9 MAINSTAT) \n                                          0 \n                                   Employee \n                                      33504 \n            Self-employed without employees \n                                       4169 \n               Self-employed with employees \n                                       1797 \n          Working for own family's business \n                                       1185 \n                                  No answer \n                                          0 \n\n\nYou see that most respondents fall into the “Employee” category, but there are also people who are self-employed with and without employees. Some also work in a family business. Finally, there are some empty categories that have no observations, but we ignore them for now.\n\nAll we really need to do is to re-code this variable into a 0/1 dummy that is equal to 1 if people are self-employed and 0 otherwise. Here, we can use the case_match() function, which is simply put a more advanced version of if_else():\n\nissp %&gt;% \n  mutate(selfemp = case_match(EMPREL,\n                              c(\"Self-employed without employees\",\n                                \"Self-employed with employees\") ~ 1,\n                              c(\"Employee\",\"Working for own family's business\") ~ 0,\n                              .default = NA)) -&gt; issp\n\nMaybe you can already see that we are here telling R to create a new variable called selfemp that is 1 if the EMPREL variable is either “Self-employed without employees” or “Self-employed with employees” and 0 otherwise. To make sure that observations that do not fit either of these conditions are excluded, we specify .default = NA.\nWe can do a quick cross-tabulation to see if the re-coding worked as intended:\n\ntable(issp$EMPREL,issp$selfemp)\n\n                                             \n                                                  0     1\n  NAP (Code 3 in WORK; NZ: Code 2-9 MAINSTAT)     0     0\n  Employee                                    33504     0\n  Self-employed without employees                 0  4169\n  Self-employed with employees                    0  1797\n  Working for own family's business            1185     0\n  No answer                                       0     0\n\n\nIt looks like things did work: the self-employed are coded as 1, all others are 0.\n\n\nNumber of employees\nThe final variable we need is how many employees those respondents who are self-employed have. Here, we can use the NEMPLOY variable from the ISSP dataset, but let’s again begin by simply checking what type this variable is:\n\nclass(issp$NEMPLOY)\n\n[1] \"numeric\"\n\n\nThe variable is already numeric, which means we do not really have to do anything with it – it is good to go. But we can nevertheless quickly visualize it to see how it is distributed:\n\nissp %&gt;% \n  ggplot(aes(x = NEMPLOY)) +\n    geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 46996 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThere are a few extreme outliers which make it difficult to see anything. We can get a clearer picture by removing those with more than 100 employees from the graph (obviously, we only do this for the graph!):\n\nissp %&gt;% \n  filter(NEMPLOY&lt;100) %&gt;% \n  ggplot(aes(x = NEMPLOY)) +\n    geom_histogram(color = \"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nMost respondents who have employees have only relatively small businesses with less than 25 employees."
  },
  {
    "objectID": "posts/measuringclass/index.html#generating-a-class-variable",
    "href": "posts/measuringclass/index.html#generating-a-class-variable",
    "title": "Measuring class with survey data",
    "section": "Generating a class variable",
    "text": "Generating a class variable\nWe now have all pieces of information we need and can get to the class variable. Let’s start by generating two of Daniel Oesch’s (2006) class schemes, the very simple one with five classes and the more advanced one with eight classes. Each can be generated with the isco08_to_oesch() function. The following code shows how to create both class schemes at once:\n\nissp %&gt;% \n  mutate(oesch_5 = DIGCLASS::isco08_to_oesch(x = isco_nums_as_text,\n                                             self_employed = selfemp,\n                                             n_employees = NEMPLOY,\n                                             n_classes = 5,\n                                             label = T,\n                                             to_factor = F),\n         oesch_8 = DIGCLASS::isco08_to_oesch(x = isco_nums_as_text,\n                                             self_employed = selfemp,\n                                             n_employees = NEMPLOY,\n                                             n_classes = 8,\n                                             label = T,\n                                             to_factor = F)) -&gt; issp\n\nℹ ISCO variable has occupations with digits less than 4. Converting to 4 digits.\n\n\n• Converted `110` to `0110`\n\n\n• Converted `310` to `0310`\n\n\n• Converted `210` to `0210`\n\n\nℹ ISCO variable has occupations with digits less than 4. Converting to 4 digits.\n\n\n• Converted `110` to `0110`\n\n\n• Converted `310` to `0310`\n\n\n• Converted `210` to `0210`\n\n\n\nLet’s have a look at the results:\n\ntable(issp$oesch_5)\n\n\n'Higher-grade service class'  'Lower-grade service class' \n                        6374                         7058 \n           'Skilled workers'      'Small business owners' \n                       12478                         1117 \n         'Unskilled workers' \n                        7558 \n\ntable(issp$oesch_8)\n\n\n                           '(Associate) managers' \n                                             5541 \n                                         'Clerks' \n                                             4043 \n                             'Production workers' \n                                             8309 \n'Self-employed professionals and large employers' \n                                              493 \n                                'Service workers' \n                                             7684 \n                          'Small business owners' \n                                             1117 \n            'Socio-cultural (semi-)professionals' \n                                             4667 \n                 'Technical (semi-)professionals' \n                                             2731 \n\n\nAnd we have what we want: Two class schemes, one simpler and the other a bit more detailed. The second one is used by for example Gingrich (2017) or Schwander & Häusermann (2013).4"
  },
  {
    "objectID": "posts/measuringclass/index.html#example-analysis",
    "href": "posts/measuringclass/index.html#example-analysis",
    "title": "Measuring class with survey data",
    "section": "Example analysis",
    "text": "Example analysis\nLet’s say we wanted to find out if people’s class has an effect on how they think about the welfare state, specifically whether the government should do more to support the unemployed. As mentioned earlier, the ISSP includes a variable that measures these attitudes and which looks like this:\n\nclass(issp$v19)\n\n[1] \"factor\"\n\ntable(issp$v19)\n\n\n             NAV (PH)       Spend much more            Spend more \n                    0                  7209                 12422 \nSpend the same as now            Spend less       Spend much less \n                16760                  6546                  2390 \n         Can't choose             No answer \n                    0                     0 \n\n\nThe variable is stored as a factor (i.e., as a categorical variable), but it has five categories – so we can, sort of, get away with treating it as if it were numeric (this is what Thewissen and Rueda 2019 also do). To be able to do that, we first have to check how it looks internally and then convert it:\n\nbst290::visfactor(dataset = issp,\n                  variable = \"v19\")\n\n values                labels\n      1              NAV (PH)\n      2       Spend much more\n      3            Spend more\n      4 Spend the same as now\n      5            Spend less\n      6       Spend much less\n      7          Can't choose\n      8             No answer\n\n\nThere is a bit of a divergence between values and labels – the NAV (PH) category is empty (see above), which means the lowest actual category has the value of 2 and so on. We can fix this by simply using droplevels() to get rid of empty categories and then as.numeric().\nOne thing we need to pay attention to is that, right now, lower scores correspond to more support for government aid to the unemployed. This is a bit strange to work with, so we reverse the scale of the new variable by subtracting it from 6 (so that the score of 1 becomes 6-1 = 5, 2 becomes 6-2 = 4, and so on:)\n\nissp %&gt;% \n  mutate(v19 = droplevels(v19),\n         unemspend = 6 - as.numeric(v19)) -&gt; issp\n\nThe new numeric variable has values from 1 to 5, which is what we want:\n\ntable(issp$unemspend)\n\n\n    1     2     3     4     5 \n 2390  6546 16760 12422  7209 \n\n\nLet’s now see to class influences attitudes toward help for the unemployed in Sweden (it is important to focus on one country alone, otherwise a simple linear regression model will give wrong results!):\n\nissp %&gt;% \n  filter(country == \"SE-Sweden\") -&gt; swe_data\n\nmod1 &lt;- lm(unemspend ~ oesch_5,\n           data = swe_data)\nsummary(mod1)\n\n\nCall:\nlm(formula = unemspend ~ oesch_5, data = swe_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.38053 -0.38053  0.03929  0.43478  2.43478 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                         2.80315    0.05211  53.793  &lt; 2e-16 ***\noesch_5'Lower-grade service class'  0.15756    0.07196   2.190   0.0288 *  \noesch_5'Skilled workers'            0.50707    0.07234   7.010 4.56e-12 ***\noesch_5'Small business owners'     -0.23793    0.18084  -1.316   0.1886    \noesch_5'Unskilled workers'          0.57738    0.09391   6.148 1.16e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8305 on 939 degrees of freedom\n  (196 observations deleted due to missingness)\nMultiple R-squared:  0.07685,   Adjusted R-squared:  0.07291 \nF-statistic: 19.54 on 4 and 939 DF,  p-value: 1.842e-15\n\n\nAs always, one category (here: “Higher-grade service class”) is omitted from the model and the coefficients show us the difference from each other class to the omitted one. This means that all classes except for small business owners are significantly more supportive of government help for the unemployed than the higher-grade service class.\nTo get a better sense, we can use prediction::prediction_summary() to get predicted support scores per class based on the model:\n\nprediction::prediction_summary(model = mod1,\n                               at = list(oesch_5 = unique(na.omit(swe_data$oesch_5))))\n\n                  at(oesch_5) Prediction      SE     z         p lower upper\n            'Skilled workers'      3.310 0.05017 65.98 0.000e+00 3.212 3.409\n 'Higher-grade service class'      2.803 0.05211 53.79 0.000e+00 2.701 2.905\n  'Lower-grade service class'      2.961 0.04963 59.65 0.000e+00 2.863 3.058\n          'Unskilled workers'      3.381 0.07813 43.27 0.000e+00 3.227 3.534\n      'Small business owners'      2.565 0.17317 14.81 1.202e-49 2.226 2.905\n\n\n\nWe can get an ever better picture of the results if we just visualize the result:\n\nprediction::prediction_summary(model = mod1,\n                               at = list(oesch_5 = unique(na.omit(swe_data$oesch_5)))) %&gt;% \n  ggplot(aes(x = Prediction, \n             y = reorder(`at(oesch_5)`,Prediction), \n             xmin = lower, xmax = upper)) +\n    geom_point(stat = \"identity\") +\n    geom_linerange() +\n    scale_x_continuous(breaks = seq(1,5,1),\n                       limits = c(1,5)) +\n    labs(x = \"Predicted support for government aid to the unemployed\",\n         y = \"Class\", caption = \"95% confidence intervals\")\n\n\n\n\n\n\n\n\nNote that we use reorder() to arrange the classes from highest to lowest support. Clearly, small business owners in Sweden are least supportive of government help for the unemployed, while unskilled and skilled workers (i.e., the “working class”) are most supportive. Looks like class does still matter in Sweden!"
  },
  {
    "objectID": "posts/measuringclass/index.html#footnotes",
    "href": "posts/measuringclass/index.html#footnotes",
    "title": "Measuring class with survey data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee also https://isco-ilo.netlify.app/en/isco-08/#download-isco-08-material↩︎\nSee https://issp.org/ and https://www.europeansocialsurvey.org/.↩︎\nSee https://code.europa.eu/digclass/digclass and https://github.com/DiogoFerrari/occupar.↩︎\nGingrich calls “socio-cultural (semi-) professionals” the “new middle class”, “technical (semi-) professionals”, “clerks”, and “(Associate) managers” are the “old middle class”, “service workers” are the “new working class”, and “Production workers” are the “old working class”. If you wanted, you could use case_match() to re-code the oesch_5 variable into a new and simpler class scheme that corresponds to what Gingrich is using.↩︎"
  },
  {
    "objectID": "posts/conflict/index.html",
    "href": "posts/conflict/index.html",
    "title": "Studying conflict and violence across countries",
    "section": "",
    "text": "Why do countries or fellow citizens go to war with each other (Waltz 1959; Fearon and Laitin 2003)? Why do some wars and conflicts last longer and cause more suffering and loss than others (Balch-Lindsay and Enterline 2000)? What can be done to end wars and conflicts, and to stabilize conflict-prone countries (Fjelde and Smidt 2022)? Many political scientists, especially those in the subfield of peace and conflict research, care deeply about these and similar questions and try to find answers.\nTo do so, they can nowadays rely on a whole array of datasets that measure the occurrence, intensity, and duration of different types of violent conflicts within and between countries. Examples include the Uppsala Conflict Data Program (UCDP; https://ucdp.uu.se/), the Armed Conflict Location & Event Data project (ACLED; https://acleddata.com/), the Correlates of War project (COW; https://correlatesofwar.org/), or the International Conflict Data Project (https://internationalconflict.ua.edu/).1\nMany of these datasets have relatively complicated structures, where the unit of analysis is a conflict-year or a single conflict event, and this can make them more difficult (but certainly not impossible) to work with for junior (political) data analysts. But there are some datasets that have simpler structures that are easier to work with, notably the typical country-year structure that is commonly used in comparative politics and comparative political economy (as introduced in this post).\nOne of these easier-to-work with datasets is the Country-Year Dataset on Organized Violence within Country Borders from the Uppsala Conflict Data Program (Davies et al. 2025; Sundberg and Melander 2013), which contains information about the number of fatalities and the actors involved in different types of conflicts (intrastate, interstate, and non-state violence, government and non-state group killings) in a large number of countries between 1989 and 2024.\nThe remainder of this post will show you how you can do some simple (descriptive) analyses with this dataset."
  },
  {
    "objectID": "posts/conflict/index.html#conflicts-violence-and-data",
    "href": "posts/conflict/index.html#conflicts-violence-and-data",
    "title": "Studying conflict and violence across countries",
    "section": "",
    "text": "Why do countries or fellow citizens go to war with each other (Waltz 1959; Fearon and Laitin 2003)? Why do some wars and conflicts last longer and cause more suffering and loss than others (Balch-Lindsay and Enterline 2000)? What can be done to end wars and conflicts, and to stabilize conflict-prone countries (Fjelde and Smidt 2022)? Many political scientists, especially those in the subfield of peace and conflict research, care deeply about these and similar questions and try to find answers.\nTo do so, they can nowadays rely on a whole array of datasets that measure the occurrence, intensity, and duration of different types of violent conflicts within and between countries. Examples include the Uppsala Conflict Data Program (UCDP; https://ucdp.uu.se/), the Armed Conflict Location & Event Data project (ACLED; https://acleddata.com/), the Correlates of War project (COW; https://correlatesofwar.org/), or the International Conflict Data Project (https://internationalconflict.ua.edu/).1\nMany of these datasets have relatively complicated structures, where the unit of analysis is a conflict-year or a single conflict event, and this can make them more difficult (but certainly not impossible) to work with for junior (political) data analysts. But there are some datasets that have simpler structures that are easier to work with, notably the typical country-year structure that is commonly used in comparative politics and comparative political economy (as introduced in this post).\nOne of these easier-to-work with datasets is the Country-Year Dataset on Organized Violence within Country Borders from the Uppsala Conflict Data Program (Davies et al. 2025; Sundberg and Melander 2013), which contains information about the number of fatalities and the actors involved in different types of conflicts (intrastate, interstate, and non-state violence, government and non-state group killings) in a large number of countries between 1989 and 2024.\nThe remainder of this post will show you how you can do some simple (descriptive) analyses with this dataset."
  },
  {
    "objectID": "posts/conflict/index.html#setup-data-import",
    "href": "posts/conflict/index.html#setup-data-import",
    "title": "Studying conflict and violence across countries",
    "section": "Setup & data import",
    "text": "Setup & data import\n\nSetup\nThis is a relatively simple descriptive analysis, so we just load the tidyverse package collection (and we can already set a default theme for all ggplot graphs).\n\nlibrary(tidyverse)\ntheme_set(theme_classic())\n\n\n\nAccessing and importing the dataset\nYou can access and download the dataset (as a ZIP file) via https://ucdp.uu.se/downloads/ (under “Other datasets”). Ideally, use the R version (.RDS), unpack the ZIP file, and save the dataset somehwere where you can easily find it again.\nTo import it, you can use the native R function to import .RDS files, readRDS():\n\nged &lt;- readRDS(\"organizedviolencecy_v25_1.rds\")"
  },
  {
    "objectID": "posts/conflict/index.html#exploring-the-ucpd-conflict-dataset",
    "href": "posts/conflict/index.html#exploring-the-ucpd-conflict-dataset",
    "title": "Studying conflict and violence across countries",
    "section": "Exploring the UCPD conflict dataset",
    "text": "Exploring the UCPD conflict dataset\nAs shown in some of the other posts, we should first get an overview over which countries are covered:\n\nunique(ged$country_cy)\n\n  [1] \"Afghanistan\"                      \"Albania\"                         \n  [3] \"Algeria\"                          \"Andorra\"                         \n  [5] \"Angola\"                           \"Antigua & Barbuda\"               \n  [7] \"Argentina\"                        \"Armenia\"                         \n  [9] \"Australia\"                        \"Austria\"                         \n [11] \"Azerbaijan\"                       \"Bahamas\"                         \n [13] \"Bahrain\"                          \"Bangladesh\"                      \n [15] \"Barbados\"                         \"Belarus\"                         \n [17] \"Belgium\"                          \"Belize\"                          \n [19] \"Benin\"                            \"Bhutan\"                          \n [21] \"Bolivia\"                          \"Bosnia-Herzegovina\"              \n [23] \"Botswana\"                         \"Brazil\"                          \n [25] \"Brunei\"                           \"Bulgaria\"                        \n [27] \"Burkina Faso\"                     \"Burundi\"                         \n [29] \"Cambodia (Kampuchea)\"             \"Cameroon\"                        \n [31] \"Canada\"                           \"Cape Verde\"                      \n [33] \"Central African Republic\"         \"Chad\"                            \n [35] \"Chile\"                            \"China\"                           \n [37] \"Colombia\"                         \"Comoros\"                         \n [39] \"Congo\"                            \"Costa Rica\"                      \n [41] \"Croatia\"                          \"Cuba\"                            \n [43] \"Cyprus\"                           \"Czech Republic\"                  \n [45] \"Czechoslovakia\"                   \"DR Congo (Zaire)\"                \n [47] \"Denmark\"                          \"Djibouti\"                        \n [49] \"Dominica\"                         \"Dominican Republic\"              \n [51] \"East Timor\"                       \"Ecuador\"                         \n [53] \"Egypt\"                            \"El Salvador\"                     \n [55] \"Equatorial Guinea\"                \"Eritrea\"                         \n [57] \"Estonia\"                          \"Ethiopia\"                        \n [59] \"Federated States of Micronesia\"   \"Fiji\"                            \n [61] \"Finland\"                          \"France\"                          \n [63] \"Gabon\"                            \"Gambia\"                          \n [65] \"Georgia\"                          \"German Democratic Republic\"      \n [67] \"Germany\"                          \"Ghana\"                           \n [69] \"Greece\"                           \"Grenada\"                         \n [71] \"Guatemala\"                        \"Guinea\"                          \n [73] \"Guinea-Bissau\"                    \"Guyana\"                          \n [75] \"Haiti\"                            \"Honduras\"                        \n [77] \"Hungary\"                          \"Iceland\"                         \n [79] \"India\"                            \"Indonesia\"                       \n [81] \"Iran\"                             \"Iraq\"                            \n [83] \"Ireland\"                          \"Israel\"                          \n [85] \"Italy\"                            \"Ivory Coast\"                     \n [87] \"Jamaica\"                          \"Japan\"                           \n [89] \"Jordan\"                           \"Kazakhstan\"                      \n [91] \"Kenya\"                            \"Kingdom of eSwatini (Swaziland)\" \n [93] \"Kiribati\"                         \"Kosovo\"                          \n [95] \"Kuwait\"                           \"Kyrgyzstan\"                      \n [97] \"Laos\"                             \"Latvia\"                          \n [99] \"Lebanon\"                          \"Lesotho\"                         \n[101] \"Liberia\"                          \"Libya\"                           \n[103] \"Liechtenstein\"                    \"Lithuania\"                       \n[105] \"Luxembourg\"                       \"Madagascar (Malagasy)\"           \n[107] \"Malawi\"                           \"Malaysia\"                        \n[109] \"Maldives\"                         \"Mali\"                            \n[111] \"Malta\"                            \"Marshall Islands\"                \n[113] \"Mauritania\"                       \"Mauritius\"                       \n[115] \"Mexico\"                           \"Moldova\"                         \n[117] \"Monaco\"                           \"Mongolia\"                        \n[119] \"Montenegro\"                       \"Morocco\"                         \n[121] \"Mozambique\"                       \"Myanmar (Burma)\"                 \n[123] \"Namibia\"                          \"Nauru\"                           \n[125] \"Nepal\"                            \"Netherlands\"                     \n[127] \"New Zealand\"                      \"Nicaragua\"                       \n[129] \"Niger\"                            \"Nigeria\"                         \n[131] \"North Korea\"                      \"North Macedonia\"                 \n[133] \"Norway\"                           \"Oman\"                            \n[135] \"Pakistan\"                         \"Palau\"                           \n[137] \"Panama\"                           \"Papua New Guinea\"                \n[139] \"Paraguay\"                         \"Peru\"                            \n[141] \"Philippines\"                      \"Poland\"                          \n[143] \"Portugal\"                         \"Qatar\"                           \n[145] \"Romania\"                          \"Russia (Soviet Union)\"           \n[147] \"Rwanda\"                           \"Saint Kitts and Nevis\"           \n[149] \"Saint Lucia\"                      \"Saint Vincent and the Grenadines\"\n[151] \"Samoa (Western Samoa)\"            \"San Marino\"                      \n[153] \"Sao Tome and Principe\"            \"Saudi Arabia\"                    \n[155] \"Senegal\"                          \"Serbia (Yugoslavia)\"             \n[157] \"Seychelles\"                       \"Sierra Leone\"                    \n[159] \"Singapore\"                        \"Slovakia\"                        \n[161] \"Slovenia\"                         \"Solomon Islands\"                 \n[163] \"Somalia\"                          \"South Africa\"                    \n[165] \"South Korea\"                      \"South Sudan\"                     \n[167] \"Spain\"                            \"Sri Lanka\"                       \n[169] \"Sudan\"                            \"Suriname\"                        \n[171] \"Sweden\"                           \"Switzerland\"                     \n[173] \"Syria\"                            \"Taiwan\"                          \n[175] \"Tajikistan\"                       \"Tanzania\"                        \n[177] \"Thailand\"                         \"Togo\"                            \n[179] \"Tonga\"                            \"Trinidad and Tobago\"             \n[181] \"Tunisia\"                          \"Turkey\"                          \n[183] \"Turkmenistan\"                     \"Tuvalu\"                          \n[185] \"Uganda\"                           \"Ukraine\"                         \n[187] \"United Arab Emirates\"             \"United Kingdom\"                  \n[189] \"United States of America\"         \"Uruguay\"                         \n[191] \"Uzbekistan\"                       \"Vanuatu\"                         \n[193] \"Vatican City State\"               \"Venezuela\"                       \n[195] \"Vietnam (North Vietnam)\"          \"Yemen (North Yemen)\"             \n[197] \"Yemen (South Yemen)\"              \"Zambia\"                          \n[199] \"Zimbabwe (Rhodesia)\"             \n\n\nPretty much the entire world – impressive.\nNext, we can check for how many years each country is covered (as also shown here):\n\nged %&gt;% \n  group_by(country_cy) %&gt;% \n  summarise(start_year = min(year_cy, na.rm = T),\n            end_year = max(year_cy, na.rm = T)) %&gt;% \n  mutate(year_range = end_year - start_year) %&gt;% \n ggplot(aes(xmin = start_year, xmax = end_year, \n            y = reorder(country_cy, year_range))) +\n    geom_linerange(linewidth = 3, color = \"grey30\") +\n    labs(y = \"\")\n\n\n\n\n\n\n\n\nThis graph is a bit messy and difficult to read, but is clear that most countries are covered for the entire period. Let’s have a closer look at those that have less than full coverage:\n\nged %&gt;% \n  group_by(country_cy) %&gt;% \n  summarise(start_year = min(year_cy, na.rm = T),\n            end_year = max(year_cy, na.rm = T)) %&gt;% \n  mutate(year_range = end_year - start_year) %&gt;% \n  filter(year_range&lt;(2024-1989)) %&gt;% \n ggplot(aes(xmin = start_year, xmax = end_year, \n            y = reorder(country_cy, year_range))) +\n    geom_linerange(linewidth = 3, color = \"grey30\") +\n    labs(y = \"\")\n\n\n\n\n\n\n\n\nHere again, only a few countries are covered for substantively shorter time periods than the others. One of them is the no longer existing German Democratic Republic (“East Germany”). To make things simpler, we can just drop these countries from the dataset:\n\nged %&gt;% \n  filter(!(country_cy %in% c(\"German Democratic Republic\",\"Yemen (South Yemen)\",\"Czechoslovakia\"))) -&gt; ged\n\n\nConflict fatalities over time and across countries\nNow that we have an overview over the main dimensions of the dataset, we can get to the actual contents. If you have a look at the codebook, you’ll see that the dataset contains mainly estimates of the number of deaths (“fatalities”) associated with different types of violence within a given country and in a given year (see also Sundberg and Melander 2013):\n\nState-based violence (variables starting with sb_), meaning conflicts between two organized actors where one is a government. This is further divided into:\n\nIntra-state conflicts (indicated with intrastate)\nInter-state conflicts (indicated with interstate)\n\nNon-state violence (variables starting with ns_), meaning conflicts between two or more groups that are not state or government actors\nOne-sided violence against unarmed civilians (variables starting with os_), which is further divided according to who perpetrated it:\n\nKillings by the government of a given country (indicated with gvt_killings)\nKillings by any state actor in a given country (indicated with any_gvt_killings)\nKillings by non-state groups in a given country (indicated with nsgroup_killings)\n\n\nThere are also estimates of the total number of fatalities for each of the major types of violence (ns, sb, and os), indicated with total in the variable name.\nSince, as a famous saying goes, “the first casuality in war is the truth”, the fatality numbers in the dataset are estimates, and at least the main variables are available as best, low, and high estimates so that users can decide for themselves which estimate(s) they want to rely on.\nIn the illustrative analysis here, we’ll focus on the total number of fatalities from state-based violence (both between states and between states and non-state actors) in a given country and year, and here the best estimate – i.e., the sb_total_deaths_best_cy variable.\n\nConflict fatalities over time\nLet’s first have a look at the variation in conflict fatalities over time – i.e., which years since 1989 were the bloodiest in terms of state-based violence?\nIf you read the other post on how to work with cross-country macro-level data, you might have an idea as to how to do this: We aggregate the data by year (year_cy) and then use summarize() to calculate relevant summary statistics (here the overall sum or total of the sb_total_deaths_best_cy variable) for each year. Once we have that, we visualize the result with ggplot(), and we use the pipe operator (%&gt;%) to do all of this in a single operation or “pipeline”:\n\nged %&gt;% \n  group_by(year_cy) %&gt;% \n  summarise(tdeaths = sum(sb_total_deaths_best_cy, na.rm = T)) %&gt;% \n  ggplot(aes(x = year_cy, y = tdeaths)) +\n    geom_bar(stat = \"identity\") +\n    scale_y_continuous(breaks = seq(0,300000,50000),\n                       limits = c(0,300000),\n                       labels=function(tdeaths) format(tdeaths, \n                                                       big.mark = \"'\", \n                                                       scientific = FALSE)) +\n    labs(x = \"\", y = \"Total fatalities from state-based violence (best estimate)\")\n\n\n\n\n\n\n\n\nNotice that we adapt the y-scale with scale_y_continuous() to range from 0 to 500’000 and we add a single quotation mark to indicate thousands in the labels.\nOverall, global state-based fatalities declined intitially from the 1990s to the 2000s (despite the onset of the Global War on Terror after 2001), and then increased noticeably again in the 2010s and especially in 2021 and 2022.\nTo get a better sense of what is behind this variation, we can calculate fatality numbers by both year and world region (region_cy) and then visualize the result in the form of a stacked bar plot (notice the position = \"stack\" option within geom_bar()) and the scale_fill_viridis_d() setting to use the colorblind-friendly viridis color scale for discrete variables.\n\nged %&gt;% \n  group_by(year_cy,region_cy) %&gt;% \n  summarise(tdeaths = sum(sb_total_deaths_best_cy, na.rm = T)) %&gt;% \n  ggplot(aes(x = year_cy, y = tdeaths, fill = region_cy)) +\n    geom_bar(stat = \"identity\", position = \"stack\") +\n    scale_fill_viridis_d() +\n    scale_y_continuous(breaks = seq(0,300000,50000),\n                       limits = c(0,300000),\n                       labels=function(tdeaths) format(tdeaths, \n                                                       big.mark = \"'\", \n                                                       scientific = FALSE)) +\n    labs(x = \"\", y = \"Total fatalities from state-based violence (best estimate)\",\n         fill = \"\") +\n    theme(legend.position = \"bottom\")\n\n`summarise()` has grouped output by 'year_cy'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nWhen we disaggregate the figures by region, we can get a better sense of what is going on in the data. During the 1990s, many state-based fatalities occurred in Africa, but the second Gulf War in 1991 (between the US-led coalition and Iraq under Saddam Hussein) is clearly visible, as are the protracted conflicts in the Balkan countries following the collapse of Jugoslavia in the mid-1990s. During the 2000s, Asia and the Middle East were most strongly affected, which arguably reflects the Global War on Terror and the US-led interventions in Afghanistan and Iraq. The 2010s are clearly dominated by conflicts in the Middle East, which reflects the aftermath of the Arab Spring and the resulting conflicts in Syria, Iraq, and Libya. Russia’s invasion of Ukraine in 2022 is clearly visible as a massive rise in fatalities in Europe, and so is the conflict in Ethiopia in 2021 and 2022 as a rise in fatalities in Africa.\n\n\nConflict fatalities across countries\nWe can check if the interpretation of the over-time graphs above makes sense by looking more in detail at the variation across countries. The main change we need to make is to group the data by country (country_cy) so that summarize() calculates summary statistics by country. Then we also filter the data so that countries that never had any fatalities due to state-based violence between 1989 and 2024 are excluded – this avoids the earlier issue that the graph becomes unreadable when too many countries are shown on the y-axis.\n\nged %&gt;% \n  group_by(country_cy) %&gt;% \n  summarise(tdeaths = sum(sb_total_deaths_best_cy, na.rm = T)) %&gt;% \n  filter(tdeaths&gt;0) %&gt;% \n  ggplot(aes(x = tdeaths, y = reorder(country_cy, -tdeaths))) +\n    geom_col() +\n    scale_x_continuous(breaks = seq(0,500000,100000),\n                       limits = c(0,500000),\n                       labels=function(tdeaths) format(tdeaths, \n                                                       big.mark = \"'\", \n                                                       scientific = FALSE)) +\n    labs(x = \"Total fatalities from state-based violence\",\n         y = \"\", caption = \"1989-2024; best estimate\")\n\n\n\n\n\n\n\n\nThe resulting bar graph covers still many countries, but it also shows that fatalities from state-based violence are quite concentrated in a handful of countries, mainly Ukraine, Afghanistan, Syria, and Ethiopia. The group of high-fatality countries includes exclusively countries in Africa, the Middle East, Asia, and the post-Soviet world, but when we go up to countries that had relatively few fatalities, we find some high-income democracies from the “Global North” such as the United States, the United Kingdom, Germany, Belgium, or Australia.\nFinally, we can also show disaggregated over-time trends of conflict fatalities in a given country. To do so, we don’t group and summarize the data but create separate graphs per country with facet_wrap(). It can make sense to do this with the entire dataset, but it is often better to do this only with a selected number of countries and time points to avoid getting many very tiny and unreadable graphs. In this case, we use filter() to limit the time period to 2020 and after and focus on observations (country-years) that had more than 1000 fatalities.\n\nged %&gt;% \n  filter(year_cy&gt;=2020 & sb_total_deaths_best_cy&gt;=1000) %&gt;% \n  ggplot(aes(x = year_cy, y = sb_total_deaths_best_cy)) +\n    geom_col() +\n    facet_wrap(~country_cy) +\n    scale_y_continuous(breaks = seq(0,200000,50000),\n                       limits = c(0,200000),\n                       labels=function(tdeaths) format(tdeaths, \n                                                       big.mark = \"'\", \n                                                       scientific = FALSE)) +\n    labs(x = \"\", y = \"Total fatalities from state-based violence (best estimate)\")\n\n\n\n\n\n\n\n\nThe resulting graph shows some of the major developments in international relations of the last years, including the drawdown of the US-led intervention in Afghanistan, the conflicts in Ethiopia and Yemen, the conflict in Israel following the October 7 attacks by Hamas in 2023, and Russia’s invasion of Ukraine."
  },
  {
    "objectID": "posts/conflict/index.html#next-steps",
    "href": "posts/conflict/index.html#next-steps",
    "title": "Studying conflict and violence across countries",
    "section": "Next steps",
    "text": "Next steps\nIf you are now thinking something like “well, this was not really more advanced that working with any other macro-level dataset”, then you are correct. That was precisely the point. This particular the UCDP dataset is just a typical country-year macro-level dataset, just like the Comparative Political Dataset or many other datasets used in comparative politics and comparative political economy. Therefore, if you can work with one of them, you can potentially work with all of them. The main difficulty is really only to understand what the different variables in the dataset measure and which one is the most relevant for a given research question or hypothesis – and this is why we have codebooks and research articles that present datasets and what they contain.\nAn obvious next step is now to try to explain some of the variation in conflict fatalities by looking, for example, at their relation to how democratic a country is (Maoz and Russett 1993; Oneal and Russett 1997) or their welfare states (Burgoon 2006), or to see what political consequences conflicts and fatalities can have (e.g., Obinger and Schmitt 2020). To be able to do so, you need to merge the UCDP data with some relevant other datasets, for example the V-DEM data on democracy (Lindberg et al. 2014; Coppedge et al. 2011) or the Quality of Government dataset (Teorell et al. 2025) that contains a wide range of variables on political actors, institutions, and public policies.\nTo merge these data, you just need to make sure that all countries are named or coded exactly equally (e.g., that the United States are called “United States” in both datasets, and not “US”, “USA”, or “United States of America” in one but not the other). The countrycode package can help with that (Arel-Bundock, Enevoldsen, and Yetman 2018). After that, you just use left_join() to combine the datasets by country and year (see also this post).\nIf you then want to do more advanced regression analyses with the combined datasets, you can have a look at the book by Urdinez & Cruz (2020)."
  },
  {
    "objectID": "posts/conflict/index.html#footnotes",
    "href": "posts/conflict/index.html#footnotes",
    "title": "Studying conflict and violence across countries",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee also https://github.com/erikgahner/PolData?tab=readme-ov-file#international-relations for a more exhaustive lists of datasets and sources on war, conflict, and international relations.↩︎"
  },
  {
    "objectID": "posts/datasources/index.html",
    "href": "posts/datasources/index.html",
    "title": "‘I can’t find any data!’",
    "section": "",
    "text": "Finding relevant data is often one of the major hurdles students face during term paper or thesis projects — which is a bit puzzling given that there is a huge and growing amount of data about many political, social, or economic aspects freely available on the open internet.\nThis post provides a (definitely not exhaustive) list of datasets and data repositories that can be relevant for research projects in the social and political sciences. It is not the first such list (see below), but it complements other lists with a stronger focus on datasets from and relevant for research on welfare states.\nLinks to most data sources are provided, those without links should be easy to find. The list and links are updated every once in a while."
  },
  {
    "objectID": "posts/datasources/index.html#forests-and-trees",
    "href": "posts/datasources/index.html#forests-and-trees",
    "title": "‘I can’t find any data!’",
    "section": "",
    "text": "Finding relevant data is often one of the major hurdles students face during term paper or thesis projects — which is a bit puzzling given that there is a huge and growing amount of data about many political, social, or economic aspects freely available on the open internet.\nThis post provides a (definitely not exhaustive) list of datasets and data repositories that can be relevant for research projects in the social and political sciences. It is not the first such list (see below), but it complements other lists with a stronger focus on datasets from and relevant for research on welfare states.\nLinks to most data sources are provided, those without links should be easy to find. The list and links are updated every once in a while."
  },
  {
    "objectID": "posts/datasources/index.html#other-better-overviews-over-available-data",
    "href": "posts/datasources/index.html#other-better-overviews-over-available-data",
    "title": "‘I can’t find any data!’",
    "section": "Other (better?) overviews over available data",
    "text": "Other (better?) overviews over available data\nErik Gahner’s excellent Dataset of Political Datasets provides a great overview over datasets that are relevant for political science research.\nIn addition, Google has a dedicated search tool for research datasets: https://datasetsearch.research.google.com/. Be aware, though, that this tool is not specific to political science or other social sciences – depending on what you look for, your search results may include lots of datasets from medical or biological research, which will most likely not be relevant for your project. You can use a filter function to limit the results to datasets from social and political science.\nFinally, there are also a number of data repositories and archives that contain even more (and partly older) data that might be relevant for some political and social science projects, for example:\n\nThe GESIS survey data repository for survey data\nThe ICPSR data repository\nThe Harvard Dataverse that contains research and replication data\nThe Norwegian Norsk senter for forskningsdata (NSD) or Datafabrikken."
  },
  {
    "objectID": "posts/datasources/index.html#survey-data",
    "href": "posts/datasources/index.html#survey-data",
    "title": "‘I can’t find any data!’",
    "section": "Survey data",
    "text": "Survey data\n\nGeneral social survey data\n\nEuropean Social Survey (ESS): https://europeansocialsurvey.org/\nInternational Social Survey Program (ISSP): https://issp.org/\nEurobarometer: Public attitudes toward large range of topics; see https://www.gesis.org/en/eurobarometer-data-service/search-data-access/topics\nArab Barometer: Public opinion data from MENA countries (https://www.arabbarometer.org/)\nWorld Values Survey (WVS)\nUS General Social Survey (GSS)\nPEW polling data (partly avaiable for free, after registration): https://www.pewresearch.org/tools-and-resources/1\n\n\n\nElection/voter survey data\n\nComparative Study of Electoral Systems (http://www.cses.org/): Standardized election surveys from several countries and elections\nAmerican National Election Study (http://www.electionstudies.org/)\nNorwegian National Election Study/Norsk Valgundersøkelser (available via NSD; see above)\nUS VOTER survey data: Unique longitudinal survey data allowing for deep exploration of public opinion on the issues and values that drive voter behavior.\n\n\n\nSocietal safety & crisis preparedness\n\nUS Federal Emergency Management Agency (FEMA) National Household Survey: Representative survey data on the American public’s preparedness actions, attitudes, and motivations.\nSeveral Special Eurobarometer surveys from different years look at attitudes toward disaster prevention and civil protection: See https://europa.eu/eurobarometer/surveys/browse/all; search in Keywords for “Civil protection”; access data via GESIS’ ZACAT Data Archive (see above)\nThe 2016 round of the International Social Survey Program (https://www.gesis.org/en/issp/modules/issp-modules-by-topic/role-of-government/2016) included questions on attitudes toward national security and anti-terror measures such as video surveillance or detention of terrorist suspects.\n\n\n\nOther\n\nThe Public Opinion Quarterly provides reviews of recent polling data on specific topics (e.g., De Boer 1983); search for “The Polls” or “Poll Reviews”\nEurostat Survey Data: Includes e.g. the European Labour Force Survey (ELFS), European Community Household Panel (ECHP), European Union Statistics on Income and Living Conditions (EU-SILC),…}\nSurvey of Health, Ageing and Retirement in Europe (SHARE)\nEvidence for Equality National Survey (EVENS): Survey data on the experiences of ethnic minorities in the UK during the COVID-19 pandemic (see also Finney et al. 2023)\nVarious national & international panel surveys (German SOEP, British BHPS, Swiss SHP, Swedish LNU, U.S. PSID, Japanese JHPS,…)"
  },
  {
    "objectID": "posts/datasources/index.html#cross-national-comparative-data",
    "href": "posts/datasources/index.html#cross-national-comparative-data",
    "title": "‘I can’t find any data!’",
    "section": "Cross-national comparative data",
    "text": "Cross-national comparative data\n\nInternational organizations\n\nOECD: https://www.oecd.org/en/data.html\nWorld Bank: https://data.worldbank.org/ (see also the WDI package for R at https://github.com/vincentarelbundock/WDI)\nIMF: https://www.imf.org/en/Data\nILO: https://ilostat.ilo.org/\nEurostat: https://ec.europa.eu/eurostat\nUN: https://data.un.org/\nUNESCO: https://databrowser.uis.unesco.org/\nUNHCR: https://www.unhcr.org/what-we-do/reports-and-publications/unhcr-data\nWHO: https://data.wto.org/en\n\n\n\nComparative politics: Governments, parliaments, parties, institutions, elections\n\nManifesto Project Database: Ideological positions of political parties (https://manifestoproject.wzb.eu/)\nIntegrated Party Organization Dataset (http://dx.doi.org/10.7910/DVN/PE8TWP)\nPARTY FACTS: Repository of comparative and historical data on political parties in around 200 countries (https://partyfacts.herokuapp.com/)\nParliament and government composition (PARLGOV; https://www.parlgov.org/) database\nInter-Parliamentary Union: Qualitative descriptions of parliaments and electoral systems, and key statistics on the composition of parliaments in most countries of the world (https://www.ipu.org/)\nPOLCON: Index of political constraints (“checks & balances”), information about heads of states, governments, government parties (https://mgmt.wharton.upenn.edu/faculty/heniszpolcon/polcondataset/)\nVarieties of Democracy (V-DEM): Indicators of democracy, very detailed and for 177 countries between 1900 and today (https://www.v-dem.net/)\nExecutive Approval Project: Cross-country comparative data on public support for political executives (http://www.executiveapproval.org/)\nDemocratic Electoral Systems Around the World, 1946-2011 (see also Golder 2005; Bormann and Golder 2013)\nEuropean Journal of Political Research Political Data Yearbook: Qualitative info on elections, government composition, important issues in national politics (http://onlinelibrary.wiley.com/journal/10.1111/%28ISSN%292047-8852)\nInternational Institute for Democracy and Electoral Assistance (IDEA): Comparative (often qualitative) data on electoral systems, campaign finance rules, direct democracy, gender quotas, voter turnout,… (https://www.idea.int/)\nGlobal Leadership Project (GLP): Data on government leaders throughout the world - including legislators, members of the executive branch, members of the judiciary, and other decisionmakers whose power may be formal or informal (https://globalleadershipproject.net/)\nCline Center Historical Phoenix Event Data: Machine-generated data on historical events between 1945 and 2015 extracted from 14 million news stories. It documents the agents, locations, and issues at stake in a wide variety of conflict, cooperation and communicative events.\nThe Electoral Integrity Project: Data from expert surveys on the integrity of elections in countries around the globe (https://www.electoralintegrityproject.com/)\nPPEG Database on political parties, presidents, elections, and governments around the world (https://ppeg.wzb.eu/)\nCongress in Data: Information about the characteristics of US Congress Members, their legislative activities, and their social connections over five US Congress cycles: from the 109th to the 113th (2005-2015)\nComparative Legislators Database (CLD)}: Rich, diverse and integrated individual-level data on national political representatives. The database contains information for over 67,000 contemporary and historical legislators from 16 countries (see also Göbel and Munzert 2022)\nWhoGov: Information (e.g., gender, party affiliation) on cabinet members in July every year in the period 1966-2021 in all countries with a population of more than 400,000 citizens (see also Nyrup and Bramwell 2020)\nParliaments Day-By-Day: Open-source data on MPs’ membership in parties, parliaments, and party groups (see also Turner-Zwinkels et al. 2022)\n\n\n\nWelfare state, social inequality, & social policies\n\nThe Social Policy Indicators (SPIN) database\nGlobal Welfare State Information System (WeSIS): comprehensive data to describe and explain social policy worldwide\nWorld Inequality Database (WID): global database on the distribution of wealth and income (see also Piketty 2014)\nComparative Welfare Entitlements Project (CWEP): Comparative data on pension, unemployment, and sickness insurance generosity\nMoira Nelson’s and my comparative Unemployment Benefit Conditionality Dataset: Comparative data on job-search requirements, the definition of suitable work, and sanctions for unemployment benefit claimants in 21 OECD countries between 1980 and 2012. See also the OECD’s dataset on benefit eligibility requirements for more recent data\nJohn Stephens & Evelyne Huber’s Comparative Welfare States & Social Policy in Latin America Datasets\nMISSOC: Qualitative information on social security and assistance schemes in EU countries (https://www.missoc.org/)\nOECD Benefits & Wages: Qualitative information on social security and social assistance schemes in OECD countries): Qualitative information on social security and social assistance schemes in OECD countries\nInternational Network on Leave Policies & Research: Qualitative reports on maternal, paternal, and parental leave schemes (https://www.leavenetwork.org/annual-review-reports/)\nEuropean Trade Union Institute (ETUI) Reforms Watch: Individual EU country dossiers with fact-based information on the state of labour market reforms, the state of pension reforms, developments in legislation on strikes and data on strike activities\nSocial Assistance in Developing Countries Database: Summary information on social assistance interventions in developing countries (https://www.social-protection.org/gimi/gess/ShowRessource.action?ressource.ressourceId=9491)\nGlobal Welfare (GLOW): The database includes 381 variables on 61 countries from years between 1989 and 2015. The database has four main categories of data: welfare, development, economy and politics\nEducation Policies and Systems across Modern History EPSM dataset: measures on compulsory education, ideological guidance and content of education, governmental intervention and level of education centralization, and teacher training covering 157 countries with populations exceeding 1 million people from 1789 to the present (see also Del Rı́o, Knutsen, and Lutscher 2024)\nGlobal Tax Expenditures Database GTED: Information on preferential tax treatments such as exemptions, deductions, credits, deferrals and reduced tax rates that are implemented by governments worldwide to promote different policy goals.\nBarro-Lee Educational Attainment data: Data on educational attainment per country across the world between 1950 and 2010 (see also Barro and Lee 2013)\nSee also Clasen and Siegel (2007), Clasen, Clegg, and Goerne (2016), and a special issue in the Journal of European Public Policy (Wenzelburger, Zohlnhöfer, and Wolf 2013; Danforth and Stephens 2013; Scruggs 2013) on the “dependent variable problem” in comparative social policy research\n\n\n\nImmigration & immigrant integration\n\nCitizenship rights of immigrants (see also Koopmans, Michalowski, and Waibel 2012)\nMigrant Integration Policy Index MIPEX\nMigration Data Portal: information on the degree of restrictiveness of immigration policies in 33 OECD countries for the period 1980–2010\nImmigration Policies in Comparison: Quantitative indices to measure immigration policies in all OECD countries and for the time period 1980-2010. See also Helbling (2013) and Helbling and Michalowski (2017)\nImmigration in Party Manifestos Dataset (https://manifesto-project.wzb.eu/information/documents/pimpo)\n\n\n\nTechnology & technological change\n\nOECD Risks that Matter surveys: Cross-country comparative survey data, incl. questions on perceived vulnerability to technological change (see e.g., Busemeyer et al. 2023; Knotz et al. 2024; Knotz 2025)\nEU KLEMS: Economic dataset that includes data on ICT investment for many countries and years (used by e.g., Gallego, Kurer, and Schöll 2022)\nAI exposure indicators: Indicators measuring exposure to AI at the occupational, industry, and regional (for US) level; see Felten, Raj, and Seamans (2021) for details\nKnowledge Economy Index: An index that measures the degree to which countries have transitioned toward a knowledge-based economy. The index covers 22 economically advanced countries between 1995 and 2019 (see also Diessner et al. 2025).\n\n\n\nSocial media\n\nTweetsKB: A database of annotated tweets, containing data for nearly 3.0 billion tweets between February 2013 - August 2022. Metadata information about the tweets as well as extracted entities, sentiments, hashtags and user mentions are exposed in RDF using established RDF/S vocabularies (see also Fafalios et al. 2018)\nThe Twitter Parliamentarian Database: a database consisting of parliamentarian names, parties and twitter ids from the following countries: Austria, Belgium, France, Denmark, Spain, Finland, Germany, Greece, Italy, Malta, Poland, Netherlands, United Kingdom, Ireland, Sweden, New Zealand, Turkey, United States, Canada, Australia, Iceland, Norway, Switzerland, Luxembourg, Latvia and Slovenia. In addition, the database includes the European Parliament\n\n\n\nTrade unions & industrial relations\n\nDatabase on Institutional Characteristics of Trade Unions, Wage Setting, State Intervention and Social Pacts ICTWSS\nEuropean Works Councils Database: Information on works councils in European and multinational companies, on EU and national-level works council legislation, and on relevant court cases (EU and national level)\nEuropean Company Survey: Comparative survey on businesses in Europe (see also Lehr, Jansen, and Brandl 2023)\nEuropean Observatory of Working Life (EurWORK)"
  },
  {
    "objectID": "posts/datasources/index.html#footnotes",
    "href": "posts/datasources/index.html#footnotes",
    "title": "‘I can’t find any data!’",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee also https://www.pewresearch.org/short-reads/2021/10/22/how-to-access-pew-research-center-survey-data/ for more details.↩︎"
  },
  {
    "objectID": "posts/compa_survey/index.html",
    "href": "posts/compa_survey/index.html",
    "title": "Comparing people’s behavior and attitudes across countries",
    "section": "",
    "text": "Are people’s opinions, behaviors, or perceptions affected by the environments (or “macro-level contexts”) they live in? Social and economic scientists often argue that this is indeed the case. For example, many sociologists argue that societal norms shape people’s behavior (e.g., that men do more of the housework in societies with egalitarian gender norms), and political scientists similarly suggest that political institutions influence political attitudes and behavior (e.g., that people participate more in elections in proportional or majoritarian electoral systems).\nObviously, these theories are only as good as the evidence in support of them. To test these types of theories, one needs to compare people’s opinions or behavior across contexts with different social norms, political institutions, or other macro-level factors that might have an influence on people. This is usually done with comparative survey data such as data from the European Social Survey, the International Social Survey Program, the Eurobarometer, the OECD Risks that Matter survey, or the World Values Study.1 The big advantage that comparative survey data offer is that they are standardized: The same survey with the exact same questions is conducted in multiple countries at the same time, so that people’s responses to the questions – i.e., their attitudes or behavior – can be directly compared.2 This means that one can use these survey data to find out for macro-level environmental factors influence (or at least are correlated with) people’s individual behaviors and attitudes.\nThis type of cross-country comparative analysis can seem daunting to students who have only learned about relatively simple methods (linear regression, graphical analyses). However, as always in life, there are easier and more complicated ways of doing this. This post shows you how to do it in the easiest way possible, using R and techniques that undergraduate students usually learn in their introductory statistics courses: Descriptive statistics and linear regression models (taking inspiration from Blekesaune and Bjørkhaug 2021).\n\n\n\n\n\n\nTL;DR\n\n\n\n\nYou pick a cross-country survey dataset (e.g., from the European Social Survey, the World Values Survey, the International Social Survey Program, or the Euro-, Afro-, or Arabbarometer) that contains relevant variables/questions for multiple countries that could be relevant to compare.\nYou select a small number (at least two and up to four or five) countries that differ in relevant ways in relevant macro-level aspects (e.g., one has overall traditional gender norms, the other is very progressive) but are otherwise as similar as possible so that you can rule out as many alternative explanations as possible.3\nYou analyze the data from each of the countries separately using standard techniques (e.g., graphs, statistical tests, linear regression), but…\nYou present the results from each country together so that you can see similarities and differences in relevant patterns (e.g., gender differences in household work or political attitudes).\n\n\n\nThis post shows you how to do this with data from the European Social Survey. Thematically, we stick to the general topic of gender and gender differences, and we look at the political gender gap: How men and women differ in their political opinions (Inglehart and Norris 2000; Iversen and Rosenbluth 2010, 2006), and how this may be affected by macro-level variables. In other words, we do a simple re-test of the “household bargaining theory” of political gender differences by Iversen & Rosenbluth (2006, 2010)."
  },
  {
    "objectID": "posts/compa_survey/index.html#are-people-influenced-by-the-environments-they-live-in",
    "href": "posts/compa_survey/index.html#are-people-influenced-by-the-environments-they-live-in",
    "title": "Comparing people’s behavior and attitudes across countries",
    "section": "",
    "text": "Are people’s opinions, behaviors, or perceptions affected by the environments (or “macro-level contexts”) they live in? Social and economic scientists often argue that this is indeed the case. For example, many sociologists argue that societal norms shape people’s behavior (e.g., that men do more of the housework in societies with egalitarian gender norms), and political scientists similarly suggest that political institutions influence political attitudes and behavior (e.g., that people participate more in elections in proportional or majoritarian electoral systems).\nObviously, these theories are only as good as the evidence in support of them. To test these types of theories, one needs to compare people’s opinions or behavior across contexts with different social norms, political institutions, or other macro-level factors that might have an influence on people. This is usually done with comparative survey data such as data from the European Social Survey, the International Social Survey Program, the Eurobarometer, the OECD Risks that Matter survey, or the World Values Study.1 The big advantage that comparative survey data offer is that they are standardized: The same survey with the exact same questions is conducted in multiple countries at the same time, so that people’s responses to the questions – i.e., their attitudes or behavior – can be directly compared.2 This means that one can use these survey data to find out for macro-level environmental factors influence (or at least are correlated with) people’s individual behaviors and attitudes.\nThis type of cross-country comparative analysis can seem daunting to students who have only learned about relatively simple methods (linear regression, graphical analyses). However, as always in life, there are easier and more complicated ways of doing this. This post shows you how to do it in the easiest way possible, using R and techniques that undergraduate students usually learn in their introductory statistics courses: Descriptive statistics and linear regression models (taking inspiration from Blekesaune and Bjørkhaug 2021).\n\n\n\n\n\n\nTL;DR\n\n\n\n\nYou pick a cross-country survey dataset (e.g., from the European Social Survey, the World Values Survey, the International Social Survey Program, or the Euro-, Afro-, or Arabbarometer) that contains relevant variables/questions for multiple countries that could be relevant to compare.\nYou select a small number (at least two and up to four or five) countries that differ in relevant ways in relevant macro-level aspects (e.g., one has overall traditional gender norms, the other is very progressive) but are otherwise as similar as possible so that you can rule out as many alternative explanations as possible.3\nYou analyze the data from each of the countries separately using standard techniques (e.g., graphs, statistical tests, linear regression), but…\nYou present the results from each country together so that you can see similarities and differences in relevant patterns (e.g., gender differences in household work or political attitudes).\n\n\n\nThis post shows you how to do this with data from the European Social Survey. Thematically, we stick to the general topic of gender and gender differences, and we look at the political gender gap: How men and women differ in their political opinions (Inglehart and Norris 2000; Iversen and Rosenbluth 2010, 2006), and how this may be affected by macro-level variables. In other words, we do a simple re-test of the “household bargaining theory” of political gender differences by Iversen & Rosenbluth (2006, 2010)."
  },
  {
    "objectID": "posts/compa_survey/index.html#the-iversenrosenbluth-hypothesis-in-the-smallest-of-nutshells",
    "href": "posts/compa_survey/index.html#the-iversenrosenbluth-hypothesis-in-the-smallest-of-nutshells",
    "title": "Comparing people’s behavior and attitudes across countries",
    "section": "The Iversen/Rosenbluth hypothesis in the smallest of nutshells",
    "text": "The Iversen/Rosenbluth hypothesis in the smallest of nutshells\nVery (very) simply put, Iversen & Rosenbluth (2006, 2010) argue that women are politically to the left of men, other things equal, but also that the size of this gap – how far apart women and men are – depends on macro-level factors such as how countries’ economies are structured. In countries that have economies that rely strongly on specific skills (think: highly trained craftsmen and -women that are really good at a few specific tasks), this gap should be particularly large. In contrast, in countries that rely more on general skills (think: flexible professionals that can quickly switch between jobs), women and men should be more equal in their political opinions."
  },
  {
    "objectID": "posts/compa_survey/index.html#re-analysis-using-ess-data",
    "href": "posts/compa_survey/index.html#re-analysis-using-ess-data",
    "title": "Comparing people’s behavior and attitudes across countries",
    "section": "Re-analysis using ESS data",
    "text": "Re-analysis using ESS data\nWe do a new test of this hypothesis using data from the tenth (2018) round of the European Social Survey (ESS).\nOut of all the countries covered by this round of the ESS, we select the following two countries based on qualitative information we have from Iversen & Rosenbluth (2006), but also other studies (Hall and Soskice 2001; Iversen and Soskice 2001):\n\nIreland, which is known to rely strongly on general skills. Here, we expect a small gender gap.\nNorway, which relies on specific skills. Here, we expect a large gender gap.\n\nIdeally, one would pick countries that are as similar as possible except for the structure of their economy so that we can really isolate the effect of that one factor (see also above), but we keep things simple and convenient for now and stick with Norway and Ireland.\nWe use the following micro-level variables from the ESS:\n\nLeft-right ideology (lrscale). This is measures people’s general political orientation and is the dependent variable.\nGender (gndr; male/female). This is the central independent variable here.\nHousehold income (hinctnta): This is a relevant control variable.\nAge (agea; years): Also a control variable.\nEducation (eduyrs): A final variable we want to control for.\n\n\nPackages\nWe use the tidyverse for data management & visualization and texreg to present regression results:\n\nlibrary(tidyverse)\nlibrary(texreg)\n\n\n\nSet theme for graphs\nThe classic theme just looks better…\n\ntheme_set(theme_classic())\n\n\n\nData import\nYou can download the data for free (after a registration) from https://www.europeansocialsurvey.org/. I use the .dta (Stata) version and saved the dataset as ESS10.dta on my computer. I use the haven package to import the dataset, and then immediately convert the dataset to the traditional R format with labelled::unlabelled (to be able to do this, you need to have both of these packages installed. Loading them with library() is not necessary).\n\ness &lt;- labelled::unlabelled(haven::read_dta(\"ESS10.dta\"))\n\n\n\nTrimming\nThe entire ESS is massive. To make things easier to handle, we select only the relevant variables (plus some useful “administrative” ones such as idno, essround, and cntry):\n\ness %&gt;% \n  select(idno,essround,cntry,lrscale,gndr,agea,eduyrs,hinctnta) -&gt; ess\n\n\n\nData cleaning\nHousehold income (hinctnta) and left-right self-placement (lrscale) are factors and need to be correctly converted to numeric before we can use them in a regression analysis:\n\nclass(ess$hinctnta)\n\n[1] \"factor\"\n\nclass(ess$lrscale)\n\n[1] \"factor\"\n\nbst290::visfactor(dataset = ess,\n                  variable = \"hinctnta\") # no label/value divergence, no adjustment needed\n\n values          labels\n      1  J - 1st decile\n      2  R - 2nd decile\n      3  C - 3rd decile\n      4  M - 4th decile\n      5  F - 5th decile\n      6  S - 6th decile\n      7  K - 7th decile\n      8  P - 8th decile\n      9  D - 9th decile\n     10 H - 10th decile\n\nbst290::visfactor(dataset = ess,\n                  variable = \"lrscale\") # labels/values are off by 1, needs to be adjusted\n\n values labels\n      1   Left\n      2      1\n      3      2\n      4      3\n      5      4\n      6      5\n      7      6\n      8      7\n      9      8\n     10      9\n     11  Right\n\ness %&gt;% \n  mutate(hhinc = as.numeric(hinctnta),\n         lrscale = as.numeric(lrscale) - 1) -&gt; ess\n\n\n\nCountry selection\nThe final “trimming” operation we need to do is to select only the two countries we want to compare. This is easy to do with filter(), and we create separate datasets for each of the two countries:\n\nunique(ess$cntry)\n\n [1] \"BE\" \"BG\" \"CH\" \"CZ\" \"EE\" \"FI\" \"FR\" \"GB\" \"GR\" \"HR\" \"HU\" \"IE\" \"IS\" \"IT\" \"LT\"\n[16] \"ME\" \"MK\" \"NL\" \"NO\" \"PT\" \"SI\" \"SK\"\n\ness %&gt;% \n  filter(cntry==\"NO\") -&gt; norway\n\ness %&gt;% \n  filter(cntry==\"IE\") -&gt; ireland\n\n\n\nDescriptive analysis of political gender gaps by country\nIt is good practice to first do a bit of visual analysis to get a sense of how the data look before moving to more complicated statistical analyses. Here, we use a bit of dplyr (group_by() & summarize()) to calculate the political gender gap in each country – how men and women differ, on average, in their ideology – and then visualize the result with a ggplot() bar graph.\nireland %&gt;%\n  group_by(gndr) %&gt;% \n  summarise(avg_lr = mean(lrscale, na.rm = T)) %&gt;% \n  ggplot(aes(x = gndr, y = avg_lr)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = round(avg_lr, digits = 1)), vjust = -.5) +\n    scale_y_continuous(limits = c(0,6)) +\n    labs(x = \"Gender\", y = \"Average left-right placement\",\n         caption = \"Higher scores = more conservative\",\n         title = \"Ireland\")\nnorway %&gt;%\n  group_by(gndr) %&gt;% \n  summarise(avg_lr = mean(lrscale, na.rm = T)) %&gt;% \n  ggplot(aes(x = gndr, y = avg_lr)) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = round(avg_lr, digits = 1)), vjust = -.5) +\n    scale_y_continuous(limits = c(0,6)) +\n    labs(x = \"Gender\", y = \"Average left-right placement\",\n         caption = \"Higher scores = more conservative\",\n         title = \"Norway\")\n\n\n\n\n\n\n\n\n\n\nIt looks like the data support the hypothesis. We expected a small ideological gap between men and women in Ireland, and that is what we find: Men and women hardly differ on average in their left-right orientation (5.3 - 5.2 = 0.1). In contrast, this difference is four times as large (5.2 - 4.8 = 0.4), which is what we would have expected.\n\n\nRegression analysis\nWhile the visual analysis is useful, we also need to do a more thorough test where we control for other variables. To do that, we do a simple linear (OLS) regression analysis separately for each country:\n\n# Baseline model\nno_mod1 &lt;- lm(lrscale ~ gndr,\n               data = norway)\n\n# With controls\nno_mod2 &lt;- lm(lrscale ~ gndr + agea + eduyrs + hhinc,\n               data = norway)\n\n# Baseline model\nie_mod1 &lt;- lm(lrscale ~ gndr,\n               data = ireland)\n\n# With controls\nie_mod2 &lt;- lm(lrscale ~ gndr + agea + eduyrs + hhinc,\n               data = ireland)\n\nWe use screenreg() from the texreg package to show the results directly next to each other so that we can spot differences between the two countries more easily:\n\nscreenreg(list(no_mod1,no_mod2,ie_mod1,ie_mod2),\n          stars = 0.05,\n          custom.header = list(\"Norway\" = 1:2, \"Ireland\" = 3:4),\n          custom.model.names = c(\"No controls\",\"Controls\",\n                                 \"No controls\",\"Controls\"),\n          custom.coef.map = list(\"(Intercept)\" = \"Intercept\",\n                                 \"gndrFemale\" = \"Female\",\n                                 \"agea\" = \"Age\",\n                                 \"eduyrs\" = \"Education (years)\",\n                                 \"hhinc\" = \"Household income (deciles)\"))\n\n\n=========================================================================\n                                    Norway                 Ireland       \n                            ----------------------  ---------------------\n                            No controls  Controls   No controls  Controls\n-------------------------------------------------------------------------\nIntercept                      5.21 *       5.41 *     5.26 *      4.61 *\n                              (0.09)       (0.35)     (0.08)      (0.38) \nFemale                        -0.41 *      -0.38 *    -0.02       -0.03  \n                              (0.13)       (0.13)     (0.11)      (0.13) \nAge                                         0.01 *                 0.02 *\n                                           (0.00)                 (0.00) \nEducation (years)                          -0.10 *                -0.01  \n                                           (0.02)                 (0.02) \nHousehold income (deciles)                  0.12 *                -0.03  \n                                           (0.03)                 (0.03) \n-------------------------------------------------------------------------\nR^2                            0.01         0.04       0.00        0.03  \nAdj. R^2                       0.01         0.04      -0.00        0.02  \nNum. obs.                   1375         1300       1516         993     \n=========================================================================\n* p &lt; 0.05\n\n\nWomen are again significantly more to the left than men in Norway but not in Ireland – which is what Iversen & Rosenbluth would have predicted. These effects are barely affected by the inclusion of controls for age, education, and household income.\nOverall, this relatively simple re-test supports the Iversen/Rosenbluth theory of gender differences."
  },
  {
    "objectID": "posts/compa_survey/index.html#next-steps",
    "href": "posts/compa_survey/index.html#next-steps",
    "title": "Comparing people’s behavior and attitudes across countries",
    "section": "Next steps",
    "text": "Next steps\nYou have now seen how you can do a simple cross-country comparative analysis of survey data with R. Obviously, you can adapt this type of analysis to many different questions so long as you have relevant data. For example, if you have macro-level indicators of how countries’ electoral systems look like (which you do: https://cpds-data.org/) and comparative survey data on people’s electoral behavior (which you can get via the ESS), you can test if rates of participation in election differ between types of electoral systems. The same applies to any combination of macro-level factor and micro-level behavior you can think of and have data for.\nImportantly, you may also have noticed that we did not use any form of quantitative data to measure macro-level factors or to pick countries – we simply relied on findings from other studies to select relevant countries.\nFinally, there are obviously ways to make this type of analysis more sophisticated. One additional step one can take is to test statistically if the coefficients from regression models are statistically significantly different from each other. Paternoster et al. (1998) have developed a simple formula for this that works basically like a standard two-sample t-test.\nThe most advanced way to compare survey data from different countries is obviously with a multi-level or hierarchical regression analysis. This is what academic researchers usally use because it multi-level regression models make it possible to use all available data from a comparative survey dataset instead of picking only a small number of countries. This makes it possible to estimate more complicated models and to get more accurate and reliable results. If you want to learn more about this, there is a series of articles that explains these models in a very intuitive and easy fashion (Merlo, Yang, et al. 2005; Merlo, Chaix, et al. 2005a, 2005b; Merlo et al. 2006; see also Steenbergen and Jones 2002), and the book by Finch et al. (2014) explains how you implement these models in R."
  },
  {
    "objectID": "posts/compa_survey/index.html#footnotes",
    "href": "posts/compa_survey/index.html#footnotes",
    "title": "Comparing people’s behavior and attitudes across countries",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee https://github.com/erikgahner/PolData?tab=readme-ov-file#cross-sectional for a list of comparative survey data projects.↩︎\nObviously, questionnaires are translated where languages differ, and there are sometimes cases where some questions are only asked in a subset of all countries that are included in a given survey.↩︎\nSee also Ringdal (2018), Chapter 9, King, Keohane, and Verba (1994), or Landman (2003), Chapters 2-3 for an explanation of relevant case selection strategies↩︎"
  },
  {
    "objectID": "posts/compa_countries/index.html",
    "href": "posts/compa_countries/index.html",
    "title": "Comparing countries with macro-level data",
    "section": "",
    "text": "Many important questions that political scientists, sociologists, or economists are asking are about patterns at the macro- or country-level: Why do some countries have bigger welfare states than others (Esping-Andersen 1990; Allan and Scruggs 2004; Korpi and Palme 2003; Iversen and Soskice 2006) or how do the political structures of a country affect its economic or environmental performance (Roller 2005; Acemoglu, Johnson, and Robinson 2001; Scruggs 2003)? Answering these questions usually requires some form of cross-country comparison with macro-level data on the size and shape of welfare states, political institutions, environmental performance, or economic growth. This analysis can be quantitative — also known as a time series cross-sectional regression analysis (see e.g., Beck 2001) — but it can also be a qualitative comparative case study — and even in the latter case, a few nice graphs that show relevant developments, patterns, and trends at the country-level can make the case study much more convincing and easy to follow.\nLuckily, we are now sitting on a mountain of (usually) freely available macro-level data on all kinds of economic, social, political, or environmental aspects for many countries and over long periods of time. To name just a few examples:\n\nInternational institutions such as the World Bank, the OECD, the European Union, or the UN offer free data on a vast number of economic, social, environmental, and political variables for their member countries.\nThe V-DEM project provides very detailed data on how democratic countries are for many countries going back to the 17-hundreds.\nThere are also many different datasets that measure countries’ political institutions, constitutional structures, party systems, election outcomes, and the representation of parties in parliaments and governments (see the Dataset of Political Datasets.)\nResearchers in international relations and peace & conflict studies have created many datasets on countries’ military strengths, alliances, conflicts, wars, terrorist attacks, and many other aspects (see https://github.com/erikgahner/PolData?tab=readme-ov-file#international-relations.)\n\nIn addition, many datasets come with associated R packages that allow you do directly import the datasets (see e.g., the vdemdata, WDI, or manifestoR packages).\nThere are different ways to work with macro-level data. A beginner-friendly way to work with macro-level data is to do descriptive analyses, and that is what the rest of this post is going to focus on.\nMore specifically, we will go over some example techniques using the Comparative Political Data Set (CPDS; https://cpds-data.org/), which is a very popular and fairly easy-to-work-with dataset in political science. It is a kind of Swiss army knife of macro-level data that includes the (usually) most relevant political, economic, and social macro-level indicators for a set of wealthy democracies in Europe, North America, and Australasia for the post-World War II period in one single source (e.g., GDP growth, the partisan composition of parliaments and governments, welfare state spending, or political institutions)."
  },
  {
    "objectID": "posts/compa_countries/index.html#why-we-do-macro-level-comparisons",
    "href": "posts/compa_countries/index.html#why-we-do-macro-level-comparisons",
    "title": "Comparing countries with macro-level data",
    "section": "",
    "text": "Many important questions that political scientists, sociologists, or economists are asking are about patterns at the macro- or country-level: Why do some countries have bigger welfare states than others (Esping-Andersen 1990; Allan and Scruggs 2004; Korpi and Palme 2003; Iversen and Soskice 2006) or how do the political structures of a country affect its economic or environmental performance (Roller 2005; Acemoglu, Johnson, and Robinson 2001; Scruggs 2003)? Answering these questions usually requires some form of cross-country comparison with macro-level data on the size and shape of welfare states, political institutions, environmental performance, or economic growth. This analysis can be quantitative — also known as a time series cross-sectional regression analysis (see e.g., Beck 2001) — but it can also be a qualitative comparative case study — and even in the latter case, a few nice graphs that show relevant developments, patterns, and trends at the country-level can make the case study much more convincing and easy to follow.\nLuckily, we are now sitting on a mountain of (usually) freely available macro-level data on all kinds of economic, social, political, or environmental aspects for many countries and over long periods of time. To name just a few examples:\n\nInternational institutions such as the World Bank, the OECD, the European Union, or the UN offer free data on a vast number of economic, social, environmental, and political variables for their member countries.\nThe V-DEM project provides very detailed data on how democratic countries are for many countries going back to the 17-hundreds.\nThere are also many different datasets that measure countries’ political institutions, constitutional structures, party systems, election outcomes, and the representation of parties in parliaments and governments (see the Dataset of Political Datasets.)\nResearchers in international relations and peace & conflict studies have created many datasets on countries’ military strengths, alliances, conflicts, wars, terrorist attacks, and many other aspects (see https://github.com/erikgahner/PolData?tab=readme-ov-file#international-relations.)\n\nIn addition, many datasets come with associated R packages that allow you do directly import the datasets (see e.g., the vdemdata, WDI, or manifestoR packages).\nThere are different ways to work with macro-level data. A beginner-friendly way to work with macro-level data is to do descriptive analyses, and that is what the rest of this post is going to focus on.\nMore specifically, we will go over some example techniques using the Comparative Political Data Set (CPDS; https://cpds-data.org/), which is a very popular and fairly easy-to-work-with dataset in political science. It is a kind of Swiss army knife of macro-level data that includes the (usually) most relevant political, economic, and social macro-level indicators for a set of wealthy democracies in Europe, North America, and Australasia for the post-World War II period in one single source (e.g., GDP growth, the partisan composition of parliaments and governments, welfare state spending, or political institutions)."
  },
  {
    "objectID": "posts/compa_countries/index.html#setup",
    "href": "posts/compa_countries/index.html#setup",
    "title": "Comparing countries with macro-level data",
    "section": "Setup",
    "text": "Setup\nIf you want to follow along, make sure you have the tidyverse loaded and, if you like, pre-set the ggplot2 graph theme to save time later:\n\nlibrary(tidyverse)\ntheme_set(theme_classic())"
  },
  {
    "objectID": "posts/compa_countries/index.html#what-macro-level-data-should-look-like",
    "href": "posts/compa_countries/index.html#what-macro-level-data-should-look-like",
    "title": "Comparing countries with macro-level data",
    "section": "What macro-level data (should) look like",
    "text": "What macro-level data (should) look like\nThe first important thing to understand is how a macro-level dataset should look like if you want to analyze it in R. As per Hadley Wickham’s Rules for Tidy Data (2014), all datasets should be structured in a way that:\n\nEvery row is an observation\nEvery column is a variable\n\nThis is easy when we work with a typical micro-level survey dataset like the European Social Survey, where the unit of observation is a single person. Here, every person is a row and every aspect that is recorded about them (their gender, income, age, etc.) is a column.\nIn a typical macro-level dataset, the unit of observation is usually a country-year: We observe Norway in 1990, 1991, 1992, and so on, and then we observe the France, Sweden, Japan, etc. in the same years.1 Here is a simple example of how this should look like using data on GDP growth (realgdpgr) from the CPDS dataset:\n\n\n\n\n\n\nImportant\n\n\n\nThis is how your dataset should look like!\n\n\n\n\n# A tibble: 12 × 4\n    year country iso   realgdpgr\n   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n 1  1990 France  FRA       3.03 \n 2  1991 France  FRA       0.944\n 3  1992 France  FRA       1.48 \n 4  1990 Japan   JPN       5.57 \n 5  1991 Japan   JPN       3.32 \n 6  1992 Japan   JPN       0.819\n 7  1990 Norway  NOR       1.93 \n 8  1991 Norway  NOR       3.08 \n 9  1992 Norway  NOR       3.57 \n10  1990 Sweden  SWE       0.755\n11  1991 Sweden  SWE      -1.15 \n12  1992 Sweden  SWE      -1.16 \n\n\nYou see that the individual observations for each country and year (country-years) are “stacked” on top of each other, and that we have two variables, country and year telling us which year and which country a given row corresponds to. These are important: You absolutely need to keep these variables in your dataset, otherwise you no longer know what each row in your dataset corresponds to. In addition, it is important to be aware that country and year in combination identify each unique observation: There is one “Norway in 1990”, one “Norway in 1991”, etc. observation, and you need to have both the country and the year variable to identify these observations.\nThe table also shows countries in two different formats: The plain English name, and the three-digit ISO country code. Many datasets use either of them (or different country codes), which can sometimes be a hassle to work with. Luckily, there is the countrycode package, which allows you to convert different country codes and names to other formats with a few lines of code.\nSometimes, and this can happen often when you download data from international organizations, the data you get look different (e.g., each row corresponds to a country and the columns refer to variables and years):\n\n\n\n\n\n\nImportant\n\n\n\nThis is how your dataset should not look like!\n\n\n\n\n# A tibble: 4 × 4\n  country realgdpgr_1990 realgdpgr_1991 realgdpgr_1992\n  &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1 France           3.03           0.944          1.48 \n2 Japan            5.57           3.32           0.819\n3 Norway           1.93           3.08           3.57 \n4 Sweden           0.755         -1.15          -1.16 \n\n\nIf you do have a dataset that looks like this, you need to learn how to pivot or reshape your dataset. Here, the pivot_longer() and pivot_wider() functions from the tidyr package (included in the tidyverse) are your best friends (see also Urdinez and Cruz 2020, chap. 2.5.1)."
  },
  {
    "objectID": "posts/compa_countries/index.html#importing-the-cpds-dataset",
    "href": "posts/compa_countries/index.html#importing-the-cpds-dataset",
    "title": "Comparing countries with macro-level data",
    "section": "Importing the CPDS dataset",
    "text": "Importing the CPDS dataset\nOK, enough theory — time to work with some data. If you want to follow along, you need to download the latest version of the CPDS dataset (https://cpds-data.org/data/). Ideally, download the Stata version, unzip the file, and store it in your RStudio project folder (or the folder that is your current Working Directory, which you can find out with the getwd() function). Once you have that, all you need to do is to use the haven package to import the dataset:\n\ncpds &lt;- haven::read_dta(\"CPDS_1960_2022_Update_2024.dta\")\n\nThe cpds object should now pop up in your Environment tab in RStudio. If you like, you can take a brief look at the data with glimpse. You should also download the official codebook and get familiar with the variables that are included in the dataset!\nAnother way to get a sense of what is contained is to look at the unique countries and years that are covered:\n\nunique(cpds$year)\n\n [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974\n[16] 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989\n[31] 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004\n[46] 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019\n[61] 2020 2021 2022\n\nunique(cpds$country)\n\n [1] \"Australia\"      \"Austria\"        \"Belgium\"        \"Bulgaria\"      \n [5] \"Canada\"         \"Croatia\"        \"Cyprus\"         \"Czech Republic\"\n [9] \"Denmark\"        \"Estonia\"        \"Finland\"        \"France\"        \n[13] \"Germany\"        \"Greece\"         \"Hungary\"        \"Iceland\"       \n[17] \"Ireland\"        \"Italy\"          \"Japan\"          \"Latvia\"        \n[21] \"Lithuania\"      \"Luxembourg\"     \"Malta\"          \"Netherlands\"   \n[25] \"New Zealand\"    \"Norway\"         \"Poland\"         \"Portugal\"      \n[29] \"Romania\"        \"Slovakia\"       \"Slovenia\"       \"Spain\"         \n[33] \"Sweden\"         \"Switzerland\"    \"United Kingdom\" \"USA\"           \n\n\nYou see that we have, in principle, data for almost all of Europe and the other advanced democracies around the globe from the 1960s onwards. What this does not show, however, is that we only have data for the Central and Eastern European countries (Poland, Bulgaria, etc.) from 1990 on, after the collapse of the Soviet Union and the Warsaw Pact:\n\ncpds |&gt; \n  filter(country == \"Poland\") |&gt; \n  select(year,country)\n\n# A tibble: 32 × 2\n    year country\n   &lt;dbl&gt; &lt;chr&gt;  \n 1  1991 Poland \n 2  1992 Poland \n 3  1993 Poland \n 4  1994 Poland \n 5  1995 Poland \n 6  1996 Poland \n 7  1997 Poland \n 8  1998 Poland \n 9  1999 Poland \n10  2000 Poland \n# ℹ 22 more rows\n\n\nBecause such a large batch of countries was added at this one time point, it makes sense to limit the data to the post-1990 period — otherwise, comparisons over time might not make sense.\n\ncpds |&gt; \n  filter(year&gt;=1990) -&gt; cpds\n\n(An alternative, if the interest is in long trends since World War II, is to leave out the post-communist countries. Here, the poco — “post-communist” — variable in the CPDS dataset is useful within filter().)"
  },
  {
    "objectID": "posts/compa_countries/index.html#descriptive-analyses-with-macro-level-data",
    "href": "posts/compa_countries/index.html#descriptive-analyses-with-macro-level-data",
    "title": "Comparing countries with macro-level data",
    "section": "Descriptive analyses with macro-level data",
    "text": "Descriptive analyses with macro-level data\nThere are four basic ways to look descriptively at macro-level data:\n\nYou can look at general trends across countries over time\nYou can compare average patterns between countries\nYou can compare trends within selected countries over time\nYou can look at average relationships (correlations) between countries\n\nEach of them tells you a different part of the entire story that is contained in the data. We will go over each of them and see how to aggregate and visualize the data. In most cases, your two best friends are the group_by() and summarize() functions from dplyr.\n\nAggregating by year to show general trends\nSometimes, we want to show to our readers general trends that existed more or less in all countries in the dataset over a given period of time. For example, we might want to show general phases of economic boom and crisis that affected all the advanced democracies, without looking specifically at individual countries. To do that, we can calculate the average rate of economic growth over all countries per year and visualize the result.\nThis is easy to do with group_by() and summarize():\n\ncpds |&gt; \n  group_by(year) |&gt; \n  summarize(avg_growth = mean(realgdpgr, na.rm = T))\n\n# A tibble: 33 × 2\n    year avg_growth\n   &lt;dbl&gt;      &lt;dbl&gt;\n 1  1990      2.76 \n 2  1991     -0.473\n 3  1992     -0.372\n 4  1993      0.124\n 5  1994      3.14 \n 6  1995      3.56 \n 7  1996      3.23 \n 8  1997      3.81 \n 9  1998      3.78 \n10  1999      2.94 \n# ℹ 23 more rows\n\n\nHere, we group the data by year and then calculate for each year the average of all GPD growth rates in all of the countries that are covered in the dataset. The result is an aggregated version of the dataset with average rates of GPD growth per year since the 1960s as individual observations.2\nWe can then visualize the result in a line graph:\n\ncpds |&gt; \n  group_by(year) |&gt; \n  summarize(avg_growth = mean(realgdpgr, na.rm = T)) |&gt; \n  ggplot(aes(x = year, y = avg_growth)) +\n    geom_line() +\n    geom_point() +\n    geom_hline(yintercept = 0, \n               linetype = \"dashed\", color = \"grey\") +\n    labs(x = \"\", y = \"Average GDP growth rate\") +\n    scale_x_continuous(breaks = seq(1960,2020,10))\n\n\n\n\n\n\n\n\nThe graph clearly shows the last two big economic crises, the 2008 Financial Crisis (a.k.a., Great Recession) and the COVID-19 pandemic, but also the economic crisis of the early 1990s.\n\n\n\n\n\n\nImportant\n\n\n\nYou may notice that we do not save the resulting aggregated dataset in a separate object — and we most definitely do not overwrite the original dataset with the aggregated version. We simply aggregate the data “on the fly”, visualize the result, and then let the aggregated version disappear into the ether. Sometimes, you might want to save an aggregated dataset so that you can use it in later analyses, but in that case you need to make sure that you give it a different name.\n\n\nIt is also important to be aware that we can also calculate other types of summary statistics like the median, the variance, or the standard deviation within summarize(). The latter is very helpful if we want to show not only average trends but also the variation around the trend line. To do that, we simply add the summary statistic we want within summarize():\n\ncpds |&gt; \n  group_by(year) |&gt; \n  summarise(avg_growth = mean(realgdpgr, na.rm = T),\n            sd_growth = sd(realgdpgr, na.rm = T))\n\n# A tibble: 33 × 3\n    year avg_growth sd_growth\n   &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1  1990      2.76       2.17\n 2  1991     -0.473      5.21\n 3  1992     -0.372      5.28\n 4  1993      0.124      4.14\n 5  1994      3.14       2.93\n 6  1995      3.56       2.29\n 7  1996      3.23       1.75\n 8  1997      3.81       4.40\n 9  1998      3.78       2.33\n10  1999      2.94       3.05\n# ℹ 23 more rows\n\n\nNow we get two summary statistics per year: The average growth rates and the standard deviation in growth rates (the average deviation from the average). To visualize this, we can calculate +/- 1 standard deviation ranges around the mean values and use geom_ribbon() in ggplot() to visualize the result:\n\ncpds |&gt; \n  group_by(year) |&gt; \n  summarise(avg_growth = mean(realgdpgr, na.rm = T),\n            sd_growth = sd(realgdpgr, na.rm = T)) |&gt; \n  mutate(upper = avg_growth + sd_growth,\n         lower = avg_growth - sd_growth) |&gt; \n  ggplot(aes(x = year, y = avg_growth, ymin = lower, ymax = upper)) +\n    geom_line() +\n    geom_point() +\n    geom_ribbon(alpha = .2) + # alpha makes the area transparent\n    geom_hline(yintercept = 0, \n               linetype = \"dashed\", color = \"black\") +\n    labs(x = \"\", y = \"Average GDP growth rate\",\n         caption = \"Shaded area indicates +/- 1 SD ranges.\") +\n    scale_x_continuous(breaks = seq(1960,2020,10))\n\n\n\n\n\n\n\n\nOne new lesson we learn is that although the average growth rate dipped into the negative in the early 1990s, there was also increased variation in growth rates — the range got visibly bigger — which indicates that not all countries were equally strongly affected by the crisis.\n\n\nAggregating by country to show differences\nAnother thing we might be interested in is which countries had, on average, the highest or lowest growth rates in the period between 1990 and today. To see this, we again use group_by() and summarize(), but we now group by country instead of year:\n\ncpds |&gt; \n  group_by(country) |&gt; \n  summarise(avg_growth = mean(realgdpgr, na.rm = T))\n\n# A tibble: 36 × 2\n   country        avg_growth\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 Australia            2.89\n 2 Austria              1.87\n 3 Belgium              1.81\n 4 Bulgaria             1.94\n 5 Canada               2.13\n 6 Croatia              2.34\n 7 Cyprus               3.41\n 8 Czech Republic       1.58\n 9 Denmark              1.76\n10 Estonia              4.02\n# ℹ 26 more rows\n\n\nAs before, we now get an aggregated version of the dataset — but now it is aggregated by country, not by year. We see that for example Australia had an average growth rate of arond 3.3% per year, while the rate in the Czech Republic was only around 1.6% per year.\nWe can again visualize the result, but here a bar graph makes most sense. We can also use reorder() to sort the bars according to the average growth rate:\n\ncpds |&gt; \n  group_by(country) |&gt; \n  summarise(avg_growth = mean(realgdpgr, na.rm = T)) |&gt; \n  ggplot(aes(x = avg_growth, y = reorder(country, avg_growth))) +\n    geom_col() +\n    labs(x = \"Average rate of GDP growth (%)\", y = \"\")\n\n\n\n\n\n\n\n\nYou see that Ireland (the Irish Tiger) had by far the highest growth rate since the 1990s, followed by Malta and Estonia. Italy, Greece, and Japan had clearly the lowest average rates of growth.\n\n\nComparing selected countries over time\nSometimes, for example when you do a comparative case study of a few selected countries, you want to show relevant developments in those countries, without any aggregation. This is obviously also possible with this type of data, and here the filter() function is your best friend.\nLet’s say we want to compare the development of economic growth rates in the four largest Nordic countries (Denmark, Finland, Norway, Sweden) since the 1990s. In that case, we just need to use filter() to subset the data to those countries:\n\ncpds |&gt; \n  select(country,year,realgdpgr) |&gt; # this is technically not necessary, but \n  # sometimes useful to avoid losing overview over the data\n  filter(country %in% c(\"Denmark\",\"Finland\",\"Sweden\",\"Norway\"))\n\n# A tibble: 132 × 3\n   country  year realgdpgr\n   &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Denmark  1990    1.48  \n 2 Denmark  1991    1.39  \n 3 Denmark  1992    1.96  \n 4 Denmark  1993    0.0107\n 5 Denmark  1994    5.33  \n 6 Denmark  1995    3.03  \n 7 Denmark  1996    2.90  \n 8 Denmark  1997    3.26  \n 9 Denmark  1998    2.21  \n10 Denmark  1999    2.95  \n# ℹ 122 more rows\n\n\nThis is all there is to it — we now have limited the dataset to the four Nordic countries. Obviously, the result is not very informative, but we can again visualize the result in a line graph with ggplot():\n\ncpds |&gt; \n  select(country,year,realgdpgr) |&gt; # this is technically not necessary, but \n  # sometimes useful to avoid losing overview over the data\n  filter(country %in% c(\"Denmark\",\"Finland\",\"Sweden\",\"Norway\")) |&gt; \n  ggplot(aes(x = year, y = realgdpgr, group = country, color = country)) +\n    geom_line(linewidth = 1) +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey\") +\n    scale_color_brewer(palette = \"Paired\") + # color-blind friendly palette\n    scale_x_continuous(breaks = seq(1990,2020,5)) +\n    labs(y = \"GDP growth rate (%)\", x = \"\", color = \"\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nIn general, growth rates in the four countries are (unsurprisingly) behaving quite similarly — when Denmark experiences a crisis, Sweden, Norway, and Finland do as well — but it does seem that Finland tends a bit more toward the extremes than the other countries. Crises tend to hit hardest in Finland, but the following recoveries are also stronger.\nAn alternative way to visualize the result is to use facet_wrap() to create a separate graph for each country. This helps if, as is the case here, the lines overlap strongly:\n\ncpds |&gt; \n  filter(country %in% c(\"Denmark\",\"Finland\",\"Sweden\",\"Norway\")) |&gt; \n  ggplot(aes(x = year, y = realgdpgr)) +\n    geom_line() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"grey\") +\n    facet_wrap(~ country) +\n    scale_x_continuous(breaks = seq(1990,2020,5)) +\n    labs(y = \"GDP growth rate (%)\", x = \"\", color = \"\") +\n    theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nThe more extreme up- and downswings in Finland are still visible.\n\n\nShowing bivariate relationships across countries\nAlthough the development of a single variable over time or its variation between countries is often relevant to look at, we are in most cases primarily interested in relationships between variables: Does one variable affect the other, or are they at least correlated with each other?\nOne wat to check for bivariate relationships between two variables is to aggregate both variables by country and then use a scatterplot to visualize the result.\nLet’s say we wanted to test the hypothesis that a stronger presence of left parties in government is bad for economic growth (as some people claim). The gov_left1 variable from the CDPS dataset gives us the share of cabinet posts that are held by left-of-center parties in a given year and country, and we can simply aggregate this variable along with the one measuring economic growth within summarize():\n\ncpds |&gt; \n  group_by(country) |&gt; \n  summarise(avg_growth = mean(realgdpgr, na.rm = T),\n            avg_leftgov = mean(gov_left1, na.rm = T)) \n\n# A tibble: 36 × 3\n   country        avg_growth avg_leftgov\n   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;\n 1 Australia            2.89        38.1\n 2 Austria              1.87        33.8\n 3 Belgium              1.81        38.2\n 4 Bulgaria             1.94        16.2\n 5 Canada               2.13         0  \n 6 Croatia              2.34        20.7\n 7 Cyprus               3.41        13.8\n 8 Czech Republic       1.58        28.3\n 9 Denmark              1.76        38.8\n10 Estonia              4.02        21.7\n# ℹ 26 more rows\n\n\nThe numbers give us the average growth rate and the average share of cabinet posts held by left parties in each country in the period since 1990. We can now use geom_point() to visualize the result in a scatterplot, and add geom_smooth() to get a fitted line that highlights the relationship between the variables:\n\ncpds |&gt; \n  group_by(country) |&gt; \n  summarise(avg_growth = mean(realgdpgr, na.rm = T),\n            avg_leftgov = mean(gov_left1, na.rm = T)) |&gt; \n  ggplot(aes(x = avg_leftgov, y = avg_growth)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = F, color = \"grey\", \n                linetype = \"dashed\") +\n    labs(x = \"Avg. share of left parties in government (%)\",\n         y = \"Average rate of economic growth (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is indeed a negative — but quite weak — relationship between the two variables. However, it important not to forget that the arrow of causality might run the other way: Maybe left parties get elected more often in times of economic crises? (What happens when you look at the relationship between economic growth and the share of right parties in government using gov_right1?)\nOne way to still improve the graph is to add labels for each country instead of anonymous black dots to be able to see where the different countries are located. To do that, we can replace geom_point() with geom_text(), and we use the iso variable (which is equivalent to the country variable) to aggregate the data. By using iso, we later have handy short labels that we can use in the graph:\n\ncpds |&gt; \n  group_by(iso) |&gt; \n  summarise(avg_growth = mean(realgdpgr, na.rm = T),\n            avg_leftgov = mean(gov_left1, na.rm = T)) |&gt; \n  ggplot(aes(x = avg_leftgov, y = avg_growth)) +\n    geom_text(aes(label = iso)) +\n    geom_smooth(method = \"lm\", se = F, color = \"grey\", \n                linetype = \"dashed\") +\n    labs(x = \"Avg. share of left parties in government (%)\",\n         y = \"Average rate of economic growth (%)\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis clarifies matters. It almost seems as if the slight negative relationship between left government participation and economic growth is mainly driven by the outlying case of Ireland…"
  },
  {
    "objectID": "posts/compa_countries/index.html#what-next",
    "href": "posts/compa_countries/index.html#what-next",
    "title": "Comparing countries with macro-level data",
    "section": "What next?",
    "text": "What next?\nThis post showed you how you can do descriptive analyses of cross-country macro-level (or time series cross-sectional) datasets. This can help you spice up a comparative case study with descriptive statistics of relevant macro-level indicators, and it can be a stepping stone toward learning how to do regression analyses with this type of data.\nMore concrete steps you can take to advance further are:\n\nGet more of an overview over what macro-level datasets there are out there (see also the sources above).\nLearn how to combine (“merge”) different datasets. This is not as difficult as it might sound. Since all these datasets have the same underlying country-year structure, you just need to figure out how to work with the left_join() function to merge datasets (see also the post on how to measure globalization exposure), and probably also how to convert different country codes and names between each other with the countrycode package (see also Urdinez and Cruz 2020, chap. 11).\nExplore other types of macro-level datasets. Relevant examples are the Manifesto Project Database, which provides quantitative estimates of the ideological positions of political parties in different countries (here, the unit of observation is a party at a given election or “party-election”) or different peace & conflict datasets (e.g., Raleigh et al. 2010; Uppsala Conflict Data Program 2014; Gibler and Miller 2023; Vogt et al. 2015).\nLearn how to do multivariate regression analyses with these datasets. Relevant works to read are Beck and Katz (1995, 1996, 2011), Beck, Katz, and Tucker (1998), Beck (2001), De Boef and Keele (2008), Wilson and Butler (2007), Carter and Signorino (2010), Honaker and King (2010), Birkel (2014), and for more advanced methods see Blackwell and Glynn (2018). Urdinez and Cruz (2020, chap. 7) and Croissant and Millo (2008) show how to implement the main techniques in R."
  },
  {
    "objectID": "posts/compa_countries/index.html#footnotes",
    "href": "posts/compa_countries/index.html#footnotes",
    "title": "Comparing countries with macro-level data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn exception are many peace & conflict datasets, where the unit of observation is a country-pair (dyad) or a conflict-year. This can make these datasets a bit more difficult to work with.↩︎\nObviously, you should always make sure that you do not have large changes in the composition of your dataset — e.g., where many new countries are added from a given year on — because that can lead to sudden jumps or dips in the average values.↩︎"
  },
  {
    "objectID": "posts/case_selection2/index.html",
    "href": "posts/case_selection2/index.html",
    "title": "Case selection with macro-level data",
    "section": "",
    "text": "The comparative case study is one of the workhorse methods in political science, sociology, and partly economics. In a typical comparative case study, we study some phenomenon of interest (war, policy reforms, party strategies, economic development, etc.) in a small number of cases to figure out the underlying causes of the phenomenon (why countries go to war, why they introduce or do not introduce reforms, why they experience economic growth or stagnate, etc.). Often, the cases in a case study are countries – but this is not a requirement (Gerring 2004). Any relevant unit or action can be a case, be it court rulings (e.g., Alter and Meunier-Aitsahalia 1994; Lindstrom 2010), policy reforms (e.g., Picot 2009), or conflicts and wars (e.g., Ruane and Todd 1996) – even a comparative analysis of survey data from a small number of countries (as shown in this post) can be seen as a comparative case study where the countries are the cases."
  },
  {
    "objectID": "posts/case_selection2/index.html#comparative-case-studies-and-case-selection",
    "href": "posts/case_selection2/index.html#comparative-case-studies-and-case-selection",
    "title": "Case selection with macro-level data",
    "section": "",
    "text": "The comparative case study is one of the workhorse methods in political science, sociology, and partly economics. In a typical comparative case study, we study some phenomenon of interest (war, policy reforms, party strategies, economic development, etc.) in a small number of cases to figure out the underlying causes of the phenomenon (why countries go to war, why they introduce or do not introduce reforms, why they experience economic growth or stagnate, etc.). Often, the cases in a case study are countries – but this is not a requirement (Gerring 2004). Any relevant unit or action can be a case, be it court rulings (e.g., Alter and Meunier-Aitsahalia 1994; Lindstrom 2010), policy reforms (e.g., Picot 2009), or conflicts and wars (e.g., Ruane and Todd 1996) – even a comparative analysis of survey data from a small number of countries (as shown in this post) can be seen as a comparative case study where the countries are the cases."
  },
  {
    "objectID": "posts/case_selection2/index.html#rules-and-strategies-for-case-studies",
    "href": "posts/case_selection2/index.html#rules-and-strategies-for-case-studies",
    "title": "Case selection with macro-level data",
    "section": "Rules and strategies for case studies",
    "text": "Rules and strategies for case studies\nA case study is only as good as the evidence (the newspaper articles, interviews, etc.) and the case selection strategy (a.k.a., research design) it builds on. As I explain in this other post on case selection and bullshit, the cases you choose affect the answers you get (Geddes 1990), and if you select your cases in a faulty way, you get faulty, misleading, and ultimately useless answers – you produce bullshit, whether you want to or not.\nFortunately, there are a number of different rules and strategies that you can use to select your cases in a way that minimizes the risk that your study produces flawed evidence (see e.g., Seawright and Gerring 2008; Przeworski and Teune 1970; King, Keohane, and Verba 1994; Geddes 1990; Ringdal 2018).\n\nWhat not to do\nTwo important rules are:\n\nDo not select on the dependent variable (Geddes 1990). You always need positive and negative or “high” and “low” cases. For example, if you want find out why countries go to war, you need to compare countries that fight wars to similar ones who do not fight wars so that you can see what makes the difference.1\nMake sure that you have enough cases. As a general rule, you need at least one more case than you have explanatory variables (King, Keohane, and Verba 1994). For example, if you think that civil wars are caused by two variables, say economic deprivation and a hilly terrain (Fearon and Laitin 2003), then you need at least three cases to be able to have a chance to see if this is actually true.\n\n\n\nWhat do to\nThere are two well-known and widely used case selection strategies that help you eliminate potential alternative explanations for your phenomenon of interest (in other words, they help you address potential critical questions from pesky supervisors, examiners, or reviewers before you even start your research). For example, let’s say you are interested in the causes of political trust – why some citizens have high trust in politicians and political institutions and others have low trust – and you think this could be influenced by countries’ electoral systems (whether they have a proportional or majoritarian “first-past-the-post” type system). If you would now compare political trust in Sweden and the United States, you would have two countries that differ in their electoral systems – but also in many other variables (regulation of the economy, form of government, welfare state spending, etc.), and you cannot rule out that any differences in trust come from these other variables rather than the electoral systems.\nA better approach would be to select two countries that are as similar as possible in all relevant aspects – except for their electoral systems. This type of case selection strategy is also called a Most-Similar-Systems-Design, or MSSD for short (Ringdal 2018, 183). Table 1 (a) below shows how this can look like: We have two cases (countries) that have different types of electoral systems but are otherwise identical in three other variables that could be relevant for political trust as well.\nAs a second example, let’s say you believe that people’s political trust is mainly influenced by their level of education, where trust increases with education. If you now look at this relationship only in one country (say, Norway), you open yourself up to the criticism that maybe this relationship exists only in this country and only because of Norway’s unique social and political circumstances, but the finding would not apply to other countries.\nOne way to address critiques like this head-on is to select cases that are as different from each other as possible. If you can show that a relationship is present in two or more cases that are very different from each other, it is likely (but not guaruanteed) that this also applies to many other cases. This case selection strategy is called a Most-Different-Systems-Design or MDSD (Ringdal 2018, 183). Table 1 (b) illustrates how such a design could look like: You pick two cases that differ in three variables that we believe are relevant, and you then ideally show that the effect of education on trust is the same in both cases. Here, we assume that the degree of female representation in politics, the political system overall, and welfare state spending could be relevant variables. (Whether that makes sense or not is a different discussion.)\n\n\n\nTable 1: Example case selection strategies\n\n\n\n\n\n\n\n(a) Most-Similar-Systems-Design\n\n\n\n\n\nVariable\nCase A\nCase B\n\n\n\n\nElectoral system\nMajoritarian\nProportional\n\n\nRegulation\nHigh\nHigh\n\n\nWelfare spending\nLow\nLow\n\n\nGovernment\nParliamentary\nParliamentary\n\n\n\n\n\n\n\n\n\n\n\n(b) Most-Different-Systems-Design\n\n\n\n\n\n\n\n\n\n\nVariable\nCase A\nCase B\n\n\n\n\nFemale representation\nHigh\nModerate\n\n\nPolitical system\nConst. parliamentary monarchy\nRepublican presidential democracy\n\n\nWelfare spending\nHigh\nLow\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen to use which strategy\nGenerally, the MDSD is best-suited if you want to show that a micro-level pattern (e.g., the effect of an individual’s education on their political trust, or gender differences) holds in very different contexts. In other words, the MDSD is a good choice if you want to control for variables that are contextual or at a higher level of analysis than the main pattern you are interested in.\nThe MSSD, on the other hand, is better if you want to control for other variables that are at the same level of analysis. In the example above, the main explanatory variable of interest is the electoral system – a macro-level variable – and we want to control for other macro-level variables. Here, the MSSD can help us do that."
  },
  {
    "objectID": "posts/case_selection2/index.html#using-r-to-select-cases",
    "href": "posts/case_selection2/index.html#using-r-to-select-cases",
    "title": "Case selection with macro-level data",
    "section": "Using R to select cases",
    "text": "Using R to select cases\nAlright, enough with the dry theory. It is one thing to have an idea of what your case selection should theoretically look like, but you also need to actually find empirical cases and you usually also need to explain and justify your case selection in your course paper, thesis, or presentation.\nThis is where R and macro-level datasets like the Comparative Political Data Set (CPDS, Armingeon et al. 2024) come in handy, because you can use them to get an overview over how countries compare on many different variables, to identify countries that are either similar or different on relevant variables, and also to illustrate your selection with graphs and tables (see also this other post on how to work with macro-level datasets).\n\nSetup\nFirst, we need to load all relevant packages. In this case, we only need to load the tidyverse packages, and we can also already set a default theme for ggplot graphs:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntheme_set(theme_classic())\n\nNext, we need to download and import the CPDS dataset. Unfortunately, it is difficult to import the dataset directly into R, so we just download the Stata (.dta) version of the dataset, unpack the ZIP archive, and save the dataset somewhere where we can find it again. Then we import the saved dataset with the haven package:\n\ncpds &lt;- haven::read_dta(\"CPDS_1960_2022_Update_2024.dta\")\n\nWe can then take a quick look at which countries are included:\n\nunique(cpds$country)\n\n [1] \"Australia\"      \"Austria\"        \"Belgium\"        \"Bulgaria\"      \n [5] \"Canada\"         \"Croatia\"        \"Cyprus\"         \"Czech Republic\"\n [9] \"Denmark\"        \"Estonia\"        \"Finland\"        \"France\"        \n[13] \"Germany\"        \"Greece\"         \"Hungary\"        \"Iceland\"       \n[17] \"Ireland\"        \"Italy\"          \"Japan\"          \"Latvia\"        \n[21] \"Lithuania\"      \"Luxembourg\"     \"Malta\"          \"Netherlands\"   \n[25] \"New Zealand\"    \"Norway\"         \"Poland\"         \"Portugal\"      \n[29] \"Romania\"        \"Slovakia\"       \"Slovenia\"       \"Spain\"         \n[33] \"Sweden\"         \"Switzerland\"    \"United Kingdom\" \"USA\"           \n\n\nIt usually also makes sense to find out for which years we have data for each country (can you make sense of this code by yourself?):2\n\ncpds %&gt;% \n  group_by(country) %&gt;% \n  summarise(start_year = min(year, na.rm = T),\n            end_year = max(year, na.rm = T)) %&gt;% \n  mutate(year_range = end_year - start_year) %&gt;% \n ggplot(aes(xmin = start_year, xmax = end_year, \n            y = reorder(country, year_range))) +\n    geom_linerange(linewidth = 3, color = \"grey30\") +\n    scale_x_continuous(breaks = seq(1960,2020,10)) +\n    labs(y = \"\")\n\n\n\n\n\n\n\n\n\n\nSelecting cases for an MSSD strategy\nLet’s say we want to find two cases for the MSSD strategy that was described earlier, where we want different electoral systems but otherwise equality or at least similarity on all other variables.\nA good start is to get an overview over which countries have which type of electoral system. The CPDS contains a variable called prop (see also Huber, Ragin, and Stephens 1993), which is a categorical variable that measures whether a country has a single-member, plurality (“first-past-the-post”) electoral system (0), a modified proportional or mixed system (1), or a proportional electoral system (2).\nTo get this overview, we can calculate the average scores of this variable per country and visualize the result:\n\ncpds %&gt;% \n  group_by(country) %&gt;% \n  summarise(mprop = mean(prop, na.rm = T)) %&gt;% \n  ggplot(aes(x = mprop, y = reorder(country, mprop))) +\n    geom_bar(stat = \"identity\") +\n    geom_text(aes(label = round(mprop, digits = 1)), hjust = -.5) +\n    scale_x_continuous(breaks = seq(0,2,1)) +\n    labs(x = \"Proportionality\", y = \"\")\n\n\n\n\n\n\n\n\nA clear majority of countries have proportional systems, a few have mixed systems (Lithuania, Japan, France, and Auustralia), and three countries (the USA, the UK, and Canada) have single-member “first-past-the-post” systems. Note also that a few countries (New Zealand, Italy, and Hungary) have average scores that fall between two whole numbers – which indicates that these countries changed their electoral systems at some point.\nLet’s have a closer look at these countries:\n\ncpds %&gt;% \n  filter(country %in% c(\"New Zealand\",\"Hungary\",\"Italy\")) %&gt;% \n  ggplot(aes(x = year, y = prop)) +\n    geom_line() +\n    geom_point(color = \"orange\") +\n    facet_wrap(~country, nrow = 3) +\n    scale_y_continuous(breaks = seq(0,2,1)) +\n    scale_x_continuous(breaks = seq(1960,2020,10)) +\n    labs(x = \"\", y = \"Proportionality\")\n\n\n\n\n\n\n\n\nYou see that Hungary went from a fully to a modified proportional system in the 2010s, Italy did the same in the 1990s but then went back to full proportionality in the 2000s, and New Zealand went from a “first-past-the-post” system to full proportionality in the mid-1990s.\nOne thought would be now to use this variation over time in the three countries to see if the changes in electoral rules led to changes in political trust among citizens – as long as we can assume that nothing else changed simultaneously in these countries (no changes in unemployment, economic growth, party politics, etc.), or we can control for it somehow.\nAn alternative would be to focus on New Zealand, which is interesting because it is the one typical English-speaking former settler colony that went away from a “first-past-the-post” system and to a proportional system, and which kept that system long enough so that its potential effects on political trust can unfold over time.\nLet’s see how New Zealand compares to countries that we know are culturally and politically very similar: Australia, Canada, the United States, and the United Kingdom. To help with that, we create a variable that identifies those countries in the dataset and which we can use to highlight them in graphs:\n\ncpds %&gt;% \n  mutate(nzl_etal = case_match(country,\n                               \"New Zealand\" ~ \"NZL\",\n                               c(\"Canada\",\"Australia\",\"USA\",\"United Kingdom\") ~ \"Similar\",\n                               .default = \"Other\")) -&gt; cpds\n\nTo compare them in terms of the aspects listed in Table 1 (a) above, we can use the following variables from the CPDS:\n\nemprot_reg is a measure of the regulation of the labor market (see also Venn 2009)\nsocexp_t_pmp is a measure of welfare state spending in relation to a country’s GDP\npres is measure of the type of government (0 = parliamentary; 1 = semi-presidential, dominant parliament; 2 = hybrid; 3 = semi-presidential, dominant president; 4 = presidential).\n\nThe socexp_t_pmp and emprot_reg variables are generally available until 2018, so we pick that year to compare countries.\nFirst, we look at the extent of labor market regulation:\n\ncpds %&gt;% \n  filter(year==2018) %&gt;% \n  ggplot(aes(x = emprot_reg, y = reorder(country, emprot_reg), fill = nzl_etal)) +\n    geom_col() +\n    scale_fill_manual(values = c(\"NZL\" = \"cornflowerblue\",\n                                 \"Similar\" = \"orange\",\n                                 \"Other\" = \"grey20\")) +\n    labs(y = \"\", x = \"Index of employment protection legislation\") +\n    theme(legend.position = \"none\")\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nNew Zealand and Australia stand a bit apart from the other English-speaking countries, but are very similar to each other.\nNext, we look at the type of government:\n\ncpds %&gt;% \n  filter(year==2018) %&gt;% \n  ggplot(aes(x = pres, y = reorder(country, pres))) +\n    geom_col() +\n    geom_text(aes(label = pres, color = nzl_etal), hjust = -.5) +\n    scale_color_manual(values = c(\"NZL\" = \"cornflowerblue\",\n                                 \"Similar\" = \"orange\",\n                                 \"Other\" = \"grey20\")) +\n    labs(y = \"\", x = \"Index of presidentialism\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nBoth Australia and New Zealand are clearly parliamentary democracies, very unlike the US.\nFinally, let’s have a look at how countries compare in terms of welfare state spending:\n\ncpds %&gt;% \n  filter(year==2018) %&gt;% \n  ggplot(aes(x = socexp_t_pmp, y = reorder(country, socexp_t_pmp), fill = nzl_etal)) +\n    geom_col() +\n    geom_text(aes(label = round(socexp_t_pmp, digits = 1)), hjust = -.5) +\n    scale_fill_manual(values = c(\"NZL\" = \"cornflowerblue\",\n                                 \"Similar\" = \"orange\",\n                                 \"Other\" = \"grey20\")) +\n    labs(y = \"\", x = \"Spending on social protection (%GDP; public and mandatory private)\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nHere, New Zealand is more similar to the United Kingdom than to Australia, but the two countries are not very far apart. Also, many would say that welfare state spending is “epiphenomenal” (Esping-Andersen 1990) anyways and that a look at the underlying rules and institutions reveals that Australia and New Zealand both belong to the liberal welfare state regime (see also Castles 1993).\nSo, at least in the late 2010s, Australia and New Zealand are two countries that are historically, politically, and economically very similar – but differ in their electoral systems. This could be a potential MSSD case selection that we can explain and defend with evidence.\n\n\nSelecting cases for an MDSD strategy\nThe workflow to select potential cases for an MDSD strategy is basically the same as that shown above for an MSSD strategy: We sift through and visualize relevant macro-level data to see how potential cases compare. The only difference is that we look for differences rather than similarities in the case of an MDSD.\nAs per Table 1 (b), we want to find two cases that are as different as possible in terms of their political systems, female representation, and welfare state spending (the variables which we assume to be relevant, for the sake of illustration).\nTo identify cases – countries – that differ in terms of their political systems, we can again use data from the CPDS, specifically the variables measuring the structure of political institutions in a given country like the prop and pres variables used above. A third one we could consider is called fed and measures the degree of federalism in a country (0 = no; 1 = weak; 2 = strong). To keep things simple, we look again at data from 2018, but this is something you can of course adapt in your own analysis.\n\ncpds %&gt;% \n  filter(year == 2018) %&gt;% \n  ggplot(aes(x = prop, y = reorder(country, prop))) +\n    geom_col() +\n    geom_text(aes(label = prop), hjust = -.5) +\n    labs(y = \"\", x = \"Index of proportionality\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIf we take Norway (the case we started from earlier) as a baseline, then the three countries with “first-past-the-post” electoral systems are very different from it. The US are in addition also the one democracy out of the three with “first-past-the-post” systems that does not have a monarch as its head of state.\n\ncpds %&gt;% \n  filter(year == 2018) %&gt;% \n  ggplot(aes(x = pres, y = reorder(country, pres))) +\n    geom_col() +\n    geom_text(aes(label = pres), hjust = -.5) +\n    labs(y = \"\", x = \"Index of presidentialism\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nWhen we look at the degree of presidentialism, the US and Norway are clearly worlds apart.\n\ncpds %&gt;% \n  filter(year == 2018) %&gt;% \n  ggplot(aes(x = fed, y = reorder(country, fed))) +\n    geom_col() +\n    geom_text(aes(label = fed), hjust = -.5) +\n    labs(y = \"\", x = \"Index of federalism\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThis is the same when we look at the degree of federalism.\n\ncpds %&gt;% \n  filter(year==2018) %&gt;% \n  ggplot(aes(x = socexp_t_pmp, y = reorder(country, socexp_t_pmp))) +\n    geom_col() +\n    geom_text(aes(label = round(socexp_t_pmp, digits = 1)), hjust = -.5) +\n    labs(y = \"\", x = \"Spending on social protection (%GDP; public and mandatory private)\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe US are roughly in the middle of the pack when it comes to welfare state spending, but score lower than Norway. And, again, a look at Esping-Andersen (1990) would suggest that the two countries are very different when it comes to the policies and institutions that are underneath the spending figures.\nNow to the final aspect, female representation in politics. Here, we can use the womenpar variable from the CPDS, which measures the percentage of seats in parliament that are held by women (regardless of their partisan affiliation). As before, this variable is available until 2018:\n\ncpds %&gt;% \n  filter(year == 2018) %&gt;% \n  ggplot(aes(x = womenpar, y = reorder(country, womenpar))) +\n    geom_col() +\n    geom_text(aes(label = round(womenpar, digits = 1)), hjust = 1.5, color = \"white\") +\n    labs(y = \"\", x = \"Female representation in parliament (%)\") +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nOnce again, Norway and the US are quite far apart. Norway, along with Sweden and Finland, is one of the top scorers when it comes to gender equality in politics, while the US are in the bottom half.\nOverall, if you would analyze survey data from Norway and the US (e.g., from the International Social Survey Project) to see if education has an effect on political trust and if you find (roughly) the same effect in both cases, then you have a strong counterargument to potential criticism that your findings might not apply to other contexts. If it works in the US and Norway, two very different countries and societies, then it likely also works in many other contexts."
  },
  {
    "objectID": "posts/case_selection2/index.html#next-steps",
    "href": "posts/case_selection2/index.html#next-steps",
    "title": "Case selection with macro-level data",
    "section": "Next steps",
    "text": "Next steps\nThis post showed how you can use macro-level data to select cases for a comparative case study, focusing on countries as cases.\nObviously, and as mentioned above, countries are not the only units that can be potential cases – regions, parties, interest groups, court rulings, or wars and conflicts can also be cases that can be compared (see also Gerring 2004). In addition, you would of course think more carefully about the variables you need to look at in a real analysis (theory is your friend here!), and this might mean that you have to look at several different datasets to get a complete picture of how your cases compare on those variables. Still, the fundamental logic – exploring data to identify relevant cases – also applies to those situations, you would only use different datasets (see e.g., this post for a list).\nThe techniques we used here were also relatively simple and purely descriptive, and there are of course more advanced methods to select cases based on a quantitative analysis for a more detailed and in-depth qualitative investigation (see Seawright and Gerring 2008)."
  },
  {
    "objectID": "posts/case_selection2/index.html#footnotes",
    "href": "posts/case_selection2/index.html#footnotes",
    "title": "Case selection with macro-level data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are of course also exceptions where the best way to answer your research question is with a single country study (Golden 2005), but this should be carefully justified.↩︎\nTo explain: We group the data by country and then let R figure out the start (min) and end (max) year per country within summarize(). Based on that result, we calculate the difference between start and end as year_range. Next, we visualize the result with ggplot, where we order the countries on the x-axis according to how many years we have data for and use geom_linerange() to indicate the time period for which a given country is covered in the dataset.↩︎"
  },
  {
    "objectID": "posts/llms/index.html",
    "href": "posts/llms/index.html",
    "title": "Using LLMs to work with Twitter data",
    "section": "",
    "text": "When people interact and communicate with each other in the political, social, or economic spheres, they often use written text. Politicians and political parties, for example, use election manifestos and campaign flyers to communicate with potential voters, and these have long been used to measure their ideological positions (Volkens et al. 2014; Laver, Benoit, and Garry 2003; Slapin and Proksch 2008). Newspapers, secondly, routinely report on economic developments, and the underlying sentiment or “tone” of these reportings can be used to get a sense of how the economy overall is developing (Ozgun and Broekel 2021). And, finally, a lot of social interaction now happens online on social media sites, where users post and engage with others’ posts. All of this taken together provides a vast amount of data (e.g., Hilbert and López 2011) that can be used to study and explain political, social, and economic behavior.\nThe catch, however, is that someone needs to analyze all this data. According to one estimate, Twitter (now X) alone produces somehwere between 500’000 and almost 5’000’000 tweets on a single day (Ulizko et al. 2021), and even a fraction of this is much more information than any individual human can process in a reasonable amount of time. Plus, the sheer amount of text data that is now available is only one of the problems one runs into. Another one is that people obviously communicate in many different languages – English, French, Chinese, German, Spanish, Russian, and so on – and most individuals are only capable of reading and analyzing a small number of languages.\nQualitative methods for analyzing text definitely run into limits here, and so do older quantitative methods like classifiers, topic models, or ideological scaling because they also require either a set of human-classified texts as training data or produce results that are not always straightforward to interpret (Grimmer and Stewart 2013; Gentzkow, Kelly, and Taddy 2019; see also Molina and Garip 2019).\nThis is where large language models (LLMs) make a big difference. As anyone who has ever tried out tools like ChatGPT or Copilot (so, everyone?) knows, LLMs are “smart” enough to process – meaning classify, translate, summarize, or check – even longer text passages according to specific criteria or demands and produce results in a format that human users can explicitly specify. Their big disadvantage is that they, as the name indicates, are large and require serious amounts of computational firepower to perform complex operations on large amounts of text.\nHowever, if one works with reasonably small amounts of short segments of text such as tweets and wants to get only relatively simple operations done – such as identifying the main topic – it is possible to use LLMs for text analysis on a regular laptop, no need for massive GPU-powered server farms. First, ollama, an open-source framework for LLMs (see https://github.com/ollama/ollama) makes it possible to run smaller LLMs one’s own laptop, for free. In addition, the mall package for R (https://mlverse.github.io/mall/) makes it possible to use ollama LLMs directly within R on a given dataset, for example to translate, classify, or otherwise “evaluate” a sequence of texts that is stored in a variable (“vector”).\nThe rest of this post shows how you can put this into action with an example analysis of tweets on immigration by French right and radical right politicians (Pietrandrea and Battaglia 2022)."
  },
  {
    "objectID": "posts/llms/index.html#text-media-and-large-language-models",
    "href": "posts/llms/index.html#text-media-and-large-language-models",
    "title": "Using LLMs to work with Twitter data",
    "section": "",
    "text": "When people interact and communicate with each other in the political, social, or economic spheres, they often use written text. Politicians and political parties, for example, use election manifestos and campaign flyers to communicate with potential voters, and these have long been used to measure their ideological positions (Volkens et al. 2014; Laver, Benoit, and Garry 2003; Slapin and Proksch 2008). Newspapers, secondly, routinely report on economic developments, and the underlying sentiment or “tone” of these reportings can be used to get a sense of how the economy overall is developing (Ozgun and Broekel 2021). And, finally, a lot of social interaction now happens online on social media sites, where users post and engage with others’ posts. All of this taken together provides a vast amount of data (e.g., Hilbert and López 2011) that can be used to study and explain political, social, and economic behavior.\nThe catch, however, is that someone needs to analyze all this data. According to one estimate, Twitter (now X) alone produces somehwere between 500’000 and almost 5’000’000 tweets on a single day (Ulizko et al. 2021), and even a fraction of this is much more information than any individual human can process in a reasonable amount of time. Plus, the sheer amount of text data that is now available is only one of the problems one runs into. Another one is that people obviously communicate in many different languages – English, French, Chinese, German, Spanish, Russian, and so on – and most individuals are only capable of reading and analyzing a small number of languages.\nQualitative methods for analyzing text definitely run into limits here, and so do older quantitative methods like classifiers, topic models, or ideological scaling because they also require either a set of human-classified texts as training data or produce results that are not always straightforward to interpret (Grimmer and Stewart 2013; Gentzkow, Kelly, and Taddy 2019; see also Molina and Garip 2019).\nThis is where large language models (LLMs) make a big difference. As anyone who has ever tried out tools like ChatGPT or Copilot (so, everyone?) knows, LLMs are “smart” enough to process – meaning classify, translate, summarize, or check – even longer text passages according to specific criteria or demands and produce results in a format that human users can explicitly specify. Their big disadvantage is that they, as the name indicates, are large and require serious amounts of computational firepower to perform complex operations on large amounts of text.\nHowever, if one works with reasonably small amounts of short segments of text such as tweets and wants to get only relatively simple operations done – such as identifying the main topic – it is possible to use LLMs for text analysis on a regular laptop, no need for massive GPU-powered server farms. First, ollama, an open-source framework for LLMs (see https://github.com/ollama/ollama) makes it possible to run smaller LLMs one’s own laptop, for free. In addition, the mall package for R (https://mlverse.github.io/mall/) makes it possible to use ollama LLMs directly within R on a given dataset, for example to translate, classify, or otherwise “evaluate” a sequence of texts that is stored in a variable (“vector”).\nThe rest of this post shows how you can put this into action with an example analysis of tweets on immigration by French right and radical right politicians (Pietrandrea and Battaglia 2022)."
  },
  {
    "objectID": "posts/llms/index.html#llms-and-ollama",
    "href": "posts/llms/index.html#llms-and-ollama",
    "title": "Using LLMs to work with Twitter data",
    "section": "LLMs and ollama",
    "text": "LLMs and ollama\n\n\n\nA llama\n\n\n\nThe LLM zoo\nThe probably most widely-known family of LLMs is the GPT series developed by OpenAI, which are what is running under ChatGPTs “hood” – but they are by far not the only ones. Nowadays, there is a proper zoo of LLMs. Some of these models are proprietary – meaning you have to pay to be able to use them – but there are also many others which are open-source and free to use for anyone. The Llama (or LLaMa) family of LLMs that was developed by Meta (the company that owns Facebook and Instagram) is one example (Touvron et al. 2023).\nLLMs also differ in size and there are generally always larger and smaller versions of a given LLM. The llama3.2 model, for example, is available in 1B and 3B versions, meaning one contains 1 billion parameters and the other 3 billion. Both versions of llama3.2, in turn, are significantly smaller than the llama3.1 models, which contain 8B, 70B, or 405B parameters. Smaller models are generally not as smart as larger models, but they also require less computing power. Therefore, if you can do a certain task with a smaller model without sacrificing (too much) quality, then that is usually worth doing. (Remember the stuff on “parsimony” from your methods course? This is what that was about.)\n\n\nInstalling ollama\nollama was created to make it easier to access all the various LLMs from their different providers. Simply put, ollama is a little program that allows you to download and run any of the long list of open-source models they have available (see https://ollama.com/search) with a few lines of code.\nYou can download and install ollama directly from their website: https://ollama.com/download.\n\n\nUsing ollama\nThere are two ways in which you can use ollama.\nThe first one is to use the ChatGPT-like chat-window that ollama comes with. Here, you choose one of the available models and then interact with it just like you would with ChatGPT or Copilot.1\nThe second and “native” way of accessing ollama is via the Terminal on Mac or the Command-Line Interface (CLI) on Windows. You can access them directly from within RStudio if you open the Terminal tab on the bottom (next to the Console tab; see Figure 1 below). When you do that, you should see another blinking cursor waiting for commands to execute. This works just like R – you type in a command, hit enter, and the computer does what you asked (or returns an error message) – only that you work with different programs and thus different languages.2\n\n\n\n\n\n\nFigure 1: Finding the Terminal in RStudio\n\n\n\nWhen you have navigated to the Terminal/CLI, you can start working with ollama. For example, to see which models you have currently installed on your computer, you would run in your Terminal or CLI (not in R):\nollama list\nMost likely, you will see no models listed since you haven’t installed any models yet.\nTo install a model, you use ollama pull &lt;model&gt;. For example, to install the llama3.1 model, you would run:\nollama pull llama3.1\nollama will then go and download the requested model – which will usually take a bit of time. Again, we are talking about large models here!\nOnce that is done, you can start chatting with your new model! To run a model, you use ollama run &lt;model&gt; – so in our case:\nollama run llama3.1\nAfter a few moments, the command line will change and you will see:\n&gt;&gt;&gt;&gt; Send a message (/? for help)\nThis is now basically a stripped-down version of ChatGPT, and it works the same way. You type in a message or request, hit enter, and the model gives you answer to the best of its ability.\nFor example, see what happens when you ask the model to “Write something funny.”:\n&gt;&gt;&gt;&gt; Write something funny.\nIf you want to stop the model, you can do so with /bye and then enter:\n&gt;&gt;&gt;&gt; /bye\nYou will then get back to the standard command line interface, where you can install and run other LLMs."
  },
  {
    "objectID": "posts/llms/index.html#running-ollama-from-within-r-with-mall",
    "href": "posts/llms/index.html#running-ollama-from-within-r-with-mall",
    "title": "Using LLMs to work with Twitter data",
    "section": "Running ollama from within R with mall",
    "text": "Running ollama from within R with mall\nWith a simple chat interface – a window like with ChatGPT or the ollama interface in the command line, we can already give our LLM some task to do. We could for example feed it a single tweet and then ask it to classify or translate it for us. But this only works when we work with a handful of tweets or other small pieces of text. If we are working with a large dataset of tweets or similar text, this approach is usually not feasible!\nA much more convenient way to use an ollama LLM is to use it programmatically: to store all the different texts we want to analyze in a dataset, import that dataset into R, and then call the LLM from witin R to let it automatically go through then entire dataset, work with all of the texts, one after the other, and then store the result – e.g., a translation or classification – as a new variable in the dataset. This is, in essence, what the mall package for R does.\nYou can install mall directly from CRAN with install.packages() – in R! Once you have done that, you load it with library().\n\nlibrary(mall)\n\nmall can use both local LLMs that are installed on your computer via ollama and remote LLMs like ChatGPT, but in the latter case only if you have access to their API (which is generally a service you need to pay for).\nFor this analysis, we use local LLMs via ollama.\nTo be able to use ollama from within R, ollama needs to be running in the background. To set this, you just need to go back to the Terminal or CLI and run ollama serve. You will then get a bit of computer gibberish and the Terminal will be busy – basically, ollama is up and running and ready to serve you a model of your choice. You can then move back to R.\nNext, we specify which LLM we want to run with the llm_use() function from mall. In our case, we only have the llama3.1 model installed, so we run that one:\n\nllm_use(\"ollama\",\"llama3.1\",\n        seed = 42)\n\n\n\n\n── mall session object \n\n\nBackend: ollama\nLLM session:\n  model:llama3.1\n\n  seed:42\n\nR session:\ncache_folder:/var/folders/9x/9t19vvdn5pv84n6k_9lwqqm40000gn/T//RtmpXjkX1a/_mall_cache651363608aac\n\n\nOnce this is taken care of, we are all set and can do our analysis.3"
  },
  {
    "objectID": "posts/llms/index.html#example-analysis-french-politicians-tweets-about-migration",
    "href": "posts/llms/index.html#example-analysis-french-politicians-tweets-about-migration",
    "title": "Using LLMs to work with Twitter data",
    "section": "Example analysis: French politicians’ tweets about migration",
    "text": "Example analysis: French politicians’ tweets about migration\nTo illustrate how one can analyze text data with mall and ollama, we will work with a dataset of tweets on migration from various French right and radical right politicans between 2011 and 2022 that were collected for the MIGR-TWIT project (see https://doi.org/10.5281/zenodo.7257708). Tweets are a good place to start when it comes to text analysis and LLMs because they are short, so computing power and processing time are less of an issue.\nBefore we import the dataset, we quickly load the tidyverse to be able to use it for data management:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nData import\nYou can import the dataset directly from the Zenodo data archive:\n\ntweets &lt;- read.csv(\"https://zenodo.org/records/7347479/files/FR-R-MIGR-TWIT-2011-2022_meta.csv?download=1\", sep = \";\")\n\nYou see directly in the Environment that the dataset contains around 40’000 tweets. We can also do a quick inspection with glimpse() to get a sense of what the variables look like:\n\nglimpse(tweets)\n\nRows: 40,112\nColumns: 44\n$ data__id                                  &lt;chr&gt; \"82713564338597888\", \"433669…\n$ data__text                                &lt;chr&gt; \"Dans cette vidéo, je dénonç…\n$ data__lang                                &lt;chr&gt; \"fr\", \"fr\", \"data__lang\", \"f…\n$ data__created_at                          &lt;chr&gt; \"2011-06-20T07:37:05.000Z\", …\n$ author__username                          &lt;chr&gt; \"@dupontaignan\", \"@dupontaig…\n$ data__author_id                           &lt;chr&gt; \"38170599\", \"38170599\", \"dat…\n$ data__conversation_id                     &lt;chr&gt; \"82713564338597888\", \"433669…\n$ data__public_metrics__retweet_count       &lt;chr&gt; \"2\", \"1\", \"data__public_metr…\n$ data__public_metrics__reply_count         &lt;chr&gt; \"1\", \"0\", \"data__public_metr…\n$ data__public_metrics__like_count          &lt;chr&gt; \"2\", \"0\", \"data__public_metr…\n$ data__public_metrics__quote_count         &lt;chr&gt; \"0\", \"0\", \"data__public_metr…\n$ data__reply_settings                      &lt;chr&gt; \"everyone\", \"everyone\", \"dat…\n$ data__possibly_sensitive                  &lt;chr&gt; \"False\", \"False\", \"data__pos…\n$ data__source                              &lt;chr&gt; \"Twitter Web Client\", \"Twitt…\n$ data__geo__place_id                       &lt;chr&gt; \"\", \"\", \"data__geo__place_id…\n$ data__referenced_tweets__type             &lt;chr&gt; \"\", \"\", \"data__referenced_tw…\n$ data__referenced_tweets__id               &lt;chr&gt; \"\", \"\", \"data__referenced_tw…\n$ data__in_reply_to_user_id                 &lt;chr&gt; \"\", \"\", \"data__in_reply_to_u…\n$ data__entities__hashtags__start           &lt;chr&gt; \"\", \"\", \"data__entities__has…\n$ data__entities__hashtags__end             &lt;chr&gt; \"\", \"\", \"data__entities__has…\n$ data__entities__hashtags__tag             &lt;chr&gt; \"\", \"\", \"data__entities__has…\n$ data__entities__mentions__start           &lt;chr&gt; \"\", \"\", \"data__entities__men…\n$ data__entities__mentions__end             &lt;chr&gt; \"\", \"\", \"data__entities__men…\n$ data__entities__mentions__username        &lt;chr&gt; \"\", \"\", \"data__entities__men…\n$ data__entities__mentions__username.1      &lt;chr&gt; \"\", \"\", \"data__entities__men…\n$ data__entities__mentions__id              &lt;chr&gt; \"\", \"\", \"data__entities__men…\n$ data__entities__urls__start               &lt;chr&gt; \"86\", \"\", \"data__entities__u…\n$ data__entities__urls__end                 &lt;chr&gt; \"105\", \"\", \"data__entities__…\n$ data__entities__urls__url                 &lt;chr&gt; \"http://t.co/J8ipeOY\", \"\", \"…\n$ data__entities__urls__expanded_url        &lt;chr&gt; \"http://dai.ly/geL6Zm\", \"\", …\n$ data__entities__urls__display_url         &lt;chr&gt; \"dai.ly/geL6Zm\", \"\", \"data__…\n$ data__entities__urls__status              &lt;chr&gt; \"200\", \"\", \"data__entities__…\n$ data__entities__urls__unwound_url         &lt;chr&gt; \"https://www.dailymotion.com…\n$ data__context_annotations__.              &lt;chr&gt; \"\", \"\", \"data__context_annot…\n$ data__context_annotations__.__id          &lt;chr&gt; \"\", \"\", \"data__context_annot…\n$ data__context_annotations__.__name        &lt;chr&gt; \"\", \"\", \"data__context_annot…\n$ data__context_annotations__.__description &lt;chr&gt; \"\", \"\", \"data__context_annot…\n$ data__attachments__media_keys__001        &lt;chr&gt; \"\", \"\", \"data__attachments__…\n$ data__attachments__media_keys__002        &lt;chr&gt; \"\", \"\", \"data__attachments__…\n$ data__attachments__media_keys__003        &lt;chr&gt; \"\", \"\", \"data__attachments__…\n$ data__attachments__media_keys__004        &lt;chr&gt; \"\", \"\", \"data__attachments__…\n$ meta__newest_id                           &lt;chr&gt; \"82713564338597888\", \"\", \"me…\n$ meta__oldest_id                           &lt;chr&gt; \"43366943381651456\", \"\", \"me…\n$ meta__result_count                        &lt;chr&gt; \"2\", \"\", \"meta__result_count…\n\n\nNote that the actual tweets – the “meat part” of the dataset – are stored in the data__text variable, and that each tweet has a unique data__id. We also have information about which politician sent out the tweet and the exact time they did so, in addition to a range of other contextual variables.\n\n\nData cleaning\nLet’s have a look at the first ten tweets:\n\ntweets |&gt; \n  slice_head(n = 10) |&gt; \n  select(data__text)\n\n                                                                                                                                     data__text\n1                                     Dans cette vidéo, je dénonçais l'arnaque des bons sentiments en matière d'immigration http://t.co/J8ipeOY\n2                                                                            Je démonte les idées reçues sur l'immigration http://dai.ly/geL6Zm\n3                                                                                                                                    data__text\n4      A 15H aux questions d'actualités, j'interogerai M. Guéant sur les faits de délinquance liés à l'immigration clandestine venue de Tunisie\n5                                                                                                                                    data__text\n6                                   Nicolas Bay invité de « Objectif Elysée » en débat sur l’immigration | Front National: http://t.co/Pd7dHD3a\n7                         Guéant reconnaît le bilan dramatique de la politique d’immigration de #Sarkozy | Front National: http://t.co/uvLXTLre\n8  Mandat de Nicolas #Sarkozy : une explosion de la fraude sociale liée à une explosion de l’immigration | Front National: http://t.co/ycJInrbH\n9   Départ de Maxime Tandonnet : Nicolas #Sarkozy se révèle en écartant de l Elysée son seul conseiller anti-immigration !: http://t.co/uGt5Dmk\n10   Marine Le Pen s'adresse aux policiers, gendarmes et douaniers de France,sur la lutte contre l'immigration clandestine: http://t.co/OoUxIjs\n\n\nIt turns out that there are some irrelevant rows containing only the variable name (data__text).\nThese rows need to be filtered out, and (just to make sure) we also filter out any observation where the data__id variable is empty:\n\ntweets |&gt; \n    filter(data__id != \"data__id\" & data__id!=\"\") -&gt; tweets\n\nIn addition, you might have noticed that the most of the tweets contain links and hashtags, and one also includes an @-symbol. The links themselves are not really relevant now, so it might be best to remove them, and we can do the same with the @ and # symbols.\nTo do that across all the tweets in one go, we use something called a regular expression (or “regex”). Simply put, a regular expression is a way to tell R (or other programming languages) to look for patterns in a given text and then do something with those pieces of text that correspond to a given pattern (e.g., delete or change).\nWriting good regular expressions is a science in and of itself (see Wickham and Grolemund 2016, chap. 14.3) and too complicated to go into detail here. Fortunately, AI chatbots like ChatGPT can help with writing them, and there is a website (https://regex101.com/) that you can use to check that the expression you got from them actually does what it is supposed to do.\nCopilot suggested the following regular expression to filter out all @ and # symbols and links starting with “http”: @|#|http?://\\\\S+. This expression (very simply put) tells R to look for the @-symbol or (|) the #-symbol or (|) a piece of text starting with “http” or “https” and followed first by “://” and then any sequence of characters that are not white space (\\S+).\nWe use this expression in the str_remove_all() function to remove these parts of each tweet and store the cleaned tweets as a new variable (clean_text):\n\ntweets |&gt; \n  mutate(clean_text = str_remove_all(data__text, \"@|#|http?://\\\\S+\")) -&gt; tweets\n\nIf we then try again, we get the first ten tweets:\n\ntweets |&gt; \n  slice_head(n = 10) |&gt; \n  select(clean_text)\n\n                                                                                                                                 clean_text\n1                                                    Dans cette vidéo, je dénonçais l'arnaque des bons sentiments en matière d'immigration \n2                                                                                            Je démonte les idées reçues sur l'immigration \n3  A 15H aux questions d'actualités, j'interogerai M. Guéant sur les faits de délinquance liés à l'immigration clandestine venue de Tunisie\n4                                                   Nicolas Bay invité de « Objectif Elysée » en débat sur l’immigration | Front National: \n5                                          Guéant reconnaît le bilan dramatique de la politique d’immigration de Sarkozy | Front National: \n6                   Mandat de Nicolas Sarkozy : une explosion de la fraude sociale liée à une explosion de l’immigration | Front National: \n7                   Départ de Maxime Tandonnet : Nicolas Sarkozy se révèle en écartant de l Elysée son seul conseiller anti-immigration !: \n8                   Marine Le Pen s'adresse aux policiers, gendarmes et douaniers de France,sur la lutte contre l'immigration clandestine: \n9      RT laprovence: Un élu quitte ses fonctions à l'UMP pour protester contre la \"frilosité\" de son parti.  UMP rebélion immigration Luca\n10                                Guéant : des mots contre l’immigration de travail mais des actes pour la favoriser comme jamais !  fn2012\n\n\nLooks like everything worked.\n\n\nTranslating tweets to English\nThe tweets may now be “clean” but they are still in French, which is not a language everyone is familiar with.\nTo get a better sense of what the tweets are about, we can use the llm_translate() function from the mall package to translate them into English. To use this function, we specify which variable we want translated and the language we want it translated into, and the function will then let the LLM we loaded earlier (llama3.1) go through the dataset and translate the individual entries.\nLet’s try it out by translating the first ten tweets into English (translating all of them would take way too much time). To limit the operation to only the first ten, we use the slice_head() function. We also record the start and finish time so that we can see how long the process takes:\n\nstart &lt;- Sys.time()\n\ntweets |&gt; \n  slice_head(n = 10) |&gt; \n  select(clean_text) |&gt; \n  llm_translate(clean_text,\n                language = \"english\") -&gt; translated\n\nend &lt;- Sys.time()\ndiff &lt;- end-start\n\nThis will then take a few moments! In my case, it takes exactly:\n\ndiff\n\nTime difference of 1.134099 mins\n\n\nThis is actually not too bad – a human translator would most likely have needed a bit more time to do this job.\nLet’s look at the translations, which are automatically stored in a new variable called .translation:\n\ntranslated |&gt; \n  select(.translation)\n\n                                                                                                                                                .translation\n1                                                                                     I'm exposing the scam of good intentions when it comes to immigration.\n2                                                                                                       Challenging common misconceptions about immigration.\n3  I will ask Mr. Guéant about the facts related to delinquency linked to clandestine immigration from Tunisia at 15H as part of current events questioning.\n4                 In this show on immigration, Nicolas Bay from the Front National Party discusses French integration policies and their effects on society.\n5                                                                        Claudie Haigneré and Jean-Pierre Chevènement were already warning about it in 2007.\n6                                                            Nicolas Sarkozy's mandate has seen a social fraud explosion linked to an immigration explosion.\n7                                      Depart of Maxime Tandonnet: Nicolas Sarkozy reveals himself by ejecting from the Elysee his sole immigration advisor.\n8        French National Front leader Marine Le Pen addresses police officers, gendarmes and customs officials on the fight against clandestine immigration.\n9                                                       A UMP elected official has resigned to protest his party's lack of engagement on immigration issues.\n10                                                                  \"Guéant speaks against immigration for work, but his actions promote it more than ever.\"\n\n\nThe fact that the tweets were originally written in French still shows, but the translation worked overall quite well – and non-francophones can now make sense of the tweets. We can see that one tweet is about police officers, another is about “social fraud”, and quite a few of them are about Nicolas Sarkozy, the former French President.\n\n\nIdentifying tweets with a specific content\nIn a real-life analysis, we may be interested in finding out how often politicians (or others) talk or tweet about a certain topic. To answer this question, we can go through our database of tweets and identify all tweets that refer to some keyword or topic of interest.\nTo do that, we can use the llm_verify() function from mall. This function works very similarly to llm_translate(): we need to specify which variable we want to be checked by the LLM and some criterion that we want to be used.\nFor example, let’s ask the LLM to identify all tweets that are related to social fraud. To do that, we simply write out the request and we also specify which name the new variable should have (pred_name = \"socialfraud\"). We save the result in a new data.frame called checked:\n\nstart &lt;- Sys.time()\ntweets %&gt;% \n  select(clean_text) %&gt;% \n  slice_head(n = 10) %&gt;% \n  llm_verify(clean_text, \"does this tweet mention social fraud?\",\n             pred_name = \"socialfraud\") -&gt; checked\n\nend &lt;- Sys.time()\ndiff &lt;- end-start\ndiff\n\nTime difference of 33.21647 secs\n\n\nThis took less than a minute.\nLet’s see what the results look like:\n\nchecked |&gt; \n  select(clean_text,socialfraud)\n\n                                                                                                                                 clean_text\n1                                                    Dans cette vidéo, je dénonçais l'arnaque des bons sentiments en matière d'immigration \n2                                                                                            Je démonte les idées reçues sur l'immigration \n3  A 15H aux questions d'actualités, j'interogerai M. Guéant sur les faits de délinquance liés à l'immigration clandestine venue de Tunisie\n4                                                   Nicolas Bay invité de « Objectif Elysée » en débat sur l’immigration | Front National: \n5                                          Guéant reconnaît le bilan dramatique de la politique d’immigration de Sarkozy | Front National: \n6                   Mandat de Nicolas Sarkozy : une explosion de la fraude sociale liée à une explosion de l’immigration | Front National: \n7                   Départ de Maxime Tandonnet : Nicolas Sarkozy se révèle en écartant de l Elysée son seul conseiller anti-immigration !: \n8                   Marine Le Pen s'adresse aux policiers, gendarmes et douaniers de France,sur la lutte contre l'immigration clandestine: \n9      RT laprovence: Un élu quitte ses fonctions à l'UMP pour protester contre la \"frilosité\" de son parti.  UMP rebélion immigration Luca\n10                                Guéant : des mots contre l’immigration de travail mais des actes pour la favoriser comme jamais !  fn2012\n   socialfraud\n1            0\n2            0\n3            0\n4            0\n5            0\n6            1\n7            0\n8            0\n9            0\n10           0\n\n\nWe know already that only tweet number 6 was about social fraud and that is also what the model found. This is good because this gives us confidence that when we scale the operation up – let it run on the entire dataset – we should get correct results.4\nWe can also see if the LLM can identify if tweets are about specific politicians. Let’s for example try to identify tweets that are in some way about Marine Le Pen, the leader of the Rasssemblement National (RN), France’s main radical right-party:\n\nstart &lt;- Sys.time()\ntweets %&gt;% \n  select(clean_text) %&gt;% \n  slice_head(n = 10) %&gt;% \n  llm_verify(clean_text, \"does this tweet mention Marine Le Pen?\",\n             pred_name = \"lepen\") -&gt; checked\n\nend &lt;- Sys.time()\ndiff &lt;- end-start\ndiff\n\nTime difference of 31.03894 secs\n\n\nIf we check the new result, we can see that the model did correctly identify the one tweet that was about Marine Le Pen:\n\nchecked |&gt; \n  select(clean_text,lepen)\n\n                                                                                                                                 clean_text\n1                                                    Dans cette vidéo, je dénonçais l'arnaque des bons sentiments en matière d'immigration \n2                                                                                            Je démonte les idées reçues sur l'immigration \n3  A 15H aux questions d'actualités, j'interogerai M. Guéant sur les faits de délinquance liés à l'immigration clandestine venue de Tunisie\n4                                                   Nicolas Bay invité de « Objectif Elysée » en débat sur l’immigration | Front National: \n5                                          Guéant reconnaît le bilan dramatique de la politique d’immigration de Sarkozy | Front National: \n6                   Mandat de Nicolas Sarkozy : une explosion de la fraude sociale liée à une explosion de l’immigration | Front National: \n7                   Départ de Maxime Tandonnet : Nicolas Sarkozy se révèle en écartant de l Elysée son seul conseiller anti-immigration !: \n8                   Marine Le Pen s'adresse aux policiers, gendarmes et douaniers de France,sur la lutte contre l'immigration clandestine: \n9      RT laprovence: Un élu quitte ses fonctions à l'UMP pour protester contre la \"frilosité\" de son parti.  UMP rebélion immigration Luca\n10                                Guéant : des mots contre l’immigration de travail mais des actes pour la favoriser comme jamais !  fn2012\n   lepen\n1      0\n2      0\n3      0\n4      0\n5      0\n6      0\n7      0\n8      1\n9      0\n10     0\n\n\nAgain, the model seems to perform well enough that we could let it run on the entire dataset and still be reasonably confident that we get useable results.\n\n\nExtracting information\nSometimes we want to not only know if a given piece of text contains something we are interested. Instead, we want to extract that particular text element and use it for a further analysis (e.g., a word cloud). To do that, we can use the llm_extract() function. Here, we again name the variable that we want examined and the piece of information that the model is supposed to look for and extract, and we can also specify a name for the new variable that will contain the extracted text elements.\nFor example, let’s try to extract the French politicians that are named in the first ten tweets:\n\nstart &lt;- Sys.time()\ntweets %&gt;% \n  select(clean_text) %&gt;% \n  slice_head(n = 10) %&gt;% \n  llm_extract(clean_text, \"French politician\",\n              pred_name = \"politician\") -&gt; extracted\n\nend &lt;- Sys.time()\ndiff &lt;- end-start\ndiff\n\nTime difference of 35.08848 secs\n\n\nThis did not take very long, and a human coder would probably need longer just to be able to write down the names of the politicians.\nHowever, the results are not fully convincing: it seems the model hallucinated and stated a name even when the tweet did not contain actual names:\n\nextracted |&gt; \n  select(clean_text, politician)\n\n                                                                                                                                 clean_text\n1                                                    Dans cette vidéo, je dénonçais l'arnaque des bons sentiments en matière d'immigration \n2                                                                                            Je démonte les idées reçues sur l'immigration \n3  A 15H aux questions d'actualités, j'interogerai M. Guéant sur les faits de délinquance liés à l'immigration clandestine venue de Tunisie\n4                                                   Nicolas Bay invité de « Objectif Elysée » en débat sur l’immigration | Front National: \n5                                          Guéant reconnaît le bilan dramatique de la politique d’immigration de Sarkozy | Front National: \n6                   Mandat de Nicolas Sarkozy : une explosion de la fraude sociale liée à une explosion de l’immigration | Front National: \n7                   Départ de Maxime Tandonnet : Nicolas Sarkozy se révèle en écartant de l Elysée son seul conseiller anti-immigration !: \n8                   Marine Le Pen s'adresse aux policiers, gendarmes et douaniers de France,sur la lutte contre l'immigration clandestine: \n9      RT laprovence: Un élu quitte ses fonctions à l'UMP pour protester contre la \"frilosité\" de son parti.  UMP rebélion immigration Luca\n10                                Guéant : des mots contre l’immigration de travail mais des actes pour la favoriser comme jamais !  fn2012\n          politician\n1  jean marie le pen\n2      marine le pen\n3             guéant\n4        nicolas bay\n5            sarkozy\n6    nicolas sarkozy\n7    nicolas sarkozy\n8      marine le pen\n9               luca\n10             clerk\n\n\nOne thing we can try to keep the model from hallucinating is to add an additional prompt to instruct it to assign an NA whenever no politican is explicitly mentioned. We can do that with the additional_prompt() option (“argument”) within llm_extract():\n\nstart &lt;- Sys.time()\ntweets %&gt;% \n  select(clean_text) %&gt;% \n  slice_head(n = 10) %&gt;% \n  llm_extract(clean_text, \"French politician\",\n              additional_prompt = \"return NA if the tweet does not explicitly mention a widely known French politician\",\n              pred_name = \"politician\") -&gt; extracted\n\nend &lt;- Sys.time()\ndiff &lt;- end-start\ndiff\n\nTime difference of 33.62775 secs\n\n\n\nextracted |&gt; \n  select(clean_text, politician)\n\n                                                                                                                                 clean_text\n1                                                    Dans cette vidéo, je dénonçais l'arnaque des bons sentiments en matière d'immigration \n2                                                                                            Je démonte les idées reçues sur l'immigration \n3  A 15H aux questions d'actualités, j'interogerai M. Guéant sur les faits de délinquance liés à l'immigration clandestine venue de Tunisie\n4                                                   Nicolas Bay invité de « Objectif Elysée » en débat sur l’immigration | Front National: \n5                                          Guéant reconnaît le bilan dramatique de la politique d’immigration de Sarkozy | Front National: \n6                   Mandat de Nicolas Sarkozy : une explosion de la fraude sociale liée à une explosion de l’immigration | Front National: \n7                   Départ de Maxime Tandonnet : Nicolas Sarkozy se révèle en écartant de l Elysée son seul conseiller anti-immigration !: \n8                   Marine Le Pen s'adresse aux policiers, gendarmes et douaniers de France,sur la lutte contre l'immigration clandestine: \n9      RT laprovence: Un élu quitte ses fonctions à l'UMP pour protester contre la \"frilosité\" de son parti.  UMP rebélion immigration Luca\n10                                Guéant : des mots contre l’immigration de travail mais des actes pour la favoriser comme jamais !  fn2012\n        politician\n1               NA\n2               NA\n3     brice guéant\n4      nicolas bay\n5          sarkozy\n6  nicolas sarkozy\n7  nicolas sarkozy\n8    marine le pen\n9     lucas graham\n10          guéant\n\n\nThis result is better (the first two tweets are correctly given NAs), but there are still some issues with the information extracted from the latter tweets.\nThis goes to show that LLMs, even though they are powerful, are not perfect. So the advice by Grimmer and Steward (2013, 271) – “validate, validate, validate” – is still relevant.\nOne potential way to fix this could be to be even more specific in the additional prompt or to simply try out a different, perhaps more powerful and accurate model."
  },
  {
    "objectID": "posts/llms/index.html#conclusion",
    "href": "posts/llms/index.html#conclusion",
    "title": "Using LLMs to work with Twitter data",
    "section": "Conclusion",
    "text": "Conclusion\nThis post showed you how you can analyze political tweets using open-source large language models in R with the mall package and ollama. While LLMs themselves are of course not even remotely close to being “simple” models, mall and ollama make it quite simple to use them for research projects.\nOverall, the model we used, llama3.1, performed reasonably well when it comes to identifying if tweets are about a specific topic or politician, but struggled with correctly extracting information from the tweets. The second issue is something you can come across, and it can make sense to try out different models to see which one produces the most convincing results. ollama offers you a wide variety of free-to-use models that you can play around with.\nThe mall package also provides a few more functions that do other relevant operations such as classifying pieces of text into groups depending on their contents (llm_classify()), extracting the tone or sentiment of a piece of text (llm_sentiment()), or also a custom request with llm_custom(). In my (very limited) experience, some of these functions work better with longer pieces of text such as a speech given by a politican. I found this to be especially the case with llm_classify().\nObviously, when you work with longer pieces of text, your analyses with LLMs can take a lot longer than a few simple analyses with short tweets – so plan accordingly, and make sure to always test analyses on subset of your dataset to see if you get sensible results before you run them on the entire dataset (and potentially waste a lot of time).\nIf you’re now very motivated to do some analysis of your own but wondering where you could find relevant text data, you are in luck: Erik Gahner’s “Dataset of Political Datasets” includes a list of datasets of political speeches from different countries and international institutions (https://github.com/erikgahner/PolData?tab=readme-ov-file#political-speeches-and-debates).\nBefore you get started with one of these larger datasets, it can also make sense to learn a bit more about how to handle text data in R so that you are prepared in case you need to do any data cleaning or management. The article by Welbers et al. (2017), chapter 13 in Urdinez and Cruz (2020), or the book on text analysis by Silge and Robinson (2017) are good places to start.5"
  },
  {
    "objectID": "posts/llms/index.html#footnotes",
    "href": "posts/llms/index.html#footnotes",
    "title": "Using LLMs to work with Twitter data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou might notice that things can take a lot longer when you run an LLM on your laptop compared to when you use OpenAI’s servers, and that the results are not always as good as the ones you get from ChatGPT. This goes to show just how much there is happening behind the curtains when you use ChatGPT or Copilot.↩︎\nSee https://github.com/ollama/ollama for an overview over the basic commands for ollama.↩︎\nI also specify a “seed” number to make my analysis reproducible on my end. This is not strictly necessary (see here for an explanation: https://mlverse.github.io/mall/articles/caching.html).↩︎\nThe model sometimes returns “invalid output” that is stored as a missing value (NA). If this is a persistent and significant problem in your own analysis, you might have to check the quality of your text data and consider modifying your prompt (see also further below).↩︎\nSee also https://www.tidytextmining.com/.↩︎"
  },
  {
    "objectID": "posts/case_selection/index.html",
    "href": "posts/case_selection/index.html",
    "title": "Research design, case selection, and bullshit",
    "section": "",
    "text": "Most if not all of the methods courses you take as a university student will have some component on “case selection” or “research design”, where you talk about technical and abstract concepts like “bias” and “inference”. While this stuff may sound dry, abstract, academic, and “not relevant in the Real World”, it is at its core really about one major thing: How to avoid bullshitting yourself and others.\nBullshit is most likely a term you are familiar with: Claims or statements that sound kind of plausible but are inherently erroneous and misleading (Bergstrom and West 2021). Bullshit can be intentional, which happens when someone tries to mislead someone, but it can also happen by accident. In either case, bullshit can do serious harm: It can lead people (or companies and institutions) to make decisions that cost them real money, and it can lead them to avoid decisions that would be beneficial. This can range from buying a used car with lots of hidden defects to basing a company’s marketing strategy on biased and misleading customer data.\nBullshit is directly related to evidence. Usually, when people bullshit, they present some evidence – but the evidence they present is flawed: It is incomplete in a distorting way, leaving out a relevant part of the whole picture and thereby misrepresents the true state of the world. By being misleading and biased, such evidence leads others to make irrational and misinformed decisions (“inferences”). Again, this can be intentional, but it can also happen by accident.\nStudents can produce bullshit, too. This happens for example when students select cases for the comparative case study they are doing in their thesis project in way that it distorts their result (which happens quite a lot). These students most likely did not want to deceive anyone, they just either did not pay enough attention in their methods courses, they might think that case selection and bias do not apply to qualitative research, or they might simply see a careful case selection and research design as an inconvience they want to skip over. In any case, they are still involuntarily and unknowingly producing bullshit.\nAs a soon-to-be academically trained professional, a significant part of your future work will be to help others to identify and avoid bullshit (e.g., job applications from people who are unqualified but try to hide it, investment decisions that are financially harmful to your company but beneficial to the seller), but also to produce non-bullshit (valid reports, evaluations, white papers, etc.) that others can use to make good decisions.\nAnd this is why you need to learn about research design and case selection strategies as a student in any empirical scientific discipline – a good research design is simply a strategy for how to select your evidence (quantitative or qualitative) in a way that your project does not produce misleading bullshit. Learning about good research design is also helpful because it trains you to be a bullshit detector."
  },
  {
    "objectID": "posts/case_selection/index.html#bullshit-and-you",
    "href": "posts/case_selection/index.html#bullshit-and-you",
    "title": "Research design, case selection, and bullshit",
    "section": "",
    "text": "Most if not all of the methods courses you take as a university student will have some component on “case selection” or “research design”, where you talk about technical and abstract concepts like “bias” and “inference”. While this stuff may sound dry, abstract, academic, and “not relevant in the Real World”, it is at its core really about one major thing: How to avoid bullshitting yourself and others.\nBullshit is most likely a term you are familiar with: Claims or statements that sound kind of plausible but are inherently erroneous and misleading (Bergstrom and West 2021). Bullshit can be intentional, which happens when someone tries to mislead someone, but it can also happen by accident. In either case, bullshit can do serious harm: It can lead people (or companies and institutions) to make decisions that cost them real money, and it can lead them to avoid decisions that would be beneficial. This can range from buying a used car with lots of hidden defects to basing a company’s marketing strategy on biased and misleading customer data.\nBullshit is directly related to evidence. Usually, when people bullshit, they present some evidence – but the evidence they present is flawed: It is incomplete in a distorting way, leaving out a relevant part of the whole picture and thereby misrepresents the true state of the world. By being misleading and biased, such evidence leads others to make irrational and misinformed decisions (“inferences”). Again, this can be intentional, but it can also happen by accident.\nStudents can produce bullshit, too. This happens for example when students select cases for the comparative case study they are doing in their thesis project in way that it distorts their result (which happens quite a lot). These students most likely did not want to deceive anyone, they just either did not pay enough attention in their methods courses, they might think that case selection and bias do not apply to qualitative research, or they might simply see a careful case selection and research design as an inconvience they want to skip over. In any case, they are still involuntarily and unknowingly producing bullshit.\nAs a soon-to-be academically trained professional, a significant part of your future work will be to help others to identify and avoid bullshit (e.g., job applications from people who are unqualified but try to hide it, investment decisions that are financially harmful to your company but beneficial to the seller), but also to produce non-bullshit (valid reports, evaluations, white papers, etc.) that others can use to make good decisions.\nAnd this is why you need to learn about research design and case selection strategies as a student in any empirical scientific discipline – a good research design is simply a strategy for how to select your evidence (quantitative or qualitative) in a way that your project does not produce misleading bullshit. Learning about good research design is also helpful because it trains you to be a bullshit detector."
  },
  {
    "objectID": "posts/case_selection/index.html#bullshit-and-case-selection",
    "href": "posts/case_selection/index.html#bullshit-and-case-selection",
    "title": "Research design, case selection, and bullshit",
    "section": "Bullshit and case selection",
    "text": "Bullshit and case selection\nAs mentioned, comparative case studies can a major source of misleading evidence if they are not based on a proper case selection strategy, i.e., research design (King, Keohane, and Verba 1994, Geddes1990). This is often the case with student theses, but one occasionally also sees this in “adult” research.\nWe also know two main reasons for why a given case study can be faulty and produces misleading evidence:\n\nSelection on the dependent variable\nAn insufficient number of cases\n\nLet’s go over each of them to illustrate how they produce misleading results.\n\nSelection on the dependent variable\nLet’s say you are a student who is interested in the causes of war – why do some countries go to war with each other? – and you want to do a qualitative case study to figure out what is going on. Your intuition may then lead you to select two or three relevant cases (wars) where countries did fight each other and look in detail at what led up to each war and why countries chose to engage.\nAs another example, let’s say you want to find out why some countries become economically wealthy. Intuitively, you decide to select two or three countries that are wealthy and study them really, really closely using super-sophisticated qualitative methods and highly detailed evidence to see what is behind their wealth.\nIn both cases, you select on the dependent variable, meaning you only select positive cases (did go to war) or “high” cases (high wealth) – but you ignore the negative or “low” cases, the countries that did not go to war or did not become wealthy, even though they plausibly could have done either of those things. Both can really lead you to completely misleading results (Geddes 1990).\nTo see why selection on the dependent variable distorts your results, have a look at the interactive plot below:\n\nThe plot shows the relationship between two generic variables, x (the explanatory variable) and y (the dependent variable)\nAs you can clearly see, the relationship is positive: More x means more y\n\nNow play around with the three sliders to see how different case selection strategies (limiting the number of cases, i.e., going from large-n quantitative toward small-n qualitative research, limiting the range of the x variable, and limiting the range of the y variable) affects the relationsip between the two variables.\n\ntplot_dat = transpose(plot_dat)\n\n\n\n\n\n\n\nviewof n_obs = Inputs.range(\n  [2, 200], \n  {value: 200, step: 1, label: \"Number of observations:\"}\n)\nviewof range_x = Inputs.range(\n  [1,100],\n  {value: 100, step: 1, label: \"Max. range of explanatory variable (X)\"}\n)\nviewof range_y = Inputs.range(\n  [14,180],\n  {value: 180, step: 1, label: \"Max. range of dependent variable (Y)\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsliced = tplot_dat.slice(0,n_obs)\nfiltered = sliced.filter(function(sliced) {\n  return range_x &gt; sliced.xvar &&\n         range_y &gt; sliced.yvar;\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot.plot({\n  marks: [\n    Plot.dot(filtered, {x: \"xvar\", y: \"yvar\", fill: \"cornflowerblue\"}),\n    Plot.dot(tplot_dat, {x: \"xvar\", y: \"yvar\", fill: \"currentColor\", fillOpacity: 0.2}),\n    Plot.linearRegressionY(filtered, {x: \"xvar\", y: \"yvar\", stroke: \"cornflowerblue\"}),\n    Plot.linearRegressionY(tplot_dat, {x: \"xvar\", y: \"yvar\", stroke: \"currentColor\"})\n],\n    x: {label: \"Explanatory variable (X)\"},\n    y: {label: \"Dependent variable (Y)\"},\n    title: \"How the evidence you select affects the answers you get\",\n    caption: \"Shaded areas indicate 95% confidence intervals.\"\n})\n\n\n\n\n\n\nYou should notice that limiting the number of observations is not inherently problematic. Sure, the line becomes wobbly and the confidence interval becomes larger (your conclusions become less certain), but your results still reflect the true relationship reasonably well unless you go for really low numbers of observations. The same applies if you limit the range of the explanatory x variable – the positive relationship stays more or less the same, unless your selection becomes very restrictive.\nBut things are very different when you limit the range of the y variable: The relationship becomes flatter and flatter – which completely misrepresents the true relationship in the data. The result is bullshit.\n\n\n\n\n\n\nThe main lesson:\n\n\n\nIf you want to find out the causes of war, you need to study cases where war happened – and cases where war could have happened, but did not. Similarly, if you want to find the causes of national wealth, you need to study countries that did get wealthy and those that could have gotten wealthy but did not. You need variation in your dependent variable.\n\n\n\n\nInsufficient number of cases\nIf the example above gave you the idea that limiting the number of cases – going from quant to qual – is not by itself a problem as long as you don’t select on the dependent variable, then that is correct (see also King, Keohane, and Verba 1994): Both qualitative and quantitative studies can produce valid results as long as they are based on a valid research design – and they can both produce misleading bullshit results if the research design is flawed.\nStill, there can be situations where your number of cases can be too small.\nTo give you a really simple example, let’s say you have a friend called Tim who is really into tennis. As is always the case, he sometimes wins matches and sometimes he loses them. You are interested in finding out why – what causes him to win and lose – and you you think that, theoretically speaking, it could be either the weather or the food he had for lunch that makes the difference.\nYou observe him playing tennis on two different days and note down your results in a table:\n\n\n\nTable 1: First round of observations\n\n\n\n\n\n\nDay 1\nDay 2\n\n\n\n\nWeather\nSunny\nCloudy\n\n\nLunch\nMeat\nFish\n\n\n\n\n\n\n\nResult\nLost\nWon\n\n\n\n\n\n\nHave a look at the findings: Can you clearly tell why Tim won on day 2 but lost on day 1?\nNow imagine that you add a third day of observations and your new results look like this:\n\n\n\nTable 2: Adding a third day of observations\n\n\n\n\n\n\nDay 1\nDay 2\nDay 3\n\n\n\n\nWeather\nSunny\nCloudy\nSunny\n\n\nLunch\nMeat\nFish\nFish\n\n\n\n\n\n\n\n\nResult\nLost\nWon\nLost\n\n\n\n\n\n\nNow you should be able to see a pattern: On the two sunny days, Tim lost; on the one cloudy day, however, he won. On the other hand, he had fish on days 2 and 3, and both won and lost on both of these days. So you see a relationship between his tennis success and the weather but no relationship with his food.\nThe main point is: The first research design in this example, which relied on two cases (days of observations) was indeterminate (King, Keohane, and Verba 1994): It had two few observations or data points relative to the number of variables: two potential reasons (or explanatory variables) and two observations. No ammount of in-depth “looking at the data” is going to tell the answer here because the “dataset” is simply not large enough to reveal a pattern. Once you add a third observation – you have two variables but three cases – the pattern emerges and you can give at least a preliminary answer to the question.\nIn other words, the first research design produces bullshit: It leads you and others to believe that there is no pattern, when in fact there really is one. The second one is valid because it at least makes it possible for a pattern to emerge and be visible.\n\n\n\n\n\n\nThe main lesson:\n\n\n\nYou always need to have one more observations than you have potential causes or explanatory variables. Anything else is likely misleading and will end with an unclear conclusion.\n\n\n\n\nResearch and case selection designs that avoid bullshit\nBy now you should hopefully understand the meaning (and importance) of the following statement: the evidence you select affects the answers you get (Geddes 1990). Faulty selection techniques produce faulty, misleading evidence – bullshit. This applies to both qualitative and quantitative research.\nIf you want to learn how you can design your research in a way that it does not produce misleading evidence, you are in luck: There are many different textbooks and articles that show you how to do this, for example:\n\nChapter 9 in Ringdal (2018)\nChapters 2 and 3 in Landman (2003)\nThe book by King et al. (1994)\nThe article by Geddes (1990)\nThe article by Seawright & Gerring (2008)\nThe book by Przeworski & Teune (1970)\nThe book by Gerring (2007)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Testing big ideas with simple methods",
    "section": "",
    "text": "Many students have grand ideas – e.g., about the role of class, social norms, political institutions, or gender in politics, society, or the economy – but they don’t quite know how to actually test these ideas in an empirical analysis. Often, the problem seems to be that they think they need to do something very complicated, similar to what “adult” researchers publish in academic journal articles. But this is wrong: Many big ideas can be tested with (relatively) simple methods. This blog shows how this works in practice and with open-source software (R).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing LLMs to work with Twitter data\n\n\n\nLLMs\n\nOllama\n\nSocial media data\n\nText\n\nImmigration\n\n\n\n\n\n\n\n\n\nSep 10, 2025\n\n\nCarlo Knotz\n\n\n\n\n\n\n\n\n\n\n\n\nStudying conflict and violence across countries\n\n\n\nMacro\n\nUCDP\n\nComparing countries\n\nPeace & conflict\n\nPolitical science\n\n\n\n\n\n\n\n\n\nJun 14, 2025\n\n\nCarlo Knotz\n\n\n\n\n\n\n\n\n\n\n\n\nCase selection with macro-level data\n\n\n\nCase selection\n\nMacro\n\nCPDS\n\nResearch design\n\nComparing countries\n\n\n\nHow to use macro-level data to select countries for a comparative case study\n\n\n\n\n\nJun 10, 2025\n\n\nCarlo Knotz\n\n\n\n\n\n\n\n\n\n\n\n\nResearch design, case selection, and bullshit\n\n\n\nCase selection\n\nResearch design\n\n\n\nWhy you should pay attention to your research design and case selection (unless you want to become a bullshit salesperson)\n\n\n\n\n\nJun 9, 2025\n\n\nCarlo Knotz\n\n\n\n\n\n\n\n\n\n\n\n\n‘I can’t find any data!’\n\n\n\nData\n\nPolitical science\n\nSociology\n\n\n\n\n\n\n\n\n\nApr 6, 2025\n\n\nCarlo Knotz\n\n\n\n\n\n\n\n\n\n\n\n\nComparing countries with macro-level data\n\n\n\nMacro\n\nCPDS\n\nComparing countries\n\nEconomics\n\nPolitical science\n\nSociology\n\n\n\n\n\n\n\n\n\nMar 30, 2025\n\n\nCarlo Knotz\n\n\n\n\n\n\n\n\n\n\n\n\nHow vulnerable are workers to globalization (and what effects does this have)?\n\n\n\nSurvey data\n\nGlobalization\n\nMerging data\n\nESS\n\nPolitical science\n\nSociology\n\nEconomics\n\n\n\n\n\n\n\n\n\nMar 10, 2025\n\n\nCarlo Knotz\n\n\n\n\n\n\n\n\n\n\n\n\nComparing people’s behavior and attitudes across countries\n\n\n\nSurvey data\n\nComparing countries\n\nGender\n\nESS\n\nPolitical science\n\nSociology\n\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\nCarlo Knotz\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring class with survey data\n\n\n\nSurvey data\n\nClass\n\nISSP\n\nPolitical science\n\nSociology\n\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\nCarlo Knotz\n\n\n\n\n\nNo matching items"
  }
]