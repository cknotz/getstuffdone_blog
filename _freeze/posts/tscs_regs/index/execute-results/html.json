{
  "hash": "f94433757c3434628e7b0b73c34c21ac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regression analyses with macro-level data\"\nauthor: \"Carlo Knotz\"\nbibliography: /Users/carloknotz/Documents/BibDesk_library/library.bib\nformat:\n  html:\n    toc: true\n    toc-depth: 4\ndraft: true\ndate: \"2025-10-10\"\nlightbox: true\nlang: en\ncategories:\n  - Macro\n  - CPDS\n  - Comparing countries\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.cell}\n\n:::\n\n\n## Macro-level questions and analyses (again)\n\nThis is another post on how to analyze country-level data, adding to the earlier post on how to do descriptive analyses ([this one](https://cknotz.github.io/getstuffdone_blog/posts/compa_countries/)) and to the one on merging macro-level data ([this one](https://cknotz.github.io/getstuffdone_blog/posts/merging/)). What the earlier posts did not go into is how to do regression analyses with cross-country data, and this is what we are going to look into here.\n\nRegression analyses with cross-country data work in principle like regression analyses with other types of data -- you specify a model that relates your dependent variable to one or more predictors and control variables, estimate the correct regression model, and interpret the effects.\n\nWhat makes macro-level analyses more complicated, however, is that macro-level data usually have a **time dimension**: They cover different countries *over some period of time*. This can range from a few decades like in the case of the *Comparative Political Data Set* [@CPDS24] to multiple centuries like in the case of the *V-Dem* dataset [@Lindbergetal2014]. This is also why we call these types of datasets *time series cross-sectional* (TSCS) data: We have a cross-section of countries that are observed at multiple points in time [see also @Beck2001].\n\nThis additional time dimension brings benefits (we can study changes over time and we simply have more observations to work with), but it also makes the dataset and therefore the analysis more complex. In political science, there was quite a discussion among methdologists about how to correctly analyze TSCS data [see @Beck2001;@Beck1995;@Beck1996;@Beck2011;@Beck1998a;@Becketal2006;@Beck2008;@Beck2011;@Stimson1985;@Plumperetal2005;@Plumpertroeger2007;@DeBoef2008;@Wilson2007], and many pointed out that results of regression analyses with TSCS data often change drastically after seemingly minor changes in model specifications [@Wilson2007]. \n\nTSCS regression analyses do become easier (and more predictable and stable) if one understands a few core concepts:\n\n - Cross-sectional and longitudinal variation\n - [Simpson's Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)\n - Fixed- vs. random- vs. between-effects regression models\n - Integration & cointegration (or, relatedly, unit roots and stationarity/non-stationarity)\n \nThe last ones, integration and co-integration, are concepts from time series analysis methods [see e.g., @Box-Steffensmeieretal2014] and they refer, simply put, to how variables change over time: Some variables change smoothly over time and tend to return to a given stable equilibrium value, while others behave erratically and change in sudden (seemingly random) \"fits and starts\". The former are considered \"stationary\", the latter are considered to have \"unit roots\". There is an older (and very short) article that illustrates the concept of unit roots, integration, and co-integration brilliantly with an analogy of a drunk and her dog [@Murray1994], and I would refer you to that one for further reading. De Doef & Keele [-@DeBoef2008] and Beck & Katz [-@Beck2011] show how you can deal with non-stationary variables in a regression analysis [see also @Box-Steffensmeier2016;@Ennsetal2016;@Grant2016;@Lebo2017].\n \nThe rest of this post will focus on the first three concepts and techniques, starting with the different types of variation and how they can generate misleading results if they are not modelled correctly. Finally, we will go over three different regression models that capture different forms of variation in TSCS data and when to use them. We will the *Comparative Political Data Set* [@CPDS24] to illustrate the main points with real-life data.\n\n## Variation in TSCS data\n\nTime series cross-sectional data contain observations of multiple *units* (usually countries) over some period of *time* (usually years). This means that this type of data can capture two basic forms of variation [see also @Kellstedt2018, Chapter 2.3]:\n\n - Differences between *units*: Cross-sectional variation\n - Changes over *time*: Longitudinal variation\n\nTo make this more concrete, we can look at some real-life examples from the *Comparative Political Data Set* [CPDS; @CPDS24] and use the simple data visualizations techniques that were covered in the [earlier blog post](https://cknotz.github.io/getstuffdone_blog/posts/compa_countries/).^[I saved the dataset as `cpds_2024.dta`. You might have saved it under a different name, in which case you obviously need to adjust that in your code.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ntheme_set(theme_classic())\n\ncpds <- haven::read_dta(\"cpds_2024.dta\")\n```\n:::\n\n\nWe will focus on two variables from the dataset:\n\n - `structur`, which measures the \"rigidity\" of countries' political instititions, a.k.a., the number of \"checks and balances\" that make introducing reforms easier or more difficult [see also @Huber1993;@Immergut1992a;@Immergut1990;@Tsebelis2002]\n - `oldage_pmp`, which measures the amount of government spending on old-age pensions in percent of countries' gross domestic product [see also @Adema2009;@Castles2007]\n\n### Cross-sectional variation\n\nOne way to look at cross-sectional variation is to look at countries' average values on the two variables over the entire period of time for which we have observations.\n\n\n::: {#fig-csvar .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ncpds |> \n  group_by(iso) |> \n  summarise(avg_cons = mean(structur, na.rm = T)) |> \n  ggplot(aes(x = avg_cons, y = reorder(iso, avg_cons))) +\n    geom_linerange(aes(xmin = 0, xmax = avg_cons),\n                 color = \"grey\") +\n    geom_point() +\n    labs(x = \"Constitutional structure (avg.)\",\n         y = \"\")\n\ncpds |> \n  filter(year>=1980) |> \n  group_by(iso) |> \n  summarise(avg_pen = mean(oldage_pmp, na.rm = T)) |> \n  ggplot(aes(x = avg_pen, y = reorder(iso, avg_pen))) +\n    geom_linerange(aes(xmin = 0, xmax = avg_pen),\n                   color = \"grey\") +\n    geom_point() +\n    labs(x = \"Public pension spending (% GDP, avg.)\",\n         y = \"\")\n```\n\n::: {.cell-output-display}\n![Constitutional structure](index_files/figure-html/fig-csvar-1.png){#fig-csvar-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Public spending on pensions](index_files/figure-html/fig-csvar-2.png){#fig-csvar-2 width=672}\n:::\n\nCross-sectional variation\n:::\n\n\n@fig-csvar shows the results. On the left side, you see that countries differ in the \"rigidity\" of their political institutions. The United States and Switzerland stand out as countries with very rigid constitutions, where introducing reforms is very difficult, and this corresponds to what we know from more qualitative observations [@Obinger2002;@Obama2016]. On the other end are a number of countries in different parts of Europe but also Israel, where there are few constraints. This does not mean that these are dictatorships, however, it just means that there are few formal constraints on what a governing majority in parliament can do in day-to-day policymaking.\n\nThe other graph on the right shows that countries also differ quite a bit in how much they spend, on average, on old-age pensions. Conservative and southern European countries top the list, while more market-liberal countries like the United States, Canada, or Ireland are at the bottom.\n\n### Longitudinal variation\n\nObviously, by averaging the data by country, we brush away all the finer longitudinal changes over time within the different countries. To visualize this, we can use line graphs that are separated by country, as shown in @fig-tsvar.\n\n\n::: {#fig-tsvar .cell layout-ncol=\"1\"}\n\n```{.r .cell-code}\ncpds |> \n  drop_na(structur) |> \n  ggplot(aes(x = year, y = structur)) +\n    geom_line() +\n    facet_wrap(~iso) +\n    labs(x = \"\", y = \"Constitutional structure\")\n\ncpds |> \n  filter(year>=1980) |> \n  ggplot(aes(x = year, y = oldage_pmp)) +\n    geom_line() +\n    facet_wrap(~iso) +\n    labs(x = \"\", y = \"Pension spending (%GDP)\") +\n    scale_x_continuous(breaks = seq(1980,2020,20))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![Constitutional structure](index_files/figure-html/fig-tsvar-1.png){#fig-tsvar-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Public spending on pensions](index_files/figure-html/fig-tsvar-2.png){#fig-tsvar-2 width=672}\n:::\n\nLongitudinal variation\n:::\n\n\nWhen you now look at the graph showing countries' constitiutional rgidity, you see that there are barely any changes over time. France is the big exception here since it introduced several constitutional changes over a decade or so, and there have also been changes in Belgium, Croatia, Hungary, Italy, New Zealand, and Sweden -- but, overall, there is mainly stability.\n\nThis contrasts with the pattern in the other graph that shows the develpment of public pension spending in each country. Many countries experienced significant increases over time (e.g., Greence, Portugal, Italy, Japan, Finland, France, or Switzerland), while spending stayed more constant in other countries (e.g., Poland, Germany, or the Netherlands). In all cases, however, there are small year-to-year changes, which is a clear difference to the completely flat lines in the graph above.\n\n\n## Simpson's Paradox\n\nThe fact that TSCS data contain both cross-sectional and longitudinal variation makes regression analyses more challenging than when one uses a \"simple\" cross-sectional dataset like survey data from the *European Social Survey*.\n\n\n::: {#fig-simp .cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![OLS regression with cross-sectional data](index_files/figure-html/fig-simp-1.png){#fig-simp-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Simpson's Paradox](index_files/figure-html/fig-simp-2.png){#fig-simp-2 width=672}\n:::\n\nOLS regression & Simpson's Paradox\n:::\n\n\nA normal regression model basically just draws a line through the data. The graph on the left in @fig-simp illustrates this with data from the `mtcars` dataset, where each observation is a car and cars differ in the strength of their engines (measured in horsepower) and in their fuel efficiency (measured in miles-per-gallon). Unsurprisingly, a more powerful engine with more horsepower means less fuel efficiency. In this case, the simple regression model works because the data are purely cross-sectional -- there is no time-dimension or other complication.\n\nNow consider what can happen when we deal with a more complex dataset with two dimensions, which is shown in the graph on the right.^[I used *ChatGPT* to generate this graph.] A \"naive\" OLS regression (indicated with the thick black line) would find that there is a negative relationship between the two variables , `x` and `y`. However, *within* the two groups, the relationship is positive. This positive relationship is only correctly captured by a model that focuses exclusively on the variation within each group and ignores the differences between them (indicated by the two colored lines).\n\nIn principle, the \"naive\" OLS model is not really wrong -- there *is* a negative relationship in the data as a whole. But the \"naive\" model is misleading, because it does not tell us the whole story. To get to that complete story, we need models that separate the between-group variation (the overall difference between groups A and B) from the within-group variation.\n\nThe following section introduces regression models that can achieve this.\n\n## Fixed-, random-, and between-effects models\n\nStatisticians have developed three basic regression model specifications for data that contain repeated observations of some group of units over time. Originally, they were developed for *panel survey* data, where researchers follow a larger (and often representative) sample of people over some period of time (often only a few weeks, months, or years) -- i.e., data where we have many units but few time points. The models can in principle also be applied to TSCS data, where we usually have observations for many years but only a few countries -- i.e., few units, but many time points [see also @Beck2001, 273].^[The main complicating factor is that TSCS data also have many of the characteristics of longer time series, meaning trends and the above-mentioned degrees of stationarity or integration. This means that analysts also have to think about how to model these features of the data [@DeBoef2008,@Beck2011]. This is less of a problem when working with \"shorter\" panel data.]\n\nThese models are available in `R` via the `plm` package [@Croissant2008], which you can install from *CRAN* with `install.packages()` and load with `library()`. In addition, we also use `texreg` [@Leifeld2013] to tabulate the results of the regression models:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"plm\")\nlibrary(plm)\nlibrary(texreg)\n```\n:::\n\n\n\n::: {.cell}\n\n:::\n\n\nTo illustrate how these models work, we will work with a tiny and artificial example dataset (`pandat`). This dataset contains only nine observations of three variables (`x`, `y`, `c`) for three groups (A, B, and C, measured with the `u` variable) at three points in time (1,2,3, measured with the `t` variable):\n\n::: {.cell}\n\n```{.r .cell-code}\npandat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 5\n  u         t     x     c     y\n  <chr> <dbl> <dbl> <dbl> <dbl>\n1 A         1   1.4   2     0.6\n2 A         2   1.7   2    -0.1\n3 A         3   2.1   2    -0.2\n4 B         1   2.1   1.5   4.6\n5 B         2   2.5   1.5   4.3\n6 B         3   2.7   1.5   3.8\n7 C         1   1.9   3.5   2.4\n8 C         2   2     3.5   2  \n9 C         3   2.3   3.5   1.4\n```\n\n\n:::\n:::\n\n\nWhen we visualize the patterns in the data (see @fig-pandat), we see that this is a textbook case of Simpson's Paradox: Within each group, there is a negative relationship between x and y over time (time points are indicated as numbers). A naive OLS model, however, would find a positive relationship.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Relationship between x and y in simulated data](index_files/figure-html/fig-pandat-1.png){#fig-pandat width=672}\n:::\n:::\n\n\n### Naive or \"pooled\" regression\n\nThe first model specification is the \"pooled\" model. This model does what we did above: It pools all the different observations together and fits a line to the data. In other words, this model ignores the grouped structure of the data.\n\nWe can estimate this model with the `plm()` function from the `plm` package. This function works essentially like `lm()` or `glm()` with the exception that we also need to specify the structure of the data with the `index` argument (`u` designates the groups, `t` the time points) regardless of whether the model picks it up or not). We also need to specify the type of model we want to estimate (with the `model` argument):\n\n::: {.cell}\n\n```{.r .cell-code}\npooled_plm <- plm::plm(y ~ x,\n                index = c(\"u\",\"t\"),\n                model = \"pooling\",\n                data = pandat)\nsummary(pooled_plm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPooling Model\n\nCall:\nplm::plm(formula = y ~ x, data = pandat, model = \"pooling\", index = c(\"u\", \n    \"t\"))\n\nBalanced Panel: n = 3, T = 3, N = 9\n\nResiduals:\n    Min.  1st Qu.   Median  3rd Qu.     Max. \n-2.35327 -1.09434  0.13646  0.82619  2.44673 \n\nCoefficients:\n            Estimate Std. Error t-value Pr(>|t|)  \n(Intercept)  -3.9312     2.8846 -1.3628  0.21515  \nx             2.8973     1.3664  2.1204  0.07167 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    26.949\nResidual Sum of Squares: 16.409\nR-Squared:      0.39111\nAdj. R-Squared: 0.30412\nF-statistic: 4.49627 on 1 and 7 DF, p-value: 0.071675\n```\n\n\n:::\n:::\n\n\nAs expected, the model naively pools the data and captures the positive overall relationship between x and y with a postive (and borderline significant) coefficient. It completely ignores the negative relationship between the groups.\n\nFor comparison, we can also estimate a standard OLS model with `lm()`:\n\n::: {.cell}\n\n```{.r .cell-code}\npooled_lm <- lm(y ~ x,\n          data = pandat)\nsummary(pooled_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = pandat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3533 -1.0943  0.1365  0.8262  2.4467 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)   -3.931      2.885  -1.363   0.2152  \nx              2.897      1.366   2.120   0.0717 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.531 on 7 degrees of freedom\nMultiple R-squared:  0.3911,\tAdjusted R-squared:  0.3041 \nF-statistic: 4.496 on 1 and 7 DF,  p-value: 0.07167\n```\n\n\n:::\n:::\n\n\nThe results are exactly identical. This is easier to see when we put the results side-by-side in a single table:\n\n::: {.cell}\n\n```{.r .cell-code}\nscreenreg(list(pooled_plm,pooled_lm),\n          stars = 0.05,\n          custom.model.names = c(\"Pooling (plm)\",\n                                 \"Regular OLS\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n=======================================\n             Pooling (plm)  Regular OLS\n---------------------------------------\n(Intercept)  -3.93          -3.93      \n             (2.88)         (2.88)     \nx             2.90           2.90      \n             (1.37)         (1.37)     \n---------------------------------------\nR^2           0.39           0.39      \nAdj. R^2      0.30           0.30      \nNum. obs.     9              9         \n=======================================\n* p < 0.05\n```\n\n\n:::\n:::\n\n\n### The between-effects model\n\nAs explained above, the problem with the pooled model is that naively throws together the cross-sectional and longitudinal variation in the data. We can get better answers by separating the two types of variation, and the first type of model that does this is the between-effects model. As the name suggests, the between-effects model focuses completely on the differences *between* the units and ignores all longitudinal variation over time.\n\nThe between-effects model is also almost embarrasingly simple: All we do is what we did in @fig-csvar above: We calculate average values per unit, which means that we eliminate the time-dimension from the data. Then we use the resulting aggregated data in a regular OLS regression. \n\nTo show how this works, we do this first by hand and create an aggregated version of the `pandat` dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\npandat |> \n  group_by(u) |> \n  summarise(avg_y = mean(y),\n            avg_x = mean(x)) -> pan_ag\n```\n:::\n\n\nThis dataset contains only the positive cross-sectional or between-unit relationship in the data:\n\n::: {.cell}\n\n```{.r .cell-code}\npan_ag |> \n  ggplot(aes(x = avg_x, y = avg_y)) +\n    geom_text(aes(label = u)) +\n    geom_smooth(method = \"lm\", se = F)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nIf we use this aggregated dataset in a conventional OLS regression, we find the expected positive (and now significant) relationship:\n\n::: {.cell}\n\n```{.r .cell-code}\nbe_ols <- lm(avg_y ~ avg_x,\n         data = pan_ag)\nsummary(be_ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = avg_y ~ avg_x, data = pan_ag)\n\nResiduals:\n       1        2        3 \n 0.04708  0.04280 -0.08988 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -10.1926     0.4664  -21.85   0.0291 *\navg_x         5.9109     0.2224   26.58   0.0239 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1101 on 1 degrees of freedom\nMultiple R-squared:  0.9986,\tAdjusted R-squared:  0.9972 \nF-statistic: 706.4 on 1 and 1 DF,  p-value: 0.02394\n```\n\n\n:::\n:::\n\n\nNow compare this to the `between` model specification in `plm()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nbe_plm <- plm::plm(y ~ x,\n                index = c(\"u\",\"t\"),\n                model = \"between\",\n                data = pandat)\nsummary(be_plm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOneway (individual) effect Between Model\n\nCall:\nplm::plm(formula = y ~ x, data = pandat, model = \"between\", index = c(\"u\", \n    \"t\"))\n\nBalanced Panel: n = 3, T = 3, N = 9\nObservations used in estimation: 3\n\nResiduals:\n        A         B         C \n 0.047080  0.042800 -0.089879 \n\nCoefficients:\n             Estimate Std. Error t-value Pr(>|t|)  \n(Intercept) -10.19260    0.46644 -21.852  0.02911 *\nx             5.91088    0.22239  26.578  0.02394 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    8.5785\nResidual Sum of Squares: 0.012127\nR-Squared:      0.99859\nAdj. R-Squared: 0.99717\nF-statistic: 706.416 on 1 and 1 DF, p-value: 0.023941\n```\n\n\n:::\n:::\n\n\nThe results are again exactly identical:\n\n::: {.cell}\n\n```{.r .cell-code}\nscreenreg(list(be_ols,be_plm),\n          stars = 0.05,\n          custom.model.names = c(\"OLS with aggregated data\",\n                                 \"Between-effects model (plm)\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n==================================================================\n             OLS with aggregated data  Between-effects model (plm)\n------------------------------------------------------------------\n(Intercept)  -10.19 *                  -10.19 *                   \n              (0.47)                    (0.47)                    \navg_x          5.91 *                                             \n              (0.22)                                              \nx                                        5.91 *                   \n                                        (0.22)                    \n------------------------------------------------------------------\nR^2            1.00                      1.00                     \nAdj. R^2       1.00                      1.00                     \nNum. obs.      3                         3                        \n==================================================================\n* p < 0.05\n```\n\n\n:::\n:::\n\n\n### The within- or fixed-effects model\n\nAs above, aggregating the data is inefficient because we lose all the longitudinal variation. Also, depending on the theory or hypothesis we want to test, we may need to focus on the within-group variation in our analysis. In those cases, the between-effects model (or the pooled model) will not get us very far.\n\nIf we want to focus on the within- or longitudinal variation, we can use the third alternative, which is the within-model. It is also known as the fixed-effects (FE) model. This model is, in principle, also very simple. One way to estimate this model is to simply add dummy variables for the different group units to the model (leaving one group out as the baseline). However, this adds extra variables to the model and therefore eats up statistical power that we often want to focus instead on the main effect of interest, here the relationship between `x` and `y`. \n\nThere is therefore another way to estimate this model, which is to de-mean the data and then estimate a linear regression on the resulting dataset. This means that we calculate average values for each of the variables in the model and then subtract the average from each observation. In a way, de-meaning the data is the opposite to calculating the group averages in the between-effects model.\n\nThis de-meaning of the data is a simple operation that we can again do by hand:\n\n::: {.cell}\n\n```{.r .cell-code}\npandat |> \n  group_by(u) |> \n  mutate(avg_x = mean(x),\n         avg_y = mean(y)) |> \n  ungroup() |> \n  mutate(diff_x = x - avg_x,\n         diff_y = y - avg_y) -> pandat\n```\n:::\n\n\nThen we use the de-meaned data in a conventional OLS regression:\n\n::: {.cell}\n\n```{.r .cell-code}\nfe_ols <- lm(diff_y ~ diff_x,\n          data = pandat)\nsummary(fe_ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = diff_y ~ diff_x, data = pandat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.24551 -0.08846 -0.02436  0.15769  0.23910 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept) -1.616e-16  6.222e-02   0.000  1.00000   \ndiff_x      -1.365e+00  2.589e-01  -5.275  0.00115 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1867 on 7 degrees of freedom\nMultiple R-squared:  0.799,\tAdjusted R-squared:  0.7703 \nF-statistic: 27.82 on 1 and 7 DF,  p-value: 0.001155\n```\n\n\n:::\n:::\n\n\nNotice that the coefficient on `x` now switches its sign: It is now *negative* -- which means that the model now captures the negative relationship between `x` and `y` *within* each group.\n\nWhen using the `plm()` function, we simply specify `within` as the model -- but we use the original data. `plm()` takes care of the de-meaning for us:\n\n::: {.cell}\n\n```{.r .cell-code}\nfe_plm <- plm::plm(y ~ x,\n                index = c(\"u\",\"t\"),\n                model = \"within\",\n                data = pandat)\nsummary(fe_plm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOneway (individual) effect Within Model\n\nCall:\nplm::plm(formula = y ~ x, data = pandat, model = \"within\", index = c(\"u\", \n    \"t\"))\n\nBalanced Panel: n = 3, T = 3, N = 9\n\nResiduals:\n        1         2         3         4         5         6         7         8 \n 0.044872 -0.245513  0.200641 -0.088462  0.157692 -0.069231  0.239103 -0.024359 \n        9 \n-0.214744 \n\nCoefficients:\n  Estimate Std. Error t-value Pr(>|t|)   \nx -1.36538    0.30629 -4.4579 0.006654 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    1.2133\nResidual Sum of Squares: 0.24391\nR-Squared:      0.79898\nAdj. R-Squared: 0.67836\nF-statistic: 19.8725 on 1 and 5 DF, p-value: 0.0066536\n```\n\n\n:::\n:::\n\n\nIf you take a very careful look at the results, you see that the coefficient estimate ($\\beta$) is exactly identical to the one we got in the OLS model on the de-meaned data. However, the standard error and `t`- and `p`-values are different! \n\nThe reason for this is that we use up some of the information in the data (i.e., *degrees of freedom*) when we calculate the group-averages in the first step. This information is then technically not available when we we calculate standard errors and `p`-values.^[You might remember that we make a similar adjustment when we calculate the standard deviation of a variable, where we also \"use up\" information in the first step by calculating the variable's mean [@Kellstedt2018, 137].] The `plm()` function automatically makes this *degree-of-freedom* adjustment, but not the naive `lm()` function, hence the different results.\n\nNow compare this to the \"dummy\" specification:\n\n::: {.cell}\n\n```{.r .cell-code}\nfe_dum <- lm(y ~ x + u,\n          data = pandat)\nsummary(fe_dum)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x + u, data = pandat)\n\nResiduals:\n       1        2        3        4        5        6        7        8 \n 0.04487 -0.24551  0.20064 -0.08846  0.15769 -0.06923  0.23910 -0.02436 \n       9 \n-0.21474 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.4667     0.5460   4.518 0.006296 ** \nx            -1.3654     0.3063  -4.458 0.006654 ** \nuB            5.0891     0.2802  18.165 9.29e-06 ***\nuC            2.2885     0.2072  11.043 0.000106 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2209 on 5 degrees of freedom\nMultiple R-squared:  0.9909,\tAdjusted R-squared:  0.9855 \nF-statistic: 182.5 on 3 and 5 DF,  p-value: 1.583e-05\n```\n\n\n:::\n:::\n\n\nNow we have two additional variables in the model, the dummies for groups B and C (group A is omitted and measured with the intercept). If you take a careful look, you will notice that the estimate for the coefficient on `x` is now exactly identical to the estimate we got from th `plm()` function. This is because the dummy-specification explicitly adjusts for the degrees of freedom by adding additional coefficients to the model (which use up information in the estimation).\n\nWe can again put the results side-by-side to see more clearly where they differ:\n\n::: {.cell}\n\n```{.r .cell-code}\nscreenreg(list(fe_ols,fe_plm,fe_dum),\n          stars = 0.05,\n          custom.model.names = c(\"Regular OLS (dem. data)\",\n                                 \"Fixed-effects (plm)\",\n                                 \"Fixed-effects (dummies)\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n==================================================================================\n             Regular OLS (dem. data)  Fixed-effects (plm)  Fixed-effects (dummies)\n----------------------------------------------------------------------------------\n(Intercept)  -0.00                                          2.47 *                \n             (0.06)                                        (0.55)                 \ndiff_x       -1.37 *                                                              \n             (0.26)                                                               \nx                                     -1.37 *              -1.37 *                \n                                      (0.31)               (0.31)                 \nuB                                                          5.09 *                \n                                                           (0.28)                 \nuC                                                          2.29 *                \n                                                           (0.21)                 \n----------------------------------------------------------------------------------\nR^2           0.80                     0.80                 0.99                  \nAdj. R^2      0.77                     0.68                 0.99                  \nNum. obs.     9                        9                    9                     \n==================================================================================\n* p < 0.05\n```\n\n\n:::\n:::\n\n\nAgain, the estimates of the coefficient on `x` are exactly identical across the three models, but the standard error is a bit lower in the first model than in the other two (which are exactly identical). You probably also notice that the within- or fixed-effects model estimated with `plm()` does not contain an intercept.\n\nIn any case, the fixed-effects or within-specification correctly captures the within-variation in the data, meaning the negative relationship between `x` and `y` within each group.\n\n\n### Fixed-effects with constant variables\n\nYou might now ask why we don't just always use the fixed-effects (FE) model for anything? One of the problems with the FE model is that it cannot capture the effects of variables that are either constant over time or change only very rarely and slowly -- the constitutional constraints variable is one of the textbook examples.\n\nTo see why, we can estimate a FE model with the one variable in our practice dataset that does not change over time (`c`, for \"constant\"):\n\n::: {.cell}\n\n```{.r .cell-code}\npandat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 9\n  u         t     x     c     y avg_x avg_y  diff_x  diff_y\n  <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>   <dbl>\n1 A         1   1.4   2     0.6  1.73  0.1  -0.333   0.5   \n2 A         2   1.7   2    -0.1  1.73  0.1  -0.0333 -0.2   \n3 A         3   2.1   2    -0.2  1.73  0.1   0.367  -0.3   \n4 B         1   2.1   1.5   4.6  2.43  4.23 -0.333   0.367 \n5 B         2   2.5   1.5   4.3  2.43  4.23  0.0667  0.0667\n6 B         3   2.7   1.5   3.8  2.43  4.23  0.267  -0.433 \n7 C         1   1.9   3.5   2.4  2.07  1.93 -0.167   0.467 \n8 C         2   2     3.5   2    2.07  1.93 -0.0667  0.0667\n9 C         3   2.3   3.5   1.4  2.07  1.93  0.233  -0.533 \n```\n\n\n:::\n:::\n\n\nNotice what happens when we do the de-meaning operation with this variable:\n\n::: {.cell}\n\n```{.r .cell-code}\npandat |> \n  group_by(u) |> \n  mutate(avg_c = mean(c)) |> \n  ungroup() |> \n  mutate(diff_c = c - avg_c) -> pandat\n\npandat |> \n  select(u,t,c,avg_c,diff_c)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 5\n  u         t     c avg_c diff_c\n  <chr> <dbl> <dbl> <dbl>  <dbl>\n1 A         1   2     2        0\n2 A         2   2     2        0\n3 A         3   2     2        0\n4 B         1   1.5   1.5      0\n5 B         2   1.5   1.5      0\n6 B         3   1.5   1.5      0\n7 C         1   3.5   3.5      0\n8 C         2   3.5   3.5      0\n9 C         3   3.5   3.5      0\n```\n\n\n:::\n:::\n\n\nThe resulting variable, `diff_c` (which measures the deviations of each variable from the group-specific means) contains only zeros! This is only logical: The variable does not change over time, so each individual observation is exactly identical to the group-specific mean. When we then subtract the two, we get only zeros.\n\n`plm()` will now simply refuse to estimate the model:\n\n::: {.cell}\n\n```{.r .cell-code}\nfe_plm_c <- plm(y ~ c,\n                index = c(\"u\",\"t\"),\n                model = \"within\",\n                data = pandat)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in plm.fit(data, model, effect, random.method, random.models, random.dfcor, : empty model\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fe_plm_c)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'fe_plm_c' not found\n```\n\n\n:::\n:::\n\n\n### Which model should I now choose?\n\n\n\n## References\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}